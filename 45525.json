{"path":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(), true, -1));\n          \n          copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(), true, -1));\n          \n          copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2ee5c613bc76903f5e2dee48b9d63c3a9ed3aeb0","date":1337448276,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        final Set<String> copiedFiles = new HashSet<String>();\n\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(), true, -1));\n          \n          copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context, copiedFiles);\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(), true, -1));\n          \n          copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4356000e349e38c9fb48034695b7c309abd54557","date":1337460341,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        final Set<String> copiedFiles = new HashSet<String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(), true, -1));\n          \n          infos.add(copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context, copiedFiles));\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(), true, -1));\n          \n          copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9d153abcf92dc5329d98571a8c3035df9bd80648","date":1337702630,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        final Set<String> copiedFiles = new HashSet<String>();\n        for (SegmentInfoPerCommit info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.info.getDocStoreSegment();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.info.docCount, info.info.sizeInBytes(), true, -1));\n          \n          infos.add(copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context, copiedFiles));\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        final Set<String> copiedFiles = new HashSet<String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(), true, -1));\n          \n          infos.add(copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context, copiedFiles));\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"203d7d3cb7712e10ef33009a63247ae40c302d7a","date":1337798111,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        final Set<String> copiedFiles = new HashSet<String>();\n        for (SegmentInfoPerCommit info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.info.getDocStoreSegment();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n          \n          infos.add(copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context, copiedFiles));\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        final Set<String> copiedFiles = new HashSet<String>();\n        for (SegmentInfoPerCommit info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.info.getDocStoreSegment();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.info.docCount, info.info.sizeInBytes(), true, -1));\n          \n          infos.add(copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context, copiedFiles));\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6a917aca07a305ab70118a83e84d931503441271","date":1337826487,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        final Set<String> copiedFiles = new HashSet<String>();\n        for (SegmentInfoPerCommit info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = Lucene3xSegmentInfoFormat.getDocStoreSegment(info.info);\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n          \n          infos.add(copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context, copiedFiles));\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        final Set<String> copiedFiles = new HashSet<String>();\n        for (SegmentInfoPerCommit info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.info.getDocStoreSegment();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n          \n          infos.add(copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context, copiedFiles));\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"615ddbd81799980d0fdd95e0238e1c498b6f47b0","date":1338233290,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        final Set<String> copiedFiles = new HashSet<String>();\n        for (SegmentInfoPerCommit info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = Lucene3xSegmentInfoFormat.getDocStoreSegment(info.info);\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n          \n          infos.add(copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context, copiedFiles));\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        final Set<String> copiedFiles = new HashSet<String>();\n\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(), true, -1));\n          \n          copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context, copiedFiles);\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"57ae3024996ccdb3c36c42cb890e1efb37df4ce8","date":1338343651,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        final Set<String> copiedFiles = new HashSet<String>();\n        for (SegmentInfoPerCommit info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.info.name;\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n          \n          infos.add(copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context, copiedFiles));\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        final Set<String> copiedFiles = new HashSet<String>();\n        for (SegmentInfoPerCommit info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = Lucene3xSegmentInfoFormat.getDocStoreSegment(info.info);\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n          \n          infos.add(copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context, copiedFiles));\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4d3e8520fd031bab31fd0e4d480e55958bc45efe","date":1340901565,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        final Set<String> copiedFiles = new HashSet<String>();\n        for (SegmentInfoPerCommit info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.info.name;\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n          \n          infos.add(copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context, copiedFiles));\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        final Set<String> copiedFiles = new HashSet<String>();\n        for (SegmentInfoPerCommit info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.info.name;\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n          \n          infos.add(copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context, copiedFiles));\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fe33227f6805edab2036cbb80645cc4e2d1fa424","date":1342713534,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        final Set<String> copiedFiles = new HashSet<String>();\n        for (SegmentInfoPerCommit info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.info.name;\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n          \n          infos.add(copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context, copiedFiles));\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        final Set<String> copiedFiles = new HashSet<String>();\n        for (SegmentInfoPerCommit info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.info.name;\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n          \n          infos.add(copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context, copiedFiles));\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"23ab146d336df02e9a396ea060f8f4b114dc33a6","date":1344450994,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n\n        for (SegmentInfoPerCommit info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.info.name;\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n          \n          infos.add(copySegmentAsIs(info, newSegName, context));\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        final Set<String> copiedFiles = new HashSet<String>();\n        for (SegmentInfoPerCommit info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.info.name;\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n          \n          infos.add(copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context, copiedFiles));\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n\n        for (SegmentInfoPerCommit info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.info.name;\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n          \n          infos.add(copySegmentAsIs(info, newSegName, context));\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        final Set<String> copiedFiles = new HashSet<String>();\n        for (SegmentInfoPerCommit info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.info.name;\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n          \n          infos.add(copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context, copiedFiles));\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c7869f64c874ebf7f317d22c00baf2b6857797a6","date":1344856617,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n\n        for (SegmentInfoPerCommit info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.info.name;\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n          \n          infos.add(copySegmentAsIs(info, newSegName, context));\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        final Set<String> copiedFiles = new HashSet<String>();\n        for (SegmentInfoPerCommit info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.info.name;\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n          \n          infos.add(copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context, copiedFiles));\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9eae2a56dc810a17cf807d831f720dec931a03de","date":1349262073,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n\n          for (SegmentInfoPerCommit info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n            String dsName = info.info.name;\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n          \n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentInfoPerCommit sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentInfoPerCommit sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n\n        for (SegmentInfoPerCommit info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.info.name;\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n          \n          infos.add(copySegmentAsIs(info, newSegName, context));\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["98a04f56464afdffd4c430d6c47a0c868a38354e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b2266ed5528eed5eee00e1d3ff77d6cce6ef0b1f","date":1349264427,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n\n          for (SegmentInfoPerCommit info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n          \n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentInfoPerCommit sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentInfoPerCommit sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n\n          for (SegmentInfoPerCommit info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n            String dsName = info.info.name;\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n          \n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentInfoPerCommit sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentInfoPerCommit sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6e2893fd5349134af382d33ccc3d84840394c6c1","date":1353682567,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n\n          for (SegmentInfoPerCommit info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n\n            for(FieldInfo fi : getFieldInfos(info.info)) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentInfoPerCommit sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentInfoPerCommit sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n\n          for (SegmentInfoPerCommit info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n          \n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentInfoPerCommit sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentInfoPerCommit sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n\n          for (SegmentInfoPerCommit info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n\n            for(FieldInfo fi : getFieldInfos(info.info)) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentInfoPerCommit sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentInfoPerCommit sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n\n          for (SegmentInfoPerCommit info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n          \n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentInfoPerCommit sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentInfoPerCommit sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":["639c36565ce03aed5b0fce7c9e4448e53a1f7efd"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b47dabfbaff6449eedcd4321017ab2f73dfa06ab","date":1360797548,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n\n          for (SegmentInfoPerCommit info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            for(FieldInfo fi : getFieldInfos(info.info)) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentInfoPerCommit sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentInfoPerCommit sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n\n          for (SegmentInfoPerCommit info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n\n            for(FieldInfo fi : getFieldInfos(info.info)) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentInfoPerCommit sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentInfoPerCommit sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3b41f996b22bd5518650f897d050088ff808ec03","date":1360969107,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n\n          for (SegmentInfoPerCommit info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            for(FieldInfo fi : getFieldInfos(info.info)) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentInfoPerCommit sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentInfoPerCommit sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n\n          for (SegmentInfoPerCommit info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));\n\n            for(FieldInfo fi : getFieldInfos(info.info)) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentInfoPerCommit sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentInfoPerCommit sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8435160e9702b19398118ddf76b61c846612b6a4","date":1380349140,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n\n          for (SegmentInfoPerCommit info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            for(FieldInfo fi : SegmentReader.readFieldInfos(info)) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentInfoPerCommit sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentInfoPerCommit sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n\n          for (SegmentInfoPerCommit info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            for(FieldInfo fi : getFieldInfos(info.info)) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentInfoPerCommit sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentInfoPerCommit sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0","date":1383367127,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<SegmentCommitInfo>();\n\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            for(FieldInfo fi : SegmentReader.readFieldInfos(info)) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();\n\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n\n          for (SegmentInfoPerCommit info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            for(FieldInfo fi : SegmentReader.readFieldInfos(info)) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentInfoPerCommit sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentInfoPerCommit sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0fcdcf196523675146a4df3193e91413533857ab","date":1390686560,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<SegmentCommitInfo>();\n\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            for(FieldInfo fi : SegmentReader.readFieldInfos(info)) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<SegmentCommitInfo>();\n\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            for(FieldInfo fi : SegmentReader.readFieldInfos(info)) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            for(FieldInfo fi : SegmentReader.readFieldInfos(info)) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<SegmentCommitInfo>();\n\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            for(FieldInfo fi : SegmentReader.readFieldInfos(info)) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d6b7c6630218ed9693cdb8643276513f9f0043f4","date":1406648084,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n      int totalDocCount = 0;\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n          totalDocCount += sis.totalDocCount();\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            for(FieldInfo fi : SegmentReader.readFieldInfos(info)) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          // Make sure adding the new documents to this index won't\n          // exceed the limit:\n          reserveDocs(totalDocCount);\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            for(FieldInfo fi : SegmentReader.readFieldInfos(info)) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["98a04f56464afdffd4c430d6c47a0c868a38354e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c3b36e97ff7b3f9377144e86f395df5f4eee5c20","date":1408670441,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n      int totalDocCount = 0;\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n          totalDocCount += sis.totalDocCount();\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            for(FieldInfo fi : SegmentReader.readFieldInfos(info)) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          // Make sure adding the new documents to this index won't\n          // exceed the limit:\n          reserveDocs(totalDocCount);\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n      int totalDocCount = 0;\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n          totalDocCount += sis.totalDocCount();\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            for(FieldInfo fi : SegmentReader.readFieldInfos(info)) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          // Make sure adding the new documents to this index won't\n          // exceed the limit:\n          reserveDocs(totalDocCount);\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"949847c0040cd70a68222d526cb0da7bf6cbb3c2","date":1410997182,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n      int totalDocCount = 0;\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n          totalDocCount += sis.totalDocCount();\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            for(FieldInfo fi : SegmentReader.readFieldInfos(info)) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          // Make sure adding the new documents to this index won't\n          // exceed the limit:\n          reserveDocs(totalDocCount);\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n      int totalDocCount = 0;\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n          totalDocCount += sis.totalDocCount();\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            for(FieldInfo fi : SegmentReader.readFieldInfos(info)) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          // Make sure adding the new documents to this index won't\n          // exceed the limit:\n          reserveDocs(totalDocCount);\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":["c48871ed951104729f5e17a8ee1091b43fa18980"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"256a0e54e76f18e115a43e7fe793b54d4e9a3005","date":1412426514,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n      int totalDocCount = 0;\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n          totalDocCount += sis.totalDocCount();\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          // Make sure adding the new documents to this index won't\n          // exceed the limit:\n          reserveDocs(totalDocCount);\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n      int totalDocCount = 0;\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n          totalDocCount += sis.totalDocCount();\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            for(FieldInfo fi : SegmentReader.readFieldInfos(info)) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          // Make sure adding the new documents to this index won't\n          // exceed the limit:\n          reserveDocs(totalDocCount);\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9bb9a29a5e71a90295f175df8919802993142c9a","date":1412517673,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n      int totalDocCount = 0;\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n          totalDocCount += sis.totalDocCount();\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          // Make sure adding the new documents to this index won't\n          // exceed the limit:\n          reserveDocs(totalDocCount);\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n      int totalDocCount = 0;\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n          totalDocCount += sis.totalDocCount();\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            for(FieldInfo fi : SegmentReader.readFieldInfos(info)) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          // Make sure adding the new documents to this index won't\n          // exceed the limit:\n          reserveDocs(totalDocCount);\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3384e6013a93e4d11b7d75388693f8d0388602bf","date":1413951663,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n      int totalDocCount = 0;\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n          totalDocCount += sis.totalDocCount();\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          // Make sure adding the new documents to this index won't\n          // exceed the limit:\n          reserveDocs(totalDocCount);\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n      int totalDocCount = 0;\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n          totalDocCount += sis.totalDocCount();\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          // Make sure adding the new documents to this index won't\n          // exceed the limit:\n          reserveDocs(totalDocCount);\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":["98a04f56464afdffd4c430d6c47a0c868a38354e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"db68c63cbfaa8698b9c4475f75ed2b9c9696d238","date":1414118621,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n      int totalDocCount = 0;\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n          totalDocCount += sis.totalDocCount();\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          // Make sure adding the new documents to this index won't\n          // exceed the limit:\n          reserveDocs(totalDocCount);\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n      int totalDocCount = 0;\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dir);\n          totalDocCount += sis.totalDocCount();\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          // Make sure adding the new documents to this index won't\n          // exceed the limit:\n          reserveDocs(totalDocCount);\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5faf65b6692f15cca0f87bf8666c87899afc619f","date":1420468108,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n      int totalDocCount = 0;\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n          totalDocCount += sis.totalDocCount();\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.getDocCount(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          // Make sure adding the new documents to this index won't\n          // exceed the limit:\n          reserveDocs(totalDocCount);\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n      int totalDocCount = 0;\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n          totalDocCount += sis.totalDocCount();\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          // Make sure adding the new documents to this index won't\n          // exceed the limit:\n          reserveDocs(totalDocCount);\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"98b44240f64a2d6935543ff25faee750b29204eb","date":1424972040,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalDocCount = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalDocCount += sis.totalDocCount();\n        commits.add(sis);\n      }\n        \n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(totalDocCount);\n\n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.getDocCount(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            IOUtils.deleteFilesIgnoringExceptions(directory, sipc.files().toArray(new String[0]));\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n      int totalDocCount = 0;\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n          totalDocCount += sis.totalDocCount();\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.getDocCount(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          // Make sure adding the new documents to this index won't\n          // exceed the limit:\n          reserveDocs(totalDocCount);\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"acf0fc8b8488d15344408e0ed0ab484f4a3e1bf2","date":1424979404,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalDocCount = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalDocCount += sis.totalDocCount();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalDocCount);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.getDocCount(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            IOUtils.deleteFilesIgnoringExceptions(directory, sipc.files().toArray(new String[0]));\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalDocCount);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalDocCount = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalDocCount += sis.totalDocCount();\n        commits.add(sis);\n      }\n        \n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(totalDocCount);\n\n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.getDocCount(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            IOUtils.deleteFilesIgnoringExceptions(directory, sipc.files().toArray(new String[0]));\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"98a04f56464afdffd4c430d6c47a0c868a38354e","date":1424985833,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalDocCount = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalDocCount += sis.totalDocCount();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalDocCount);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.getDocCount(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            IOUtils.deleteFilesIgnoringExceptions(directory, sipc.files().toArray(new String[0]));\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalDocCount);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n      int totalDocCount = 0;\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n          totalDocCount += sis.totalDocCount();\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.getDocCount(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          // Make sure adding the new documents to this index won't\n          // exceed the limit:\n          reserveDocs(totalDocCount);\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":["2a186ae8733084223c22044e935e4ef848a143d1","3384e6013a93e4d11b7d75388693f8d0388602bf","9eae2a56dc810a17cf807d831f720dec931a03de","58c6bbc222f074c844e736e6fb23647e3db9cfe3","d6b7c6630218ed9693cdb8643276513f9f0043f4"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b0267c69e2456a3477a1ad785723f2135da3117e","date":1425317087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            IOUtils.deleteFilesIgnoringExceptions(directory, sipc.files().toArray(new String[0]));\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalDocCount = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalDocCount += sis.totalDocCount();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalDocCount);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.getDocCount(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            IOUtils.deleteFilesIgnoringExceptions(directory, sipc.files().toArray(new String[0]));\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalDocCount);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b06445ae1731e049327712db0454e5643ca9b7fe","date":1425329139,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            IOUtils.deleteFilesIgnoringExceptions(directory, sipc.files().toArray(new String[0]));\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalDocCount = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalDocCount += sis.totalDocCount();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalDocCount);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.getDocCount(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            IOUtils.deleteFilesIgnoringExceptions(directory, sipc.files().toArray(new String[0]));\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalDocCount);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            IOUtils.deleteFilesIgnoringExceptions(directory, sipc.files().toArray(new String[0]));\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n      int totalDocCount = 0;\n      boolean success = false;\n      try {\n        for (Directory dir : dirs) {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n          }\n          SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n          totalDocCount += sis.totalDocCount();\n\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.getDocCount(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            for(String file : sipc.files()) {\n              try {\n                directory.deleteFile(file);\n              } catch (Throwable t) {\n              }\n            }\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n          // Make sure adding the new documents to this index won't\n          // exceed the limit:\n          reserveDocs(totalDocCount);\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"618635065f043788c9e034f96ca5cd5cea1b4592","date":1433442044,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            IOUtils.deleteFilesIgnoringExceptions(directory, sipc.files().toArray(new String[0]));\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws LockObtainFailedException if we were unable to\n   *   acquire the write lock in at least one directory\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            IOUtils.deleteFilesIgnoringExceptions(directory, sipc.files().toArray(new String[0]));\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"950882a2bd2a5f9dc16a154871584eaa643d882a","date":1436366563,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            IOUtils.deleteFilesIgnoringExceptions(directory, sipc.files().toArray(new String[0]));\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              for(String file : sipc.files()) {\n                try {\n                  directory.deleteFile(file);\n                } catch (Throwable t) {\n                }\n              }\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ca792c26af46bd6c4a08d81117c60440cf6a7e3d","date":1445938295,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getDimensionCount(), fi.getDimensionNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c48871ed951104729f5e17a8ee1091b43fa18980","date":1446564542,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getDimensionCount(), fi.getDimensionNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getDimensionCount(), fi.getDimensionNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6","949847c0040cd70a68222d526cb0da7bf6cbb3c2"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cab7a79353f33d1a94cd307bf33aa5148601ebe6","date":1453391888,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getDimensionCount(), fi.getDimensionNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ceaef6cfc68c8ab22a684192e469a8280f9e6e70","date":1462354657,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  // nocommit doesn't support index sorting?  or sorts must be the same?\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5e03940e6e9044943de4b7ac08f8581da37a9534","date":1462870173,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  // nocommit doesn't support index sorting?  or sorts must be the same?\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3d33e731a93d4b57e662ff094f64f94a745422d4","date":1463128289,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0ad30c6a479e764150a3316e57263319775f1df2","date":1463395403,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f6df47cbfd656ea50ca2996361f7954531ee18b","date":1464133540,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    // no need to increment:\n    return docWriter.deleteQueue.seqNo.get();\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"16ebfabc294f23b88b6a39722a02c9d39b353195","date":1464343867,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    // no need to increment:\n    return docWriter.deleteQueue.seqNo.get();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6483e4260c08168709c02238ae083a51519a28dd","date":1465117546,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"191128ac5b85671b1671e2c857437694283b6ebf","date":1465297861,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    boolean successTop = false;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d97ba94eef1fc33c5451259a7aa2ac682646c1af","date":1488285427,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (Objects.equals(segmentInfos.getIndexCreatedVersion(), sis.getIndexCreatedVersion()) == false) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by \"\n              + segmentInfos.getIndexCreatedVersion()\n              + \" while one of the directories contains an index that was generated with \"\n              + sis.getIndexCreatedVersion());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"31741cf1390044e38a2ec3127cf302ba841bfd75","date":1491292636,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (Objects.equals(segmentInfos.getIndexCreatedVersion(), sis.getIndexCreatedVersion()) == false) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by \"\n              + segmentInfos.getIndexCreatedVersion()\n              + \" while one of the directories contains an index that was generated with \"\n              + sis.getIndexCreatedVersion());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"92212fd254551a0b1156aafc3a1a6ed1a43932ad","date":1491296431,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (Objects.equals(segmentInfos.getIndexCreatedVersion(), sis.getIndexCreatedVersion()) == false) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by \"\n              + segmentInfos.getIndexCreatedVersion()\n              + \" while one of the directories contains an index that was generated with \"\n              + sis.getIndexCreatedVersion());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"165c905a42bedc7c7d1acb37b177498306b7e866","date":1518704038,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getIndexOptions(), fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"845b760a99e5f369fcd0a5d723a87b8def6a3f56","date":1521117993,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getIndexOptions(), fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      throw tragedy;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getIndexOptions(), fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eeba0a4d0845889a402dd225793d62f009d029c9","date":1527938093,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getIndexOptions(), fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes(), fi.isSoftDeletesField());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      throw tragedy;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getIndexOptions(), fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      throw tragedy;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab548c8f96022b4780f7500a30b19b4f4a5feeb6","date":1527940044,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getIndexOptions(), fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      throw tragedy;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getIndexOptions(), fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes(), fi.isSoftDeletesField());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      throw tragedy;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7e4ca6dc9612ff741d8713743e2bccfae5eadac","date":1528093718,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getIndexOptions(), fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes(), fi.isSoftDeletesField());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      throw tragedy;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getIndexOptions(), fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      throw tragedy;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f592209545c71895260367152601e9200399776d","date":1528238935,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getIndexOptions(), fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes(), fi.isSoftDeletesField());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      throw tragedy;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getIndexOptions(), fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      throw tragedy;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b70042a8a492f7054d480ccdd2be9796510d4327","date":1528386658,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getIndexOptions(), fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes(), fi.isSoftDeletesField());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      throw tragedy;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getIndexOptions(), fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      throw tragedy;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6e22133c22c69a013e8c3c14bb986e7848c7296e","date":1537859647,"type":3,"author":"Jim Ferenczi","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && (segmentIndexSort == null || isCongruentSort(indexSort, segmentIndexSort) == false)) {\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getIndexOptions(), fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes(), fi.isSoftDeletesField());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      throw tragedy;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && segmentIndexSort != null && indexSort.equals(segmentIndexSort) == false) {\n              // TODO: we could make this smarter, e.g. if the incoming indexSort is congruent with our sort (\"starts with\") then it's OK\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getIndexOptions(), fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes(), fi.isSoftDeletesField());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      throw tragedy;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f6652c943595e92c187ee904c382863013eae28f","date":1539042663,"type":3,"author":"Nicholas Knize","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && (segmentIndexSort == null || isCongruentSort(indexSort, segmentIndexSort) == false)) {\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getIndexOptions(), fi.getDocValuesType(), fi.getPointDataDimensionCount(), fi.getPointIndexDimensionCount(), fi.getPointNumBytes(), fi.isSoftDeletesField());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      throw tragedy;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && (segmentIndexSort == null || isCongruentSort(indexSort, segmentIndexSort) == false)) {\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getIndexOptions(), fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes(), fi.isSoftDeletesField());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      throw tragedy;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"59ed8c026ba85e3c42fb89605b2032dc6f9cc241","date":1581113294,"type":3,"author":"Nicholas Knize","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && (segmentIndexSort == null || isCongruentSort(indexSort, segmentIndexSort) == false)) {\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getIndexOptions(), fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointIndexDimensionCount(), fi.getPointNumBytes(), fi.isSoftDeletesField());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      throw tragedy;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && (segmentIndexSort == null || isCongruentSort(indexSort, segmentIndexSort) == false)) {\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getIndexOptions(), fi.getDocValuesType(), fi.getPointDataDimensionCount(), fi.getPointIndexDimensionCount(), fi.getPointNumBytes(), fi.isSoftDeletesField());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      throw tragedy;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b07024a7318c25225dc4d070cf6d047315b73aaf","date":1586885963,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && (segmentIndexSort == null || isCongruentSort(indexSort, segmentIndexSort) == false)) {\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getIndexOptions(), fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointIndexDimensionCount(), fi.getPointNumBytes(), fi.isSoftDeletesField());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      throw tragedy;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> this method acquires the write lock in\n   * each directory, to ensure that no {@code IndexWriter}\n   * is currently open or tries to open while this is\n   * running.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>All added indexes must have been created by the same\n   * Lucene version as this index.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   * @throws IllegalArgumentException if addIndexes would cause\n   *   the index to exceed {@link #MAX_DOCS}, or if the indoming\n   *   index sort does not match this index's index sort\n   */\n  public long addIndexes(Directory... dirs) throws IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    List<Lock> locks = acquireWriteLocks(dirs);\n\n    Sort indexSort = config.getIndexSort();\n\n    boolean successTop = false;\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentCommitInfo> infos = new ArrayList<>();\n\n      // long so we can detect int overflow:\n      long totalMaxDoc = 0;\n      List<SegmentInfos> commits = new ArrayList<>(dirs.length);\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir\n        if (segmentInfos.getIndexCreatedVersionMajor() != sis.getIndexCreatedVersionMajor()) {\n          throw new IllegalArgumentException(\"Cannot use addIndexes(Directory) with indexes that have been created \"\n              + \"by a different Lucene version. The current index was generated by Lucene \"\n              + segmentInfos.getIndexCreatedVersionMajor()\n              + \" while one of the directories contains an index that was generated with Lucene \"\n              + sis.getIndexCreatedVersionMajor());\n        }\n        totalMaxDoc += sis.totalMaxDoc();\n        commits.add(sis);\n      }\n\n      // Best-effort up front check:\n      testReserveDocs(totalMaxDoc);\n        \n      boolean success = false;\n      try {\n        for (SegmentInfos sis : commits) {\n          for (SegmentCommitInfo info : sis) {\n            assert !infos.contains(info): \"dup info dir=\" + info.info.dir + \" name=\" + info.info.name;\n\n            Sort segmentIndexSort = info.info.getIndexSort();\n\n            if (indexSort != null && (segmentIndexSort == null || isCongruentSort(indexSort, segmentIndexSort) == false)) {\n              throw new IllegalArgumentException(\"cannot change index sort from \" + segmentIndexSort + \" to \" + indexSort);\n            }\n\n            String newSegName = newSegmentName();\n\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.info.name + \" newName=\" + newSegName + \" info=\" + info);\n            }\n\n            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));\n\n            FieldInfos fis = readFieldInfos(info);\n            for(FieldInfo fi : fis) {\n              // This will throw exceptions if any of the incoming fields have an illegal schema change:\n              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getIndexOptions(), fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointIndexDimensionCount(), fi.getPointNumBytes(), fi.isSoftDeletesField());\n            }\n            infos.add(copySegmentAsIs(info, newSegName, context));\n          }\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          for(SegmentCommitInfo sipc : infos) {\n            // Safe: these files must exist\n            deleteNewFiles(sipc.files());\n          }\n        }\n      }\n\n      synchronized (this) {\n        success = false;\n        try {\n          ensureOpen();\n\n          // Now reserve the docs, just before we update SIS:\n          reserveDocs(totalMaxDoc);\n\n          seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n\n          success = true;\n        } finally {\n          if (!success) {\n            for(SegmentCommitInfo sipc : infos) {\n              // Safe: these files must exist\n              deleteNewFiles(sipc.files());\n            }\n          }\n        }\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n      successTop = true;\n\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(Directory...)\");\n      throw tragedy;\n    } finally {\n      if (successTop) {\n        IOUtils.close(locks);\n      } else {\n        IOUtils.closeWhileHandlingException(locks);\n      }\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b2266ed5528eed5eee00e1d3ff77d6cce6ef0b1f":["9eae2a56dc810a17cf807d831f720dec931a03de"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c3b36e97ff7b3f9377144e86f395df5f4eee5c20":["d6b7c6630218ed9693cdb8643276513f9f0043f4"],"165c905a42bedc7c7d1acb37b177498306b7e866":["31741cf1390044e38a2ec3127cf302ba841bfd75"],"57ae3024996ccdb3c36c42cb890e1efb37df4ce8":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"3b41f996b22bd5518650f897d050088ff808ec03":["d4d69c535930b5cce125cff868d40f6373dc27d4","b47dabfbaff6449eedcd4321017ab2f73dfa06ab"],"ab548c8f96022b4780f7500a30b19b4f4a5feeb6":["eeba0a4d0845889a402dd225793d62f009d029c9"],"b70042a8a492f7054d480ccdd2be9796510d4327":["845b760a99e5f369fcd0a5d723a87b8def6a3f56","b7e4ca6dc9612ff741d8713743e2bccfae5eadac"],"d6b7c6630218ed9693cdb8643276513f9f0043f4":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"949847c0040cd70a68222d526cb0da7bf6cbb3c2":["c3b36e97ff7b3f9377144e86f395df5f4eee5c20"],"203d7d3cb7712e10ef33009a63247ae40c302d7a":["9d153abcf92dc5329d98571a8c3035df9bd80648"],"98a04f56464afdffd4c430d6c47a0c868a38354e":["5faf65b6692f15cca0f87bf8666c87899afc619f","acf0fc8b8488d15344408e0ed0ab484f4a3e1bf2"],"cab7a79353f33d1a94cd307bf33aa5148601ebe6":["c48871ed951104729f5e17a8ee1091b43fa18980"],"23ab146d336df02e9a396ea060f8f4b114dc33a6":["4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"b0267c69e2456a3477a1ad785723f2135da3117e":["98a04f56464afdffd4c430d6c47a0c868a38354e"],"3384e6013a93e4d11b7d75388693f8d0388602bf":["256a0e54e76f18e115a43e7fe793b54d4e9a3005"],"16ebfabc294f23b88b6a39722a02c9d39b353195":["0f6df47cbfd656ea50ca2996361f7954531ee18b"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["b2266ed5528eed5eee00e1d3ff77d6cce6ef0b1f","6e2893fd5349134af382d33ccc3d84840394c6c1"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["57ae3024996ccdb3c36c42cb890e1efb37df4ce8","4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"ca792c26af46bd6c4a08d81117c60440cf6a7e3d":["950882a2bd2a5f9dc16a154871584eaa643d882a"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["4d3e8520fd031bab31fd0e4d480e55958bc45efe","23ab146d336df02e9a396ea060f8f4b114dc33a6"],"0f6df47cbfd656ea50ca2996361f7954531ee18b":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"f592209545c71895260367152601e9200399776d":["ab548c8f96022b4780f7500a30b19b4f4a5feeb6","b7e4ca6dc9612ff741d8713743e2bccfae5eadac"],"845b760a99e5f369fcd0a5d723a87b8def6a3f56":["165c905a42bedc7c7d1acb37b177498306b7e866"],"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["8435160e9702b19398118ddf76b61c846612b6a4"],"256a0e54e76f18e115a43e7fe793b54d4e9a3005":["949847c0040cd70a68222d526cb0da7bf6cbb3c2"],"59ed8c026ba85e3c42fb89605b2032dc6f9cc241":["f6652c943595e92c187ee904c382863013eae28f"],"f6652c943595e92c187ee904c382863013eae28f":["6e22133c22c69a013e8c3c14bb986e7848c7296e"],"6a917aca07a305ab70118a83e84d931503441271":["203d7d3cb7712e10ef33009a63247ae40c302d7a"],"0ad30c6a479e764150a3316e57263319775f1df2":["cab7a79353f33d1a94cd307bf33aa5148601ebe6","3d33e731a93d4b57e662ff094f64f94a745422d4"],"191128ac5b85671b1671e2c857437694283b6ebf":["d470c8182e92b264680e34081b75e70a9f2b3c89","6483e4260c08168709c02238ae083a51519a28dd"],"31741cf1390044e38a2ec3127cf302ba841bfd75":["d97ba94eef1fc33c5451259a7aa2ac682646c1af"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["cab7a79353f33d1a94cd307bf33aa5148601ebe6","0ad30c6a479e764150a3316e57263319775f1df2"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["5faf65b6692f15cca0f87bf8666c87899afc619f","b0267c69e2456a3477a1ad785723f2135da3117e"],"4356000e349e38c9fb48034695b7c309abd54557":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":["fe33227f6805edab2036cbb80645cc4e2d1fa424","23ab146d336df02e9a396ea060f8f4b114dc33a6"],"6e22133c22c69a013e8c3c14bb986e7848c7296e":["b7e4ca6dc9612ff741d8713743e2bccfae5eadac"],"acf0fc8b8488d15344408e0ed0ab484f4a3e1bf2":["98b44240f64a2d6935543ff25faee750b29204eb"],"b06445ae1731e049327712db0454e5643ca9b7fe":["98a04f56464afdffd4c430d6c47a0c868a38354e","b0267c69e2456a3477a1ad785723f2135da3117e"],"9bb9a29a5e71a90295f175df8919802993142c9a":["949847c0040cd70a68222d526cb0da7bf6cbb3c2","256a0e54e76f18e115a43e7fe793b54d4e9a3005"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b7e4ca6dc9612ff741d8713743e2bccfae5eadac":["ab548c8f96022b4780f7500a30b19b4f4a5feeb6"],"d97ba94eef1fc33c5451259a7aa2ac682646c1af":["191128ac5b85671b1671e2c857437694283b6ebf"],"4d3e8520fd031bab31fd0e4d480e55958bc45efe":["57ae3024996ccdb3c36c42cb890e1efb37df4ce8"],"92212fd254551a0b1156aafc3a1a6ed1a43932ad":["d97ba94eef1fc33c5451259a7aa2ac682646c1af"],"98b44240f64a2d6935543ff25faee750b29204eb":["5faf65b6692f15cca0f87bf8666c87899afc619f"],"b47dabfbaff6449eedcd4321017ab2f73dfa06ab":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["0fcdcf196523675146a4df3193e91413533857ab"],"9eae2a56dc810a17cf807d831f720dec931a03de":["23ab146d336df02e9a396ea060f8f4b114dc33a6"],"2ee5c613bc76903f5e2dee48b9d63c3a9ed3aeb0":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"6e2893fd5349134af382d33ccc3d84840394c6c1":["b2266ed5528eed5eee00e1d3ff77d6cce6ef0b1f"],"9d153abcf92dc5329d98571a8c3035df9bd80648":["4356000e349e38c9fb48034695b7c309abd54557"],"8435160e9702b19398118ddf76b61c846612b6a4":["b47dabfbaff6449eedcd4321017ab2f73dfa06ab"],"6483e4260c08168709c02238ae083a51519a28dd":["d470c8182e92b264680e34081b75e70a9f2b3c89","16ebfabc294f23b88b6a39722a02c9d39b353195"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["cab7a79353f33d1a94cd307bf33aa5148601ebe6","191128ac5b85671b1671e2c857437694283b6ebf"],"c48871ed951104729f5e17a8ee1091b43fa18980":["ca792c26af46bd6c4a08d81117c60440cf6a7e3d"],"618635065f043788c9e034f96ca5cd5cea1b4592":["b0267c69e2456a3477a1ad785723f2135da3117e"],"b07024a7318c25225dc4d070cf6d047315b73aaf":["59ed8c026ba85e3c42fb89605b2032dc6f9cc241"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["2ee5c613bc76903f5e2dee48b9d63c3a9ed3aeb0","6a917aca07a305ab70118a83e84d931503441271"],"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["cab7a79353f33d1a94cd307bf33aa5148601ebe6"],"eeba0a4d0845889a402dd225793d62f009d029c9":["845b760a99e5f369fcd0a5d723a87b8def6a3f56"],"950882a2bd2a5f9dc16a154871584eaa643d882a":["618635065f043788c9e034f96ca5cd5cea1b4592"],"5faf65b6692f15cca0f87bf8666c87899afc619f":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"0fcdcf196523675146a4df3193e91413533857ab":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["9bb9a29a5e71a90295f175df8919802993142c9a","3384e6013a93e4d11b7d75388693f8d0388602bf"],"5e03940e6e9044943de4b7ac08f8581da37a9534":["ceaef6cfc68c8ab22a684192e469a8280f9e6e70"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["cab7a79353f33d1a94cd307bf33aa5148601ebe6","5e03940e6e9044943de4b7ac08f8581da37a9534"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b07024a7318c25225dc4d070cf6d047315b73aaf"]},"commit2Childs":{"b2266ed5528eed5eee00e1d3ff77d6cce6ef0b1f":["d4d69c535930b5cce125cff868d40f6373dc27d4","6e2893fd5349134af382d33ccc3d84840394c6c1"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["4356000e349e38c9fb48034695b7c309abd54557","2ee5c613bc76903f5e2dee48b9d63c3a9ed3aeb0"],"c3b36e97ff7b3f9377144e86f395df5f4eee5c20":["949847c0040cd70a68222d526cb0da7bf6cbb3c2"],"165c905a42bedc7c7d1acb37b177498306b7e866":["845b760a99e5f369fcd0a5d723a87b8def6a3f56"],"57ae3024996ccdb3c36c42cb890e1efb37df4ce8":["fe33227f6805edab2036cbb80645cc4e2d1fa424","4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"3b41f996b22bd5518650f897d050088ff808ec03":[],"ab548c8f96022b4780f7500a30b19b4f4a5feeb6":["f592209545c71895260367152601e9200399776d","b7e4ca6dc9612ff741d8713743e2bccfae5eadac"],"b70042a8a492f7054d480ccdd2be9796510d4327":[],"d6b7c6630218ed9693cdb8643276513f9f0043f4":["c3b36e97ff7b3f9377144e86f395df5f4eee5c20"],"949847c0040cd70a68222d526cb0da7bf6cbb3c2":["256a0e54e76f18e115a43e7fe793b54d4e9a3005","9bb9a29a5e71a90295f175df8919802993142c9a"],"203d7d3cb7712e10ef33009a63247ae40c302d7a":["6a917aca07a305ab70118a83e84d931503441271"],"98a04f56464afdffd4c430d6c47a0c868a38354e":["b0267c69e2456a3477a1ad785723f2135da3117e","b06445ae1731e049327712db0454e5643ca9b7fe"],"cab7a79353f33d1a94cd307bf33aa5148601ebe6":["0ad30c6a479e764150a3316e57263319775f1df2","d470c8182e92b264680e34081b75e70a9f2b3c89","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","ceaef6cfc68c8ab22a684192e469a8280f9e6e70","3d33e731a93d4b57e662ff094f64f94a745422d4"],"23ab146d336df02e9a396ea060f8f4b114dc33a6":["d6f074e73200c07d54f242d3880a8da5a35ff97b","c7869f64c874ebf7f317d22c00baf2b6857797a6","9eae2a56dc810a17cf807d831f720dec931a03de"],"b0267c69e2456a3477a1ad785723f2135da3117e":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe","618635065f043788c9e034f96ca5cd5cea1b4592"],"3384e6013a93e4d11b7d75388693f8d0388602bf":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"16ebfabc294f23b88b6a39722a02c9d39b353195":["6483e4260c08168709c02238ae083a51519a28dd"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["3b41f996b22bd5518650f897d050088ff808ec03","b47dabfbaff6449eedcd4321017ab2f73dfa06ab"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["c7869f64c874ebf7f317d22c00baf2b6857797a6"],"ca792c26af46bd6c4a08d81117c60440cf6a7e3d":["c48871ed951104729f5e17a8ee1091b43fa18980"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":[],"0f6df47cbfd656ea50ca2996361f7954531ee18b":["16ebfabc294f23b88b6a39722a02c9d39b353195"],"f592209545c71895260367152601e9200399776d":[],"845b760a99e5f369fcd0a5d723a87b8def6a3f56":["b70042a8a492f7054d480ccdd2be9796510d4327","eeba0a4d0845889a402dd225793d62f009d029c9"],"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["0fcdcf196523675146a4df3193e91413533857ab"],"256a0e54e76f18e115a43e7fe793b54d4e9a3005":["3384e6013a93e4d11b7d75388693f8d0388602bf","9bb9a29a5e71a90295f175df8919802993142c9a"],"59ed8c026ba85e3c42fb89605b2032dc6f9cc241":["b07024a7318c25225dc4d070cf6d047315b73aaf"],"f6652c943595e92c187ee904c382863013eae28f":["59ed8c026ba85e3c42fb89605b2032dc6f9cc241"],"6a917aca07a305ab70118a83e84d931503441271":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"0ad30c6a479e764150a3316e57263319775f1df2":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"191128ac5b85671b1671e2c857437694283b6ebf":["d97ba94eef1fc33c5451259a7aa2ac682646c1af","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"31741cf1390044e38a2ec3127cf302ba841bfd75":["165c905a42bedc7c7d1acb37b177498306b7e866"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["0f6df47cbfd656ea50ca2996361f7954531ee18b","191128ac5b85671b1671e2c857437694283b6ebf","6483e4260c08168709c02238ae083a51519a28dd"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"4356000e349e38c9fb48034695b7c309abd54557":["9d153abcf92dc5329d98571a8c3035df9bd80648"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":[],"6e22133c22c69a013e8c3c14bb986e7848c7296e":["f6652c943595e92c187ee904c382863013eae28f"],"acf0fc8b8488d15344408e0ed0ab484f4a3e1bf2":["98a04f56464afdffd4c430d6c47a0c868a38354e"],"b06445ae1731e049327712db0454e5643ca9b7fe":[],"9bb9a29a5e71a90295f175df8919802993142c9a":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"b7e4ca6dc9612ff741d8713743e2bccfae5eadac":["b70042a8a492f7054d480ccdd2be9796510d4327","f592209545c71895260367152601e9200399776d","6e22133c22c69a013e8c3c14bb986e7848c7296e"],"4d3e8520fd031bab31fd0e4d480e55958bc45efe":["23ab146d336df02e9a396ea060f8f4b114dc33a6","fe33227f6805edab2036cbb80645cc4e2d1fa424","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"d97ba94eef1fc33c5451259a7aa2ac682646c1af":["31741cf1390044e38a2ec3127cf302ba841bfd75","92212fd254551a0b1156aafc3a1a6ed1a43932ad"],"92212fd254551a0b1156aafc3a1a6ed1a43932ad":[],"98b44240f64a2d6935543ff25faee750b29204eb":["acf0fc8b8488d15344408e0ed0ab484f4a3e1bf2"],"b47dabfbaff6449eedcd4321017ab2f73dfa06ab":["3b41f996b22bd5518650f897d050088ff808ec03","8435160e9702b19398118ddf76b61c846612b6a4"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["d6b7c6630218ed9693cdb8643276513f9f0043f4"],"9eae2a56dc810a17cf807d831f720dec931a03de":["b2266ed5528eed5eee00e1d3ff77d6cce6ef0b1f"],"6e2893fd5349134af382d33ccc3d84840394c6c1":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"2ee5c613bc76903f5e2dee48b9d63c3a9ed3aeb0":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"9d153abcf92dc5329d98571a8c3035df9bd80648":["203d7d3cb7712e10ef33009a63247ae40c302d7a"],"8435160e9702b19398118ddf76b61c846612b6a4":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"6483e4260c08168709c02238ae083a51519a28dd":["191128ac5b85671b1671e2c857437694283b6ebf"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"c48871ed951104729f5e17a8ee1091b43fa18980":["cab7a79353f33d1a94cd307bf33aa5148601ebe6"],"618635065f043788c9e034f96ca5cd5cea1b4592":["950882a2bd2a5f9dc16a154871584eaa643d882a"],"b07024a7318c25225dc4d070cf6d047315b73aaf":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["57ae3024996ccdb3c36c42cb890e1efb37df4ce8"],"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["5e03940e6e9044943de4b7ac08f8581da37a9534"],"eeba0a4d0845889a402dd225793d62f009d029c9":["ab548c8f96022b4780f7500a30b19b4f4a5feeb6"],"950882a2bd2a5f9dc16a154871584eaa643d882a":["ca792c26af46bd6c4a08d81117c60440cf6a7e3d"],"5faf65b6692f15cca0f87bf8666c87899afc619f":["98a04f56464afdffd4c430d6c47a0c868a38354e","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","98b44240f64a2d6935543ff25faee750b29204eb"],"0fcdcf196523675146a4df3193e91413533857ab":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["5faf65b6692f15cca0f87bf8666c87899afc619f"],"5e03940e6e9044943de4b7ac08f8581da37a9534":["3d33e731a93d4b57e662ff094f64f94a745422d4"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["0ad30c6a479e764150a3316e57263319775f1df2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3b41f996b22bd5518650f897d050088ff808ec03","b70042a8a492f7054d480ccdd2be9796510d4327","d6f074e73200c07d54f242d3880a8da5a35ff97b","f592209545c71895260367152601e9200399776d","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","c7869f64c874ebf7f317d22c00baf2b6857797a6","b06445ae1731e049327712db0454e5643ca9b7fe","92212fd254551a0b1156aafc3a1a6ed1a43932ad","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}