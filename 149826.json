{"path":"lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriterDelete#testOperationsOnDiskFull(boolean).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriterDelete#testOperationsOnDiskFull(boolean).mjava","pathOld":"backwards/src/test/org/apache/lucene/index/TestIndexWriterDelete#testOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void testOperationsOnDiskFull(boolean updates) throws IOException {\n\n    boolean debug = false;\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockRAMDirectory startDir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(startDir,\n                                         new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(new Field(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockRAMDirectory dir = new MockRAMDirectory(startDir);\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);\n\n      modifier.setMaxBufferedDocs(1000); // use flush or close\n      modifier.setMaxBufferedDeleteTerms(1000); // use flush or close\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (debug) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (debug) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(new Field(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (debug) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success)\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n\n        searcher.close();\n        newReader.close();\n\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n\n      dir.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void testOperationsOnDiskFull(boolean updates) throws IOException {\n\n    boolean debug = false;\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockRAMDirectory startDir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(startDir,\n                                         new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(new Field(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockRAMDirectory dir = new MockRAMDirectory(startDir);\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);\n\n      modifier.setMaxBufferedDocs(1000); // use flush or close\n      modifier.setMaxBufferedDeleteTerms(1000); // use flush or close\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (debug) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (debug) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(new Field(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (debug) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success)\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n\n        searcher.close();\n        newReader.close();\n\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n\n      dir.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6","date":1272983566,"type":4,"author":"Uwe Schindler","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriterDelete#testOperationsOnDiskFull(boolean).mjava","sourceNew":null,"sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void testOperationsOnDiskFull(boolean updates) throws IOException {\n\n    boolean debug = false;\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockRAMDirectory startDir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(startDir,\n                                         new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(new Field(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockRAMDirectory dir = new MockRAMDirectory(startDir);\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);\n\n      modifier.setMaxBufferedDocs(1000); // use flush or close\n      modifier.setMaxBufferedDeleteTerms(1000); // use flush or close\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (debug) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (debug) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(new Field(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (debug) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success)\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n\n        searcher.close();\n        newReader.close();\n\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n\n      dir.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}