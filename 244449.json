{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter#testMaxPosition2().mjava","commits":[{"id":"fdd61b10b980a6d0b8a8d63baf7e8f5e19e8437a","date":1363558184,"type":0,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter#testMaxPosition2().mjava","pathOld":"/dev/null","sourceNew":"  public void testMaxPosition2() throws IOException {\n    for (final boolean consumeAll : new boolean[] { true, false }) {\n      Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n          // if we are consuming all tokens, we can use the checks, otherwise we can't\n          tokenizer.setEnableChecks(consumeAll);\n          return new TokenStreamComponents(tokenizer, new LimitTokenPositionFilter(tokenizer, 2, consumeAll));\n        }\n      };\n\n      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case its correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  2     3  4  5\")), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, consumeAll ? 9 : null);\n\n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  \")),\n                                new String[] { \"1\" }, new int[] { 0 }, new int[] { 1 }, consumeAll ? 3 : null);\n                                                                                   \n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  2  \")), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 6 : null);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["c83d6c4335f31cae14f625a222bc842f20073dcd"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c83d6c4335f31cae14f625a222bc842f20073dcd","date":1373306148,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter#testMaxPosition2().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter#testMaxPosition2().mjava","sourceNew":"  public void testMaxPosition2() throws IOException {\n    for (final boolean consumeAll : new boolean[] { true, false }) {\n      Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n          // if we are consuming all tokens, we can use the checks, otherwise we can't\n          tokenizer.setEnableChecks(consumeAll);\n          return new TokenStreamComponents(tokenizer, new LimitTokenPositionFilter(tokenizer, 2, consumeAll));\n        }\n      };\n\n      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case its correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2     3  4  5\"), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, consumeAll ? 9 : null);\n\n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  \"),\n                                new String[] { \"1\" }, new int[] { 0 }, new int[] { 1 }, consumeAll ? 3 : null);\n                                                                                   \n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2  \"), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 6 : null);\n    }\n  }\n\n","sourceOld":"  public void testMaxPosition2() throws IOException {\n    for (final boolean consumeAll : new boolean[] { true, false }) {\n      Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n          // if we are consuming all tokens, we can use the checks, otherwise we can't\n          tokenizer.setEnableChecks(consumeAll);\n          return new TokenStreamComponents(tokenizer, new LimitTokenPositionFilter(tokenizer, 2, consumeAll));\n        }\n      };\n\n      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case its correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  2     3  4  5\")), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, consumeAll ? 9 : null);\n\n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  \")),\n                                new String[] { \"1\" }, new int[] { 0 }, new int[] { 1 }, consumeAll ? 3 : null);\n                                                                                   \n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  2  \")), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 6 : null);\n    }\n  }\n\n","bugFix":["fdd61b10b980a6d0b8a8d63baf7e8f5e19e8437a"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter#testMaxPosition2().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter#testMaxPosition2().mjava","sourceNew":"  public void testMaxPosition2() throws IOException {\n    for (final boolean consumeAll : new boolean[] { true, false }) {\n      Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n          // if we are consuming all tokens, we can use the checks, otherwise we can't\n          tokenizer.setEnableChecks(consumeAll);\n          return new TokenStreamComponents(tokenizer, new LimitTokenPositionFilter(tokenizer, 2, consumeAll));\n        }\n      };\n\n      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case its correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2     3  4  5\"), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, consumeAll ? 9 : null);\n\n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  \"),\n                                new String[] { \"1\" }, new int[] { 0 }, new int[] { 1 }, consumeAll ? 3 : null);\n                                                                                   \n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2  \"), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 6 : null);\n    }\n  }\n\n","sourceOld":"  public void testMaxPosition2() throws IOException {\n    for (final boolean consumeAll : new boolean[] { true, false }) {\n      Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n          // if we are consuming all tokens, we can use the checks, otherwise we can't\n          tokenizer.setEnableChecks(consumeAll);\n          return new TokenStreamComponents(tokenizer, new LimitTokenPositionFilter(tokenizer, 2, consumeAll));\n        }\n      };\n\n      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case its correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  2     3  4  5\")), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, consumeAll ? 9 : null);\n\n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  \")),\n                                new String[] { \"1\" }, new int[] { 0 }, new int[] { 1 }, consumeAll ? 3 : null);\n                                                                                   \n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  2  \")), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 6 : null);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter#testMaxPosition2().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter#testMaxPosition2().mjava","sourceNew":"  public void testMaxPosition2() throws IOException {\n    for (final boolean consumeAll : new boolean[] { true, false }) {\n      Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n          // if we are consuming all tokens, we can use the checks, otherwise we can't\n          tokenizer.setEnableChecks(consumeAll);\n          return new TokenStreamComponents(tokenizer, new LimitTokenPositionFilter(tokenizer, 2, consumeAll));\n        }\n      };\n\n      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case its correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2     3  4  5\"), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, consumeAll ? 9 : null);\n\n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  \"),\n                                new String[] { \"1\" }, new int[] { 0 }, new int[] { 1 }, consumeAll ? 3 : null);\n                                                                                   \n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2  \"), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 6 : null);\n    }\n  }\n\n","sourceOld":"  public void testMaxPosition2() throws IOException {\n    for (final boolean consumeAll : new boolean[] { true, false }) {\n      Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n          // if we are consuming all tokens, we can use the checks, otherwise we can't\n          tokenizer.setEnableChecks(consumeAll);\n          return new TokenStreamComponents(tokenizer, new LimitTokenPositionFilter(tokenizer, 2, consumeAll));\n        }\n      };\n\n      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case its correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2     3  4  5\"), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, consumeAll ? 9 : null);\n\n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  \"),\n                                new String[] { \"1\" }, new int[] { 0 }, new int[] { 1 }, consumeAll ? 3 : null);\n                                                                                   \n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2  \"), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 6 : null);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"43acd3a99a12a5bec9c72097de0e294c80cb6daa","date":1396327381,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter#testMaxPosition2().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter#testMaxPosition2().mjava","sourceNew":"  public void testMaxPosition2() throws IOException {\n    for (final boolean consumeAll : new boolean[]{true, false}) {\n      Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n          // if we are consuming all tokens, we can use the checks, otherwise we can't\n          tokenizer.setEnableChecks(consumeAll);\n          return new TokenStreamComponents(tokenizer, new LimitTokenPositionFilter(tokenizer, 2, consumeAll));\n        }\n      };\n\n      // don't use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case its correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2     3  4  5\"),\n          new String[]{\"1\", \"2\"}, new int[]{0, 3}, new int[]{1, 4}, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")),\n          new String[]{\"1\", \"2\"}, new int[]{0, 2}, new int[]{1, 3}, consumeAll ? 9 : null);\n\n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  \"),\n          new String[]{\"1\"}, new int[]{0}, new int[]{1}, consumeAll ? 3 : null);\n\n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2  \"),\n          new String[]{\"1\", \"2\"}, new int[]{0, 3}, new int[]{1, 4}, consumeAll ? 6 : null);\n    }\n  }\n\n","sourceOld":"  public void testMaxPosition2() throws IOException {\n    for (final boolean consumeAll : new boolean[] { true, false }) {\n      Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n          // if we are consuming all tokens, we can use the checks, otherwise we can't\n          tokenizer.setEnableChecks(consumeAll);\n          return new TokenStreamComponents(tokenizer, new LimitTokenPositionFilter(tokenizer, 2, consumeAll));\n        }\n      };\n\n      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case its correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2     3  4  5\"), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, consumeAll ? 9 : null);\n\n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  \"),\n                                new String[] { \"1\" }, new int[] { 0 }, new int[] { 1 }, consumeAll ? 3 : null);\n                                                                                   \n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2  \"), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 6 : null);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5eb2511ababf862ea11e10761c70ee560cd84510","date":1396607225,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter#testMaxPosition2().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter#testMaxPosition2().mjava","sourceNew":"  public void testMaxPosition2() throws IOException {\n    for (final boolean consumeAll : new boolean[]{true, false}) {\n      Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n          // if we are consuming all tokens, we can use the checks, otherwise we can't\n          tokenizer.setEnableChecks(consumeAll);\n          return new TokenStreamComponents(tokenizer, new LimitTokenPositionFilter(tokenizer, 2, consumeAll));\n        }\n      };\n\n      // don't use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case its correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2     3  4  5\"),\n          new String[]{\"1\", \"2\"}, new int[]{0, 3}, new int[]{1, 4}, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")),\n          new String[]{\"1\", \"2\"}, new int[]{0, 2}, new int[]{1, 3}, consumeAll ? 9 : null);\n\n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  \"),\n          new String[]{\"1\"}, new int[]{0}, new int[]{1}, consumeAll ? 3 : null);\n\n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2  \"),\n          new String[]{\"1\", \"2\"}, new int[]{0, 3}, new int[]{1, 4}, consumeAll ? 6 : null);\n    }\n  }\n\n","sourceOld":"  public void testMaxPosition2() throws IOException {\n    for (final boolean consumeAll : new boolean[] { true, false }) {\n      Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n          // if we are consuming all tokens, we can use the checks, otherwise we can't\n          tokenizer.setEnableChecks(consumeAll);\n          return new TokenStreamComponents(tokenizer, new LimitTokenPositionFilter(tokenizer, 2, consumeAll));\n        }\n      };\n\n      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case its correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2     3  4  5\"), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, consumeAll ? 9 : null);\n\n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  \"),\n                                new String[] { \"1\" }, new int[] { 0 }, new int[] { 1 }, consumeAll ? 3 : null);\n                                                                                   \n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2  \"), \n                                new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 6 : null);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1","date":1419400138,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter#testMaxPosition2().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter#testMaxPosition2().mjava","sourceNew":"  public void testMaxPosition2() throws IOException {\n    for (final boolean consumeAll : new boolean[]{true, false}) {\n      Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n          // if we are consuming all tokens, we can use the checks, otherwise we can't\n          tokenizer.setEnableChecks(consumeAll);\n          return new TokenStreamComponents(tokenizer, new LimitTokenPositionFilter(tokenizer, 2, consumeAll));\n        }\n      };\n\n      // don't use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case it's correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2     3  4  5\"),\n          new String[]{\"1\", \"2\"}, new int[]{0, 3}, new int[]{1, 4}, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")),\n          new String[]{\"1\", \"2\"}, new int[]{0, 2}, new int[]{1, 3}, consumeAll ? 9 : null);\n\n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  \"),\n          new String[]{\"1\"}, new int[]{0}, new int[]{1}, consumeAll ? 3 : null);\n\n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2  \"),\n          new String[]{\"1\", \"2\"}, new int[]{0, 3}, new int[]{1, 4}, consumeAll ? 6 : null);\n    }\n  }\n\n","sourceOld":"  public void testMaxPosition2() throws IOException {\n    for (final boolean consumeAll : new boolean[]{true, false}) {\n      Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n          // if we are consuming all tokens, we can use the checks, otherwise we can't\n          tokenizer.setEnableChecks(consumeAll);\n          return new TokenStreamComponents(tokenizer, new LimitTokenPositionFilter(tokenizer, 2, consumeAll));\n        }\n      };\n\n      // don't use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case its correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2     3  4  5\"),\n          new String[]{\"1\", \"2\"}, new int[]{0, 3}, new int[]{1, 4}, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")),\n          new String[]{\"1\", \"2\"}, new int[]{0, 2}, new int[]{1, 3}, consumeAll ? 9 : null);\n\n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  \"),\n          new String[]{\"1\"}, new int[]{0}, new int[]{1}, consumeAll ? 3 : null);\n\n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2  \"),\n          new String[]{\"1\", \"2\"}, new int[]{0, 3}, new int[]{1, 4}, consumeAll ? 6 : null);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a56958d7f71a28824f20031ffbb2e13502a0274e","date":1425573902,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter#testMaxPosition2().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter#testMaxPosition2().mjava","sourceNew":"  public void testMaxPosition2() throws IOException {\n    for (final boolean consumeAll : new boolean[]{true, false}) {\n      Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n          // if we are consuming all tokens, we can use the checks, otherwise we can't\n          tokenizer.setEnableChecks(consumeAll);\n          return new TokenStreamComponents(tokenizer, new LimitTokenPositionFilter(tokenizer, 2, consumeAll));\n        }\n      };\n\n      // don't use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case it's correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2     3  4  5\"),\n          new String[]{\"1\", \"2\"}, new int[]{0, 3}, new int[]{1, 4}, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")),\n          new String[]{\"1\", \"2\"}, new int[]{0, 2}, new int[]{1, 3}, consumeAll ? 9 : null);\n\n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  \"),\n          new String[]{\"1\"}, new int[]{0}, new int[]{1}, consumeAll ? 3 : null);\n\n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2  \"),\n          new String[]{\"1\", \"2\"}, new int[]{0, 3}, new int[]{1, 4}, consumeAll ? 6 : null);\n      a.close();\n    }\n  }\n\n","sourceOld":"  public void testMaxPosition2() throws IOException {\n    for (final boolean consumeAll : new boolean[]{true, false}) {\n      Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n          // if we are consuming all tokens, we can use the checks, otherwise we can't\n          tokenizer.setEnableChecks(consumeAll);\n          return new TokenStreamComponents(tokenizer, new LimitTokenPositionFilter(tokenizer, 2, consumeAll));\n        }\n      };\n\n      // don't use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case it's correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2     3  4  5\"),\n          new String[]{\"1\", \"2\"}, new int[]{0, 3}, new int[]{1, 4}, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")),\n          new String[]{\"1\", \"2\"}, new int[]{0, 2}, new int[]{1, 3}, consumeAll ? 9 : null);\n\n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  \"),\n          new String[]{\"1\"}, new int[]{0}, new int[]{1}, consumeAll ? 3 : null);\n\n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2  \"),\n          new String[]{\"1\", \"2\"}, new int[]{0, 3}, new int[]{1, 4}, consumeAll ? 6 : null);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter#testMaxPosition2().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter#testMaxPosition2().mjava","sourceNew":"  public void testMaxPosition2() throws IOException {\n    for (final boolean consumeAll : new boolean[]{true, false}) {\n      Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n          // if we are consuming all tokens, we can use the checks, otherwise we can't\n          tokenizer.setEnableChecks(consumeAll);\n          return new TokenStreamComponents(tokenizer, new LimitTokenPositionFilter(tokenizer, 2, consumeAll));\n        }\n      };\n\n      // don't use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case it's correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2     3  4  5\"),\n          new String[]{\"1\", \"2\"}, new int[]{0, 3}, new int[]{1, 4}, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")),\n          new String[]{\"1\", \"2\"}, new int[]{0, 2}, new int[]{1, 3}, consumeAll ? 9 : null);\n\n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  \"),\n          new String[]{\"1\"}, new int[]{0}, new int[]{1}, consumeAll ? 3 : null);\n\n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2  \"),\n          new String[]{\"1\", \"2\"}, new int[]{0, 3}, new int[]{1, 4}, consumeAll ? 6 : null);\n      a.close();\n    }\n  }\n\n","sourceOld":"  public void testMaxPosition2() throws IOException {\n    for (final boolean consumeAll : new boolean[]{true, false}) {\n      Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n          // if we are consuming all tokens, we can use the checks, otherwise we can't\n          tokenizer.setEnableChecks(consumeAll);\n          return new TokenStreamComponents(tokenizer, new LimitTokenPositionFilter(tokenizer, 2, consumeAll));\n        }\n      };\n\n      // don't use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case it's correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2     3  4  5\"),\n          new String[]{\"1\", \"2\"}, new int[]{0, 3}, new int[]{1, 4}, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")),\n          new String[]{\"1\", \"2\"}, new int[]{0, 2}, new int[]{1, 3}, consumeAll ? 9 : null);\n\n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  \"),\n          new String[]{\"1\"}, new int[]{0}, new int[]{1}, consumeAll ? 3 : null);\n\n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2  \"),\n          new String[]{\"1\", \"2\"}, new int[]{0, 3}, new int[]{1, 4}, consumeAll ? 6 : null);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"5eb2511ababf862ea11e10761c70ee560cd84510":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","43acd3a99a12a5bec9c72097de0e294c80cb6daa"],"fdd61b10b980a6d0b8a8d63baf7e8f5e19e8437a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1","a56958d7f71a28824f20031ffbb2e13502a0274e"],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["43acd3a99a12a5bec9c72097de0e294c80cb6daa"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["fdd61b10b980a6d0b8a8d63baf7e8f5e19e8437a","c83d6c4335f31cae14f625a222bc842f20073dcd"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["c83d6c4335f31cae14f625a222bc842f20073dcd"],"c83d6c4335f31cae14f625a222bc842f20073dcd":["fdd61b10b980a6d0b8a8d63baf7e8f5e19e8437a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a56958d7f71a28824f20031ffbb2e13502a0274e":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"43acd3a99a12a5bec9c72097de0e294c80cb6daa":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a56958d7f71a28824f20031ffbb2e13502a0274e"]},"commit2Childs":{"5eb2511ababf862ea11e10761c70ee560cd84510":[],"fdd61b10b980a6d0b8a8d63baf7e8f5e19e8437a":["37a0f60745e53927c4c876cfe5b5a58170f0646c","c83d6c4335f31cae14f625a222bc842f20073dcd"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","a56958d7f71a28824f20031ffbb2e13502a0274e"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["5eb2511ababf862ea11e10761c70ee560cd84510","43acd3a99a12a5bec9c72097de0e294c80cb6daa"],"c83d6c4335f31cae14f625a222bc842f20073dcd":["37a0f60745e53927c4c876cfe5b5a58170f0646c","ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["fdd61b10b980a6d0b8a8d63baf7e8f5e19e8437a"],"43acd3a99a12a5bec9c72097de0e294c80cb6daa":["5eb2511ababf862ea11e10761c70ee560cd84510","8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"a56958d7f71a28824f20031ffbb2e13502a0274e":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5eb2511ababf862ea11e10761c70ee560cd84510","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","37a0f60745e53927c4c876cfe5b5a58170f0646c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}