{"path":"lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.Writer#merge(MergeState).mjava","commits":[{"id":"3b7a068f550e13e49517c6899cc3b94c8eeb72e5","date":1309354772,"type":0,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.Writer#merge(MergeState).mjava","pathOld":"/dev/null","sourceNew":"    @Override\n    protected void merge(MergeState state) throws IOException {\n      merge = true;\n      datOut = getDataOut();\n      boolean success = false;\n      try {\n        if (state.bits == null && state.reader instanceof Reader) {\n          // bulk merge since we don't have any deletes\n          Reader reader = (Reader) state.reader;\n          final int maxDocs = reader.maxDoc;\n          if (maxDocs == 0) {\n            return;\n          }\n          if (lastDocID+1 < state.docBase) {\n            fill(state.docBase);\n            lastDocID = state.docBase-1;\n          }\n          final long numDataBytes;\n          final IndexInput cloneIdx = reader.cloneIndex();\n          try {\n            numDataBytes = cloneIdx.readVLong();\n            final ReaderIterator iter = PackedInts.getReaderIterator(cloneIdx);\n            for (int i = 0; i < maxDocs; i++) {\n              long offset = iter.next();\n              ++lastDocID;\n              if (lastDocID >= docToAddress.length) {\n                int oldSize = docToAddress.length;\n                docToAddress = ArrayUtil.grow(docToAddress, 1 + lastDocID);\n                bytesUsed.addAndGet((docToAddress.length - oldSize)\n                    * RamUsageEstimator.NUM_BYTES_INT);\n              }\n              docToAddress[lastDocID] = address + offset;\n            }\n            address += numDataBytes; // this is the address after all addr pointers are updated\n            iter.close();\n          } finally {\n            IOUtils.closeSafely(true, cloneIdx);\n          }\n          final IndexInput cloneData = reader.cloneData();\n          try {\n            datOut.copyBytes(cloneData, numDataBytes);\n          } finally {\n            IOUtils.closeSafely(true, cloneData);  \n          }\n        } else {\n          super.merge(state);\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          IOUtils.closeSafely(!success, datOut);\n        }\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["733b83db0afdfe169b80c8580043ceca57967b19"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","date":1309960478,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.Writer#merge(MergeState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.Writer#merge(MergeState).mjava","sourceNew":"    @Override\n    protected void merge(MergeState state) throws IOException {\n      merge = true;\n      datOut = getDataOut();\n      boolean success = false;\n      try {\n        if (state.liveDocs == null && state.reader instanceof Reader) {\n          // bulk merge since we don't have any deletes\n          Reader reader = (Reader) state.reader;\n          final int maxDocs = reader.maxDoc;\n          if (maxDocs == 0) {\n            return;\n          }\n          if (lastDocID+1 < state.docBase) {\n            fill(state.docBase);\n            lastDocID = state.docBase-1;\n          }\n          final long numDataBytes;\n          final IndexInput cloneIdx = reader.cloneIndex();\n          try {\n            numDataBytes = cloneIdx.readVLong();\n            final ReaderIterator iter = PackedInts.getReaderIterator(cloneIdx);\n            for (int i = 0; i < maxDocs; i++) {\n              long offset = iter.next();\n              ++lastDocID;\n              if (lastDocID >= docToAddress.length) {\n                int oldSize = docToAddress.length;\n                docToAddress = ArrayUtil.grow(docToAddress, 1 + lastDocID);\n                bytesUsed.addAndGet((docToAddress.length - oldSize)\n                    * RamUsageEstimator.NUM_BYTES_INT);\n              }\n              docToAddress[lastDocID] = address + offset;\n            }\n            address += numDataBytes; // this is the address after all addr pointers are updated\n            iter.close();\n          } finally {\n            IOUtils.closeSafely(true, cloneIdx);\n          }\n          final IndexInput cloneData = reader.cloneData();\n          try {\n            datOut.copyBytes(cloneData, numDataBytes);\n          } finally {\n            IOUtils.closeSafely(true, cloneData);  \n          }\n        } else {\n          super.merge(state);\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          IOUtils.closeSafely(!success, datOut);\n        }\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void merge(MergeState state) throws IOException {\n      merge = true;\n      datOut = getDataOut();\n      boolean success = false;\n      try {\n        if (state.bits == null && state.reader instanceof Reader) {\n          // bulk merge since we don't have any deletes\n          Reader reader = (Reader) state.reader;\n          final int maxDocs = reader.maxDoc;\n          if (maxDocs == 0) {\n            return;\n          }\n          if (lastDocID+1 < state.docBase) {\n            fill(state.docBase);\n            lastDocID = state.docBase-1;\n          }\n          final long numDataBytes;\n          final IndexInput cloneIdx = reader.cloneIndex();\n          try {\n            numDataBytes = cloneIdx.readVLong();\n            final ReaderIterator iter = PackedInts.getReaderIterator(cloneIdx);\n            for (int i = 0; i < maxDocs; i++) {\n              long offset = iter.next();\n              ++lastDocID;\n              if (lastDocID >= docToAddress.length) {\n                int oldSize = docToAddress.length;\n                docToAddress = ArrayUtil.grow(docToAddress, 1 + lastDocID);\n                bytesUsed.addAndGet((docToAddress.length - oldSize)\n                    * RamUsageEstimator.NUM_BYTES_INT);\n              }\n              docToAddress[lastDocID] = address + offset;\n            }\n            address += numDataBytes; // this is the address after all addr pointers are updated\n            iter.close();\n          } finally {\n            IOUtils.closeSafely(true, cloneIdx);\n          }\n          final IndexInput cloneData = reader.cloneData();\n          try {\n            datOut.copyBytes(cloneData, numDataBytes);\n          } finally {\n            IOUtils.closeSafely(true, cloneData);  \n          }\n        } else {\n          super.merge(state);\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          IOUtils.closeSafely(!success, datOut);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":0,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.Writer#merge(MergeState).mjava","pathOld":"/dev/null","sourceNew":"    @Override\n    protected void merge(MergeState state) throws IOException {\n      merge = true;\n      datOut = getDataOut();\n      boolean success = false;\n      try {\n        if (state.liveDocs == null && state.reader instanceof Reader) {\n          // bulk merge since we don't have any deletes\n          Reader reader = (Reader) state.reader;\n          final int maxDocs = reader.maxDoc;\n          if (maxDocs == 0) {\n            return;\n          }\n          if (lastDocID+1 < state.docBase) {\n            fill(state.docBase);\n            lastDocID = state.docBase-1;\n          }\n          final long numDataBytes;\n          final IndexInput cloneIdx = reader.cloneIndex();\n          try {\n            numDataBytes = cloneIdx.readVLong();\n            final ReaderIterator iter = PackedInts.getReaderIterator(cloneIdx);\n            for (int i = 0; i < maxDocs; i++) {\n              long offset = iter.next();\n              ++lastDocID;\n              if (lastDocID >= docToAddress.length) {\n                int oldSize = docToAddress.length;\n                docToAddress = ArrayUtil.grow(docToAddress, 1 + lastDocID);\n                bytesUsed.addAndGet((docToAddress.length - oldSize)\n                    * RamUsageEstimator.NUM_BYTES_INT);\n              }\n              docToAddress[lastDocID] = address + offset;\n            }\n            address += numDataBytes; // this is the address after all addr pointers are updated\n            iter.close();\n          } finally {\n            IOUtils.closeSafely(true, cloneIdx);\n          }\n          final IndexInput cloneData = reader.cloneData();\n          try {\n            datOut.copyBytes(cloneData, numDataBytes);\n          } finally {\n            IOUtils.closeSafely(true, cloneData);  \n          }\n        } else {\n          super.merge(state);\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          IOUtils.closeSafely(!success, datOut);\n        }\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.Writer#merge(MergeState).mjava","pathOld":"/dev/null","sourceNew":"    @Override\n    protected void merge(MergeState state) throws IOException {\n      merge = true;\n      datOut = getDataOut();\n      boolean success = false;\n      try {\n        if (state.liveDocs == null && state.reader instanceof Reader) {\n          // bulk merge since we don't have any deletes\n          Reader reader = (Reader) state.reader;\n          final int maxDocs = reader.maxDoc;\n          if (maxDocs == 0) {\n            return;\n          }\n          if (lastDocID+1 < state.docBase) {\n            fill(state.docBase);\n            lastDocID = state.docBase-1;\n          }\n          final long numDataBytes;\n          final IndexInput cloneIdx = reader.cloneIndex();\n          try {\n            numDataBytes = cloneIdx.readVLong();\n            final ReaderIterator iter = PackedInts.getReaderIterator(cloneIdx);\n            for (int i = 0; i < maxDocs; i++) {\n              long offset = iter.next();\n              ++lastDocID;\n              if (lastDocID >= docToAddress.length) {\n                int oldSize = docToAddress.length;\n                docToAddress = ArrayUtil.grow(docToAddress, 1 + lastDocID);\n                bytesUsed.addAndGet((docToAddress.length - oldSize)\n                    * RamUsageEstimator.NUM_BYTES_INT);\n              }\n              docToAddress[lastDocID] = address + offset;\n            }\n            address += numDataBytes; // this is the address after all addr pointers are updated\n            iter.close();\n          } finally {\n            IOUtils.closeSafely(true, cloneIdx);\n          }\n          final IndexInput cloneData = reader.cloneData();\n          try {\n            datOut.copyBytes(cloneData, numDataBytes);\n          } finally {\n            IOUtils.closeSafely(true, cloneData);  \n          }\n        } else {\n          super.merge(state);\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          IOUtils.closeSafely(!success, datOut);\n        }\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"733b83db0afdfe169b80c8580043ceca57967b19","date":1314458101,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.Writer#merge(MergeState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.Writer#merge(MergeState).mjava","sourceNew":"    @Override\n    protected void merge(MergeState state) throws IOException {\n      merge = true;\n      datOut = getDataOut();\n      boolean success = false;\n      try {\n        if (state.liveDocs == null && state.reader instanceof Reader) {\n          // bulk merge since we don't have any deletes\n          Reader reader = (Reader) state.reader;\n          final int maxDocs = reader.maxDoc;\n          if (maxDocs == 0) {\n            return;\n          }\n          if (lastDocID+1 < state.docBase) {\n            fill(state.docBase);\n            lastDocID = state.docBase-1;\n          }\n          final long numDataBytes;\n          final IndexInput cloneIdx = reader.cloneIndex();\n          try {\n            numDataBytes = cloneIdx.readVLong();\n            final ReaderIterator iter = PackedInts.getReaderIterator(cloneIdx);\n            for (int i = 0; i < maxDocs; i++) {\n              long offset = iter.next();\n              ++lastDocID;\n              if (lastDocID >= docToAddress.length) {\n                int oldSize = docToAddress.length;\n                docToAddress = ArrayUtil.grow(docToAddress, 1 + lastDocID);\n                bytesUsed.addAndGet((docToAddress.length - oldSize)\n                    * RamUsageEstimator.NUM_BYTES_INT);\n              }\n              docToAddress[lastDocID] = address + offset;\n            }\n            address += numDataBytes; // this is the address after all addr pointers are updated\n            iter.close();\n          } finally {\n            IOUtils.closeSafely(false, cloneIdx);\n          }\n          final IndexInput cloneData = reader.cloneData();\n          try {\n            datOut.copyBytes(cloneData, numDataBytes);\n          } finally {\n            IOUtils.closeSafely(false, cloneData);  \n          }\n        } else {\n          super.merge(state);\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          IOUtils.closeSafely(!success, datOut);\n        }\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void merge(MergeState state) throws IOException {\n      merge = true;\n      datOut = getDataOut();\n      boolean success = false;\n      try {\n        if (state.liveDocs == null && state.reader instanceof Reader) {\n          // bulk merge since we don't have any deletes\n          Reader reader = (Reader) state.reader;\n          final int maxDocs = reader.maxDoc;\n          if (maxDocs == 0) {\n            return;\n          }\n          if (lastDocID+1 < state.docBase) {\n            fill(state.docBase);\n            lastDocID = state.docBase-1;\n          }\n          final long numDataBytes;\n          final IndexInput cloneIdx = reader.cloneIndex();\n          try {\n            numDataBytes = cloneIdx.readVLong();\n            final ReaderIterator iter = PackedInts.getReaderIterator(cloneIdx);\n            for (int i = 0; i < maxDocs; i++) {\n              long offset = iter.next();\n              ++lastDocID;\n              if (lastDocID >= docToAddress.length) {\n                int oldSize = docToAddress.length;\n                docToAddress = ArrayUtil.grow(docToAddress, 1 + lastDocID);\n                bytesUsed.addAndGet((docToAddress.length - oldSize)\n                    * RamUsageEstimator.NUM_BYTES_INT);\n              }\n              docToAddress[lastDocID] = address + offset;\n            }\n            address += numDataBytes; // this is the address after all addr pointers are updated\n            iter.close();\n          } finally {\n            IOUtils.closeSafely(true, cloneIdx);\n          }\n          final IndexInput cloneData = reader.cloneData();\n          try {\n            datOut.copyBytes(cloneData, numDataBytes);\n          } finally {\n            IOUtils.closeSafely(true, cloneData);  \n          }\n        } else {\n          super.merge(state);\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          IOUtils.closeSafely(!success, datOut);\n        }\n      }\n    }\n\n","bugFix":["3b7a068f550e13e49517c6899cc3b94c8eeb72e5"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"24230fe54121f9be9d85f2c2067536296785e421","date":1314462346,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.Writer#merge(MergeState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.Writer#merge(MergeState).mjava","sourceNew":"    @Override\n    protected void merge(MergeState state) throws IOException {\n      merge = true;\n      datOut = getDataOut();\n      boolean success = false;\n      try {\n        if (state.liveDocs == null && state.reader instanceof Reader) {\n          // bulk merge since we don't have any deletes\n          Reader reader = (Reader) state.reader;\n          final int maxDocs = reader.maxDoc;\n          if (maxDocs == 0) {\n            return;\n          }\n          if (lastDocID+1 < state.docBase) {\n            fill(state.docBase);\n            lastDocID = state.docBase-1;\n          }\n          final long numDataBytes;\n          final IndexInput cloneIdx = reader.cloneIndex();\n          try {\n            numDataBytes = cloneIdx.readVLong();\n            final ReaderIterator iter = PackedInts.getReaderIterator(cloneIdx);\n            for (int i = 0; i < maxDocs; i++) {\n              long offset = iter.next();\n              ++lastDocID;\n              if (lastDocID >= docToAddress.length) {\n                int oldSize = docToAddress.length;\n                docToAddress = ArrayUtil.grow(docToAddress, 1 + lastDocID);\n                bytesUsed.addAndGet((docToAddress.length - oldSize)\n                    * RamUsageEstimator.NUM_BYTES_INT);\n              }\n              docToAddress[lastDocID] = address + offset;\n            }\n            address += numDataBytes; // this is the address after all addr pointers are updated\n            iter.close();\n          } finally {\n            IOUtils.close(cloneIdx);\n          }\n          final IndexInput cloneData = reader.cloneData();\n          try {\n            datOut.copyBytes(cloneData, numDataBytes);\n          } finally {\n            IOUtils.close(cloneData);  \n          }\n        } else {\n          super.merge(state);\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          IOUtils.closeWhileHandlingException(datOut);\n        }\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void merge(MergeState state) throws IOException {\n      merge = true;\n      datOut = getDataOut();\n      boolean success = false;\n      try {\n        if (state.liveDocs == null && state.reader instanceof Reader) {\n          // bulk merge since we don't have any deletes\n          Reader reader = (Reader) state.reader;\n          final int maxDocs = reader.maxDoc;\n          if (maxDocs == 0) {\n            return;\n          }\n          if (lastDocID+1 < state.docBase) {\n            fill(state.docBase);\n            lastDocID = state.docBase-1;\n          }\n          final long numDataBytes;\n          final IndexInput cloneIdx = reader.cloneIndex();\n          try {\n            numDataBytes = cloneIdx.readVLong();\n            final ReaderIterator iter = PackedInts.getReaderIterator(cloneIdx);\n            for (int i = 0; i < maxDocs; i++) {\n              long offset = iter.next();\n              ++lastDocID;\n              if (lastDocID >= docToAddress.length) {\n                int oldSize = docToAddress.length;\n                docToAddress = ArrayUtil.grow(docToAddress, 1 + lastDocID);\n                bytesUsed.addAndGet((docToAddress.length - oldSize)\n                    * RamUsageEstimator.NUM_BYTES_INT);\n              }\n              docToAddress[lastDocID] = address + offset;\n            }\n            address += numDataBytes; // this is the address after all addr pointers are updated\n            iter.close();\n          } finally {\n            IOUtils.closeSafely(false, cloneIdx);\n          }\n          final IndexInput cloneData = reader.cloneData();\n          try {\n            datOut.copyBytes(cloneData, numDataBytes);\n          } finally {\n            IOUtils.closeSafely(false, cloneData);  \n          }\n        } else {\n          super.merge(state);\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          IOUtils.closeSafely(!success, datOut);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"85eb75e0c0203e44dcf686f35876cf6080f3a671","date":1317221550,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.Writer#merge(MergeState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.Writer#merge(MergeState).mjava","sourceNew":"    @Override\n    protected void merge(MergeState state) throws IOException {\n      merge = true;\n      datOut = getOrCreateDataOut();\n      boolean success = false;\n      try {\n        if (state.liveDocs == null && state.reader instanceof Reader) {\n          // bulk merge since we don't have any deletes\n          Reader reader = (Reader) state.reader;\n          final int maxDocs = reader.maxDoc;\n          if (maxDocs == 0) {\n            return;\n          }\n          if (lastDocID+1 < state.docBase) {\n            fill(state.docBase);\n            lastDocID = state.docBase-1;\n          }\n          final long numDataBytes;\n          final IndexInput cloneIdx = reader.cloneIndex();\n          try {\n            numDataBytes = cloneIdx.readVLong();\n            final ReaderIterator iter = PackedInts.getReaderIterator(cloneIdx);\n            for (int i = 0; i < maxDocs; i++) {\n              long offset = iter.next();\n              ++lastDocID;\n              if (lastDocID >= docToAddress.length) {\n                int oldSize = docToAddress.length;\n                docToAddress = ArrayUtil.grow(docToAddress, 1 + lastDocID);\n                bytesUsed.addAndGet((docToAddress.length - oldSize)\n                    * RamUsageEstimator.NUM_BYTES_INT);\n              }\n              docToAddress[lastDocID] = address + offset;\n            }\n            address += numDataBytes; // this is the address after all addr pointers are updated\n            iter.close();\n          } finally {\n            IOUtils.close(cloneIdx);\n          }\n          final IndexInput cloneData = reader.cloneData();\n          try {\n            datOut.copyBytes(cloneData, numDataBytes);\n          } finally {\n            IOUtils.close(cloneData);  \n          }\n        } else {\n          super.merge(state);\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          IOUtils.closeWhileHandlingException(datOut);\n        }\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void merge(MergeState state) throws IOException {\n      merge = true;\n      datOut = getDataOut();\n      boolean success = false;\n      try {\n        if (state.liveDocs == null && state.reader instanceof Reader) {\n          // bulk merge since we don't have any deletes\n          Reader reader = (Reader) state.reader;\n          final int maxDocs = reader.maxDoc;\n          if (maxDocs == 0) {\n            return;\n          }\n          if (lastDocID+1 < state.docBase) {\n            fill(state.docBase);\n            lastDocID = state.docBase-1;\n          }\n          final long numDataBytes;\n          final IndexInput cloneIdx = reader.cloneIndex();\n          try {\n            numDataBytes = cloneIdx.readVLong();\n            final ReaderIterator iter = PackedInts.getReaderIterator(cloneIdx);\n            for (int i = 0; i < maxDocs; i++) {\n              long offset = iter.next();\n              ++lastDocID;\n              if (lastDocID >= docToAddress.length) {\n                int oldSize = docToAddress.length;\n                docToAddress = ArrayUtil.grow(docToAddress, 1 + lastDocID);\n                bytesUsed.addAndGet((docToAddress.length - oldSize)\n                    * RamUsageEstimator.NUM_BYTES_INT);\n              }\n              docToAddress[lastDocID] = address + offset;\n            }\n            address += numDataBytes; // this is the address after all addr pointers are updated\n            iter.close();\n          } finally {\n            IOUtils.close(cloneIdx);\n          }\n          final IndexInput cloneData = reader.cloneData();\n          try {\n            datOut.copyBytes(cloneData, numDataBytes);\n          } finally {\n            IOUtils.close(cloneData);  \n          }\n        } else {\n          super.merge(state);\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          IOUtils.closeWhileHandlingException(datOut);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e128967bca58657bc0039d4bfe631e63e81f1977","date":1317978310,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.Writer#merge(MergeState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.Writer#merge(MergeState).mjava","sourceNew":"    @Override\n    protected void merge(MergeState state) throws IOException {\n      merge = true;\n      datOut = getOrCreateDataOut();\n      boolean success = false;\n      try {\n        if (state.liveDocs == null && state.reader instanceof VarStraightReader) {\n          // bulk merge since we don't have any deletes\n          VarStraightReader reader = (VarStraightReader) state.reader;\n          final int maxDocs = reader.maxDoc;\n          if (maxDocs == 0) {\n            return;\n          }\n          if (lastDocID+1 < state.docBase) {\n            fill(state.docBase, address);\n            lastDocID = state.docBase-1;\n          }\n          final long numDataBytes;\n          final IndexInput cloneIdx = reader.cloneIndex();\n          try {\n            numDataBytes = cloneIdx.readVLong();\n            final ReaderIterator iter = PackedInts.getReaderIterator(cloneIdx);\n            for (int i = 0; i < maxDocs; i++) {\n              long offset = iter.next();\n              ++lastDocID;\n              if (lastDocID >= docToAddress.length) {\n                int oldSize = docToAddress.length;\n                docToAddress = ArrayUtil.grow(docToAddress, 1 + lastDocID);\n                bytesUsed.addAndGet((docToAddress.length - oldSize)\n                    * RamUsageEstimator.NUM_BYTES_INT);\n              }\n              docToAddress[lastDocID] = address + offset;\n            }\n            address += numDataBytes; // this is the address after all addr pointers are updated\n            iter.close();\n          } finally {\n            IOUtils.close(cloneIdx);\n          }\n          final IndexInput cloneData = reader.cloneData();\n          try {\n            datOut.copyBytes(cloneData, numDataBytes);\n          } finally {\n            IOUtils.close(cloneData);  \n          }\n        } else {\n          super.merge(state);\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          IOUtils.closeWhileHandlingException(datOut);\n        }\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void merge(MergeState state) throws IOException {\n      merge = true;\n      datOut = getOrCreateDataOut();\n      boolean success = false;\n      try {\n        if (state.liveDocs == null && state.reader instanceof Reader) {\n          // bulk merge since we don't have any deletes\n          Reader reader = (Reader) state.reader;\n          final int maxDocs = reader.maxDoc;\n          if (maxDocs == 0) {\n            return;\n          }\n          if (lastDocID+1 < state.docBase) {\n            fill(state.docBase);\n            lastDocID = state.docBase-1;\n          }\n          final long numDataBytes;\n          final IndexInput cloneIdx = reader.cloneIndex();\n          try {\n            numDataBytes = cloneIdx.readVLong();\n            final ReaderIterator iter = PackedInts.getReaderIterator(cloneIdx);\n            for (int i = 0; i < maxDocs; i++) {\n              long offset = iter.next();\n              ++lastDocID;\n              if (lastDocID >= docToAddress.length) {\n                int oldSize = docToAddress.length;\n                docToAddress = ArrayUtil.grow(docToAddress, 1 + lastDocID);\n                bytesUsed.addAndGet((docToAddress.length - oldSize)\n                    * RamUsageEstimator.NUM_BYTES_INT);\n              }\n              docToAddress[lastDocID] = address + offset;\n            }\n            address += numDataBytes; // this is the address after all addr pointers are updated\n            iter.close();\n          } finally {\n            IOUtils.close(cloneIdx);\n          }\n          final IndexInput cloneData = reader.cloneData();\n          try {\n            datOut.copyBytes(cloneData, numDataBytes);\n          } finally {\n            IOUtils.close(cloneData);  \n          }\n        } else {\n          super.merge(state);\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          IOUtils.closeWhileHandlingException(datOut);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"32aca6bb0a6aa0a1813e7d035ac0e039f54269f4","date":1318260487,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.Writer#merge(SingleSubMergeState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.Writer#merge(MergeState).mjava","sourceNew":"    @Override\n    protected void merge(SingleSubMergeState state) throws IOException {\n      merge = true;\n      datOut = getOrCreateDataOut();\n      boolean success = false;\n      try {\n        if (state.liveDocs == null && state.reader instanceof VarStraightReader) {\n          // bulk merge since we don't have any deletes\n          VarStraightReader reader = (VarStraightReader) state.reader;\n          final int maxDocs = reader.maxDoc;\n          if (maxDocs == 0) {\n            return;\n          }\n          if (lastDocID+1 < state.docBase) {\n            fill(state.docBase, address);\n            lastDocID = state.docBase-1;\n          }\n          final long numDataBytes;\n          final IndexInput cloneIdx = reader.cloneIndex();\n          try {\n            numDataBytes = cloneIdx.readVLong();\n            final ReaderIterator iter = PackedInts.getReaderIterator(cloneIdx);\n            for (int i = 0; i < maxDocs; i++) {\n              long offset = iter.next();\n              ++lastDocID;\n              if (lastDocID >= docToAddress.length) {\n                int oldSize = docToAddress.length;\n                docToAddress = ArrayUtil.grow(docToAddress, 1 + lastDocID);\n                bytesUsed.addAndGet((docToAddress.length - oldSize)\n                    * RamUsageEstimator.NUM_BYTES_INT);\n              }\n              docToAddress[lastDocID] = address + offset;\n            }\n            address += numDataBytes; // this is the address after all addr pointers are updated\n            iter.close();\n          } finally {\n            IOUtils.close(cloneIdx);\n          }\n          final IndexInput cloneData = reader.cloneData();\n          try {\n            datOut.copyBytes(cloneData, numDataBytes);\n          } finally {\n            IOUtils.close(cloneData);  \n          }\n        } else {\n          super.merge(state);\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          IOUtils.closeWhileHandlingException(datOut);\n        }\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void merge(MergeState state) throws IOException {\n      merge = true;\n      datOut = getOrCreateDataOut();\n      boolean success = false;\n      try {\n        if (state.liveDocs == null && state.reader instanceof VarStraightReader) {\n          // bulk merge since we don't have any deletes\n          VarStraightReader reader = (VarStraightReader) state.reader;\n          final int maxDocs = reader.maxDoc;\n          if (maxDocs == 0) {\n            return;\n          }\n          if (lastDocID+1 < state.docBase) {\n            fill(state.docBase, address);\n            lastDocID = state.docBase-1;\n          }\n          final long numDataBytes;\n          final IndexInput cloneIdx = reader.cloneIndex();\n          try {\n            numDataBytes = cloneIdx.readVLong();\n            final ReaderIterator iter = PackedInts.getReaderIterator(cloneIdx);\n            for (int i = 0; i < maxDocs; i++) {\n              long offset = iter.next();\n              ++lastDocID;\n              if (lastDocID >= docToAddress.length) {\n                int oldSize = docToAddress.length;\n                docToAddress = ArrayUtil.grow(docToAddress, 1 + lastDocID);\n                bytesUsed.addAndGet((docToAddress.length - oldSize)\n                    * RamUsageEstimator.NUM_BYTES_INT);\n              }\n              docToAddress[lastDocID] = address + offset;\n            }\n            address += numDataBytes; // this is the address after all addr pointers are updated\n            iter.close();\n          } finally {\n            IOUtils.close(cloneIdx);\n          }\n          final IndexInput cloneData = reader.cloneData();\n          try {\n            datOut.copyBytes(cloneData, numDataBytes);\n          } finally {\n            IOUtils.close(cloneData);  \n          }\n        } else {\n          super.merge(state);\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          IOUtils.closeWhileHandlingException(datOut);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"24230fe54121f9be9d85f2c2067536296785e421":["733b83db0afdfe169b80c8580043ceca57967b19"],"733b83db0afdfe169b80c8580043ceca57967b19":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"85eb75e0c0203e44dcf686f35876cf6080f3a671":["24230fe54121f9be9d85f2c2067536296785e421"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["3b7a068f550e13e49517c6899cc3b94c8eeb72e5"],"3b7a068f550e13e49517c6899cc3b94c8eeb72e5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e128967bca58657bc0039d4bfe631e63e81f1977":["85eb75e0c0203e44dcf686f35876cf6080f3a671"],"32aca6bb0a6aa0a1813e7d035ac0e039f54269f4":["e128967bca58657bc0039d4bfe631e63e81f1977"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["32aca6bb0a6aa0a1813e7d035ac0e039f54269f4"]},"commit2Childs":{"24230fe54121f9be9d85f2c2067536296785e421":["85eb75e0c0203e44dcf686f35876cf6080f3a671"],"733b83db0afdfe169b80c8580043ceca57967b19":["24230fe54121f9be9d85f2c2067536296785e421"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["733b83db0afdfe169b80c8580043ceca57967b19","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88"],"85eb75e0c0203e44dcf686f35876cf6080f3a671":["e128967bca58657bc0039d4bfe631e63e81f1977"],"3b7a068f550e13e49517c6899cc3b94c8eeb72e5":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3b7a068f550e13e49517c6899cc3b94c8eeb72e5","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88"],"e128967bca58657bc0039d4bfe631e63e81f1977":["32aca6bb0a6aa0a1813e7d035ac0e039f54269f4"],"32aca6bb0a6aa0a1813e7d035ac0e039f54269f4":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"817d8435e9135b756f08ce6710ab0baac51bdf88":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}