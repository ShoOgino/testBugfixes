{"path":"backwards/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","commits":[{"id":"480d01e5b0ef8efb136d51670fec297ae5ae2c9c","date":1268821447,"type":0,"author":"Uwe Schindler","isMerge":false,"pathNew":"backwards/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"/dev/null","sourceNew":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer();\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          if (i == -1) {\n            throw new IOException(\"read past EOF\");\n          }\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/backwards/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"backwards/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer();\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          if (i == -1) {\n            throw new IOException(\"read past EOF\");\n          }\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer();\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          if (i == -1) {\n            throw new IOException(\"read past EOF\");\n          }\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"480d01e5b0ef8efb136d51670fec297ae5ae2c9c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["480d01e5b0ef8efb136d51670fec297ae5ae2c9c"]},"commit2Childs":{"480d01e5b0ef8efb136d51670fec297ae5ae2c9c":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["480d01e5b0ef8efb136d51670fec297ae5ae2c9c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}