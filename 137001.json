{"path":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","commits":[{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    final CharsRef charsRef = new CharsRef();\n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    final CharsRef charsRef = new CharsRef();\n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    final CharsRef charsRef = new CharsRef();\n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    final CharsRef charsRef = new CharsRef();\n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":1,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    final CharsRef charsRef = new CharsRef();\n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    final CharsRef charsRef = new CharsRef();\n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"11982a7b6834a8571852448312db4624c32990b5","date":1321300684,"type":3,"author":"Ryan McKinley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, sf, facet_sf, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    final CharsRef charsRef = new CharsRef();\n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2dd6ecb8250c497ed227653279d6a4f470bfbb31","date":1326814483,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(new SlowMultiReaderWrapper(searcher.getIndexReader()), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, sf, facet_sf, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, sf, facet_sf, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"59fc0e55b44c555c39d950def9414b5596c6ebe2","date":1327620010,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(new SlowMultiReaderWrapper(searcher.getIndexReader()), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, sf, facet_sf, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(new SlowMultiReaderWrapper(searcher.getIndexReader()));\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(new SlowMultiReaderWrapper(searcher.getIndexReader()), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, sf, facet_sf, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"78a55f24d9b493c2a1cecf79f1d78279062b545b","date":1327688152,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(new SlowMultiReaderWrapper(searcher.getIndexReader()), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, sf, facet_sf, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(new SlowMultiReaderWrapper(searcher.getIndexReader()));\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(new SlowMultiReaderWrapper(searcher.getIndexReader()), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, sf, facet_sf, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fd92b8bcc88e969302510acf77bd6970da3994c4","date":1327839530,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(new SlowMultiReaderWrapper(searcher.getIndexReader()), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, sf, facet_sf, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(new SlowMultiReaderWrapper(searcher.getIndexReader()));\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(new SlowMultiReaderWrapper(searcher.getIndexReader()), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, sf, facet_sf, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"96d207426bd26fa5c1014e26d21d87603aea68b7","date":1327944562,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, sf, facet_sf, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(new SlowMultiReaderWrapper(searcher.getIndexReader()), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, sf, facet_sf, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(new SlowMultiReaderWrapper(searcher.getIndexReader()));\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, sf, facet_sf, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(new SlowMultiReaderWrapper(searcher.getIndexReader()), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, sf, facet_sf, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(new SlowMultiReaderWrapper(searcher.getIndexReader()));\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c7bd1fdddb8e84c1857d1a55c32ced51f0ed2069","date":1348430063,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, sf, facet_sf, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, sf, facet_sf, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9d7e5f3aa5935964617824d1f9b2599ddb334464","date":1353762831,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    SortedDocValues si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, sf, facet_sf, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, sf, facet_sf, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    SortedDocValues si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, sf, facet_sf, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, sf, facet_sf, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":["be20f9fed1d3edcb1c84abcc39df87a90fab22df"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"73bb5a57dc75b54a39494f99986599cae7dff417","date":1361040620,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    SortedDocValues si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      finfo[i] = new FieldFacetStats(searcher, f, sf, facet_sf);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    SortedDocValues si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, sf, facet_sf, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"95303ff3749680c743b9425f9cf99e6e4065e8a8","date":1361061922,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    SortedDocValues si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      finfo[i] = new FieldFacetStats(searcher, f, sf, facet_sf);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    SortedDocValues si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, sf, facet_sf, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"19275ba31e621f6da1b83bf13af75233876fd3d4","date":1374846698,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    SortedDocValues si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      finfo[i] = new FieldFacetStats(searcher, f, sf, facet_sf);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    SortedDocValues si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      finfo[i] = new FieldFacetStats(searcher, f, sf, facet_sf);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","date":1376375609,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    SortedDocValues si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      finfo[i] = new FieldFacetStats(searcher, f, sf, facet_sf);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    SortedDocValues si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      finfo[i] = new FieldFacetStats(searcher, f, sf, facet_sf);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bf795ee457272965bd751f513787065bbf0a650a","date":1385015231,"type":5,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,boolean,String[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param calcDistinct whether distinct values should be collected and counted\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, boolean calcDistinct, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf, calcDistinct);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    SortedDocValues si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      finfo[i] = new FieldFacetStats(searcher, f, sf, facet_sf, calcDistinct);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    SortedDocValues si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      finfo[i] = new FieldFacetStats(searcher, f, sf, facet_sf);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"74f45af4339b0daf7a95c820ab88c1aea74fbce0","date":1387475327,"type":4,"author":"Michael McCandless","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":null,"sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    SortedDocValues si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      finfo[i] = new FieldFacetStats(searcher, f, sf, facet_sf);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c7bd1fdddb8e84c1857d1a55c32ced51f0ed2069":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"78a55f24d9b493c2a1cecf79f1d78279062b545b":["2dd6ecb8250c497ed227653279d6a4f470bfbb31","59fc0e55b44c555c39d950def9414b5596c6ebe2"],"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec":["73bb5a57dc75b54a39494f99986599cae7dff417","19275ba31e621f6da1b83bf13af75233876fd3d4"],"c26f00b574427b55127e869b935845554afde1fa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"96d207426bd26fa5c1014e26d21d87603aea68b7":["fd92b8bcc88e969302510acf77bd6970da3994c4"],"fd92b8bcc88e969302510acf77bd6970da3994c4":["2dd6ecb8250c497ed227653279d6a4f470bfbb31","59fc0e55b44c555c39d950def9414b5596c6ebe2"],"19275ba31e621f6da1b83bf13af75233876fd3d4":["73bb5a57dc75b54a39494f99986599cae7dff417"],"bf795ee457272965bd751f513787065bbf0a650a":["19275ba31e621f6da1b83bf13af75233876fd3d4"],"2dd6ecb8250c497ed227653279d6a4f470bfbb31":["11982a7b6834a8571852448312db4624c32990b5"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["59fc0e55b44c555c39d950def9414b5596c6ebe2","96d207426bd26fa5c1014e26d21d87603aea68b7"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["c7bd1fdddb8e84c1857d1a55c32ced51f0ed2069","9d7e5f3aa5935964617824d1f9b2599ddb334464"],"95303ff3749680c743b9425f9cf99e6e4065e8a8":["d4d69c535930b5cce125cff868d40f6373dc27d4","73bb5a57dc75b54a39494f99986599cae7dff417"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"59fc0e55b44c555c39d950def9414b5596c6ebe2":["2dd6ecb8250c497ed227653279d6a4f470bfbb31"],"9d7e5f3aa5935964617824d1f9b2599ddb334464":["c7bd1fdddb8e84c1857d1a55c32ced51f0ed2069"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":["19275ba31e621f6da1b83bf13af75233876fd3d4","bf795ee457272965bd751f513787065bbf0a650a"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["bf795ee457272965bd751f513787065bbf0a650a"],"73bb5a57dc75b54a39494f99986599cae7dff417":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"11982a7b6834a8571852448312db4624c32990b5":["c26f00b574427b55127e869b935845554afde1fa"]},"commit2Childs":{"c7bd1fdddb8e84c1857d1a55c32ced51f0ed2069":["d4d69c535930b5cce125cff868d40f6373dc27d4","9d7e5f3aa5935964617824d1f9b2599ddb334464"],"78a55f24d9b493c2a1cecf79f1d78279062b545b":[],"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec":[],"c26f00b574427b55127e869b935845554afde1fa":["11982a7b6834a8571852448312db4624c32990b5"],"96d207426bd26fa5c1014e26d21d87603aea68b7":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"fd92b8bcc88e969302510acf77bd6970da3994c4":["96d207426bd26fa5c1014e26d21d87603aea68b7"],"19275ba31e621f6da1b83bf13af75233876fd3d4":["716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","bf795ee457272965bd751f513787065bbf0a650a","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"bf795ee457272965bd751f513787065bbf0a650a":["74f45af4339b0daf7a95c820ab88c1aea74fbce0","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["c7bd1fdddb8e84c1857d1a55c32ced51f0ed2069"],"2dd6ecb8250c497ed227653279d6a4f470bfbb31":["78a55f24d9b493c2a1cecf79f1d78279062b545b","fd92b8bcc88e969302510acf77bd6970da3994c4","59fc0e55b44c555c39d950def9414b5596c6ebe2"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["95303ff3749680c743b9425f9cf99e6e4065e8a8","73bb5a57dc75b54a39494f99986599cae7dff417"],"95303ff3749680c743b9425f9cf99e6e4065e8a8":[],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c26f00b574427b55127e869b935845554afde1fa","c903c3d15906a3da96b8c0c2fb704491005fdbdb","a258fbb26824fd104ed795e5d9033d2d040049ee"],"59fc0e55b44c555c39d950def9414b5596c6ebe2":["78a55f24d9b493c2a1cecf79f1d78279062b545b","fd92b8bcc88e969302510acf77bd6970da3994c4","5cab9a86bd67202d20b6adc463008c8e982b070a"],"9d7e5f3aa5935964617824d1f9b2599ddb334464":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":[],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"73bb5a57dc75b54a39494f99986599cae7dff417":["716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","19275ba31e621f6da1b83bf13af75233876fd3d4","95303ff3749680c743b9425f9cf99e6e4065e8a8"],"11982a7b6834a8571852448312db4624c32990b5":["2dd6ecb8250c497ed227653279d6a4f470bfbb31"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["78a55f24d9b493c2a1cecf79f1d78279062b545b","716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","95303ff3749680c743b9425f9cf99e6e4065e8a8","74f45af4339b0daf7a95c820ab88c1aea74fbce0","a258fbb26824fd104ed795e5d9033d2d040049ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}