{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOutOfMemory#doTest(MockDirectoryWrapper.Failure).mjava","commits":[{"id":"30310b71978c10ec44d08c346837a2f4bfe7dfed","date":1410955605,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOutOfMemory#doTest(MockDirectoryWrapper.Failure).mjava","pathOld":"/dev/null","sourceNew":"  // just one thread, serial merge policy, hopefully debuggable\n  private void doTest(MockDirectoryWrapper.Failure failOn) throws Exception {   \n    // log all exceptions we hit, in case we fail (for debugging)\n    ByteArrayOutputStream exceptionLog = new ByteArrayOutputStream();\n    PrintStream exceptionStream = new PrintStream(exceptionLog, true, \"UTF-8\");\n    //PrintStream exceptionStream = System.out;\n    \n    final long analyzerSeed = random().nextLong();\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // we are gonna make it angry\n        TokenStream stream = tokenizer;\n        // emit some payloads\n        if (fieldName.contains(\"payloads\")) {\n          stream = new MockVariableLengthPayloadFilter(new Random(analyzerSeed), stream);\n        }\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    \n    MockDirectoryWrapper dir = null;\n    \n    final int numIterations = TEST_NIGHTLY ? atLeast(500) : atLeast(20);\n    \n    STARTOVER:\n    for (int iter = 0; iter < numIterations; iter++) {\n      try {\n        // close from last run\n        if (dir != null) {\n          dir.close();\n        }\n        // disable slow things: we don't rely upon sleeps here.\n        dir = newMockDirectory();\n        dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n        dir.setUseSlowOpenClosers(false);\n      \n        IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n        // just for now, try to keep this test reproducible\n        conf.setMergeScheduler(new SerialMergeScheduler());\n      \n        // test never makes it this far...\n        int numDocs = atLeast(2000);\n      \n        IndexWriter iw = new IndexWriter(dir, conf);\n        iw.commit(); // ensure there is always a commit\n\n        dir.failOn(failOn);\n        \n        for (int i = 0; i < numDocs; i++) {\n          Document doc = new Document();\n          doc.add(newStringField(\"id\", Integer.toString(i), Field.Store.NO));\n          doc.add(new NumericDocValuesField(\"dv\", i));\n          doc.add(new BinaryDocValuesField(\"dv2\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedDocValuesField(\"dv3\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedSetDocValuesField(\"dv4\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedSetDocValuesField(\"dv4\", new BytesRef(Integer.toString(i-1))));\n          doc.add(new SortedNumericDocValuesField(\"dv5\", i));\n          doc.add(new SortedNumericDocValuesField(\"dv5\", i-1));\n          doc.add(newTextField(\"text1\", TestUtil.randomAnalysisString(random(), 20, true), Field.Store.NO));\n          // ensure we store something\n          doc.add(new StoredField(\"stored1\", \"foo\"));\n          doc.add(new StoredField(\"stored1\", \"bar\"));    \n          // ensure we get some payloads\n          doc.add(newTextField(\"text_payloads\", TestUtil.randomAnalysisString(random(), 6, true), Field.Store.NO));\n          // ensure we get some vectors\n          FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n          ft.setStoreTermVectors(true);\n          doc.add(newField(\"text_vectors\", TestUtil.randomAnalysisString(random(), 6, true), ft));\n          \n          if (random().nextInt(10) > 0) {\n            // single doc\n            try {\n              iw.addDocument(doc);\n              // we made it, sometimes delete our doc, or update a dv\n              int thingToDo = random().nextInt(4);\n              if (thingToDo == 0) {\n                iw.deleteDocuments(new Term(\"id\", Integer.toString(i)));\n              } else if (thingToDo == 1) {\n                iw.updateNumericDocValue(new Term(\"id\", Integer.toString(i)), \"dv\", i+1L);\n              } else if (thingToDo == 2) {\n                iw.updateBinaryDocValue(new Term(\"id\", Integer.toString(i)), \"dv2\", new BytesRef(Integer.toString(i+1)));\n              }\n            } catch (OutOfMemoryError e) {\n              if (e.getMessage() != null && e.getMessage().startsWith(\"Fake OutOfMemoryError\")) {\n                exceptionStream.println(\"\\nTEST: got expected fake exc:\" + e.getMessage());\n                e.printStackTrace(exceptionStream);\n                try {\n                  iw.rollback();\n                } catch (Throwable t) {}\n                continue STARTOVER;\n              } else {\n                Rethrow.rethrow(e);\n              }\n            }\n          } else {\n            // block docs\n            Document doc2 = new Document();\n            doc2.add(newStringField(\"id\", Integer.toString(-i), Field.Store.NO));\n            doc2.add(newTextField(\"text1\", TestUtil.randomAnalysisString(random(), 20, true), Field.Store.NO));\n            doc2.add(new StoredField(\"stored1\", \"foo\"));\n            doc2.add(new StoredField(\"stored1\", \"bar\"));\n            doc2.add(newField(\"text_vectors\", TestUtil.randomAnalysisString(random(), 6, true), ft));\n            \n            try {\n              iw.addDocuments(Arrays.asList(doc, doc2));\n              // we made it, sometimes delete our docs\n              if (random().nextBoolean()) {\n                iw.deleteDocuments(new Term(\"id\", Integer.toString(i)), new Term(\"id\", Integer.toString(-i)));\n              }\n            } catch (OutOfMemoryError e) {\n              if (e.getMessage() != null && e.getMessage().startsWith(\"Fake OutOfMemoryError\")) {\n                exceptionStream.println(\"\\nTEST: got expected fake exc:\" + e.getMessage());\n                e.printStackTrace(exceptionStream);\n              } else {\n                Rethrow.rethrow(e);\n              }\n              try {\n                iw.rollback();\n              } catch (Throwable t) {}\n              continue STARTOVER;\n            }\n          }\n          \n          if (random().nextInt(10) == 0) {\n            // trigger flush:\n            try {\n              if (random().nextBoolean()) {\n                DirectoryReader ir = null;\n                try {\n                  ir = DirectoryReader.open(iw, random().nextBoolean());\n                  TestUtil.checkReader(ir);\n                } finally {\n                  IOUtils.closeWhileHandlingException(ir);\n                }\n              } else {\n                iw.commit();\n              }\n              if (DirectoryReader.indexExists(dir)) {\n                TestUtil.checkIndex(dir);\n              }\n            } catch (OutOfMemoryError e) {\n              if (e.getMessage() != null && e.getMessage().startsWith(\"Fake OutOfMemoryError\")) {\n                exceptionStream.println(\"\\nTEST: got expected fake exc:\" + e.getMessage());\n                e.printStackTrace(exceptionStream);\n              } else {\n                Rethrow.rethrow(e);\n              }\n              try {\n                iw.rollback();\n              } catch (Throwable t) {}\n              continue STARTOVER;\n            }\n          }\n        }\n        \n        try {\n          iw.close();\n        } catch (OutOfMemoryError e) {\n          if (e.getMessage() != null && e.getMessage().startsWith(\"Fake OutOfMemoryError\")) {\n            exceptionStream.println(\"\\nTEST: got expected fake exc:\" + e.getMessage());\n            e.printStackTrace(exceptionStream);\n            try {\n              iw.rollback();\n            } catch (Throwable t) {}\n            continue STARTOVER;\n          } else {\n            Rethrow.rethrow(e);\n          }\n        }\n      } catch (Throwable t) {\n        System.out.println(\"Unexpected exception: dumping fake-exception-log:...\");\n        exceptionStream.flush();\n        System.out.println(exceptionLog.toString(\"UTF-8\"));\n        System.out.flush();\n        Rethrow.rethrow(t);\n      }\n    }\n    dir.close();\n    if (VERBOSE) {\n      System.out.println(\"TEST PASSED: dumping fake-exception-log:...\");\n      System.out.println(exceptionLog.toString(\"UTF-8\"));\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"949847c0040cd70a68222d526cb0da7bf6cbb3c2","date":1410997182,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOutOfMemory#doTest(MockDirectoryWrapper.Failure).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOutOfMemory#doTest(MockDirectoryWrapper.Failure).mjava","sourceNew":"  // just one thread, serial merge policy, hopefully debuggable\n  private void doTest(MockDirectoryWrapper.Failure failOn) throws Exception {   \n    // log all exceptions we hit, in case we fail (for debugging)\n    ByteArrayOutputStream exceptionLog = new ByteArrayOutputStream();\n    PrintStream exceptionStream = new PrintStream(exceptionLog, true, \"UTF-8\");\n    //PrintStream exceptionStream = System.out;\n    \n    final long analyzerSeed = random().nextLong();\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // we are gonna make it angry\n        TokenStream stream = tokenizer;\n        // emit some payloads\n        if (fieldName.contains(\"payloads\")) {\n          stream = new MockVariableLengthPayloadFilter(new Random(analyzerSeed), stream);\n        }\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    \n    MockDirectoryWrapper dir = null;\n    \n    final int numIterations = TEST_NIGHTLY ? atLeast(500) : atLeast(20);\n    \n    STARTOVER:\n    for (int iter = 0; iter < numIterations; iter++) {\n      try {\n        // close from last run\n        if (dir != null) {\n          dir.close();\n        }\n        // disable slow things: we don't rely upon sleeps here.\n        dir = newMockDirectory();\n        dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n        dir.setUseSlowOpenClosers(false);\n      \n        IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n        // just for now, try to keep this test reproducible\n        conf.setMergeScheduler(new SerialMergeScheduler());\n      \n        // test never makes it this far...\n        int numDocs = atLeast(2000);\n      \n        IndexWriter iw = new IndexWriter(dir, conf);\n        iw.commit(); // ensure there is always a commit\n\n        dir.failOn(failOn);\n        \n        for (int i = 0; i < numDocs; i++) {\n          Document doc = new Document();\n          doc.add(newStringField(\"id\", Integer.toString(i), Field.Store.NO));\n          doc.add(new NumericDocValuesField(\"dv\", i));\n          doc.add(new BinaryDocValuesField(\"dv2\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedDocValuesField(\"dv3\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedSetDocValuesField(\"dv4\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedSetDocValuesField(\"dv4\", new BytesRef(Integer.toString(i-1))));\n          doc.add(new SortedNumericDocValuesField(\"dv5\", i));\n          doc.add(new SortedNumericDocValuesField(\"dv5\", i-1));\n          doc.add(newTextField(\"text1\", TestUtil.randomAnalysisString(random(), 20, true), Field.Store.NO));\n          // ensure we store something\n          doc.add(new StoredField(\"stored1\", \"foo\"));\n          doc.add(new StoredField(\"stored1\", \"bar\"));    \n          // ensure we get some payloads\n          doc.add(newTextField(\"text_payloads\", TestUtil.randomAnalysisString(random(), 6, true), Field.Store.NO));\n          // ensure we get some vectors\n          FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n          ft.setStoreTermVectors(true);\n          doc.add(newField(\"text_vectors\", TestUtil.randomAnalysisString(random(), 6, true), ft));\n          \n          if (random().nextInt(10) > 0) {\n            // single doc\n            try {\n              iw.addDocument(doc);\n              // we made it, sometimes delete our doc, or update a dv\n              int thingToDo = random().nextInt(4);\n              if (thingToDo == 0) {\n                iw.deleteDocuments(new Term(\"id\", Integer.toString(i)));\n              } else if (thingToDo == 1) {\n                iw.updateNumericDocValue(new Term(\"id\", Integer.toString(i)), \"dv\", i+1L);\n              } else if (thingToDo == 2) {\n                iw.updateBinaryDocValue(new Term(\"id\", Integer.toString(i)), \"dv2\", new BytesRef(Integer.toString(i+1)));\n              }\n            } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n              getOOM(disaster, iw, exceptionStream);\n              continue STARTOVER;\n            }\n          } else {\n            // block docs\n            Document doc2 = new Document();\n            doc2.add(newStringField(\"id\", Integer.toString(-i), Field.Store.NO));\n            doc2.add(newTextField(\"text1\", TestUtil.randomAnalysisString(random(), 20, true), Field.Store.NO));\n            doc2.add(new StoredField(\"stored1\", \"foo\"));\n            doc2.add(new StoredField(\"stored1\", \"bar\"));\n            doc2.add(newField(\"text_vectors\", TestUtil.randomAnalysisString(random(), 6, true), ft));\n            \n            try {\n              iw.addDocuments(Arrays.asList(doc, doc2));\n              // we made it, sometimes delete our docs\n              if (random().nextBoolean()) {\n                iw.deleteDocuments(new Term(\"id\", Integer.toString(i)), new Term(\"id\", Integer.toString(-i)));\n              }\n            } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n              getOOM(disaster, iw, exceptionStream);\n              continue STARTOVER;\n            }\n          }\n          \n          if (random().nextInt(10) == 0) {\n            // trigger flush:\n            try {\n              if (random().nextBoolean()) {\n                DirectoryReader ir = null;\n                try {\n                  ir = DirectoryReader.open(iw, random().nextBoolean());\n                  TestUtil.checkReader(ir);\n                } finally {\n                  IOUtils.closeWhileHandlingException(ir);\n                }\n              } else {\n                iw.commit();\n              }\n              if (DirectoryReader.indexExists(dir)) {\n                TestUtil.checkIndex(dir);\n              }\n            } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n              getOOM(disaster, iw, exceptionStream);\n              continue STARTOVER;\n            }\n          }\n        }\n        \n        try {\n          iw.close();\n        } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n          getOOM(disaster, iw, exceptionStream);\n          continue STARTOVER;\n        }\n      } catch (Throwable t) {\n        System.out.println(\"Unexpected exception: dumping fake-exception-log:...\");\n        exceptionStream.flush();\n        System.out.println(exceptionLog.toString(\"UTF-8\"));\n        System.out.flush();\n        Rethrow.rethrow(t);\n      }\n    }\n    dir.close();\n    if (VERBOSE) {\n      System.out.println(\"TEST PASSED: dumping fake-exception-log:...\");\n      System.out.println(exceptionLog.toString(\"UTF-8\"));\n    }\n  }\n\n","sourceOld":"  // just one thread, serial merge policy, hopefully debuggable\n  private void doTest(MockDirectoryWrapper.Failure failOn) throws Exception {   \n    // log all exceptions we hit, in case we fail (for debugging)\n    ByteArrayOutputStream exceptionLog = new ByteArrayOutputStream();\n    PrintStream exceptionStream = new PrintStream(exceptionLog, true, \"UTF-8\");\n    //PrintStream exceptionStream = System.out;\n    \n    final long analyzerSeed = random().nextLong();\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // we are gonna make it angry\n        TokenStream stream = tokenizer;\n        // emit some payloads\n        if (fieldName.contains(\"payloads\")) {\n          stream = new MockVariableLengthPayloadFilter(new Random(analyzerSeed), stream);\n        }\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    \n    MockDirectoryWrapper dir = null;\n    \n    final int numIterations = TEST_NIGHTLY ? atLeast(500) : atLeast(20);\n    \n    STARTOVER:\n    for (int iter = 0; iter < numIterations; iter++) {\n      try {\n        // close from last run\n        if (dir != null) {\n          dir.close();\n        }\n        // disable slow things: we don't rely upon sleeps here.\n        dir = newMockDirectory();\n        dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n        dir.setUseSlowOpenClosers(false);\n      \n        IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n        // just for now, try to keep this test reproducible\n        conf.setMergeScheduler(new SerialMergeScheduler());\n      \n        // test never makes it this far...\n        int numDocs = atLeast(2000);\n      \n        IndexWriter iw = new IndexWriter(dir, conf);\n        iw.commit(); // ensure there is always a commit\n\n        dir.failOn(failOn);\n        \n        for (int i = 0; i < numDocs; i++) {\n          Document doc = new Document();\n          doc.add(newStringField(\"id\", Integer.toString(i), Field.Store.NO));\n          doc.add(new NumericDocValuesField(\"dv\", i));\n          doc.add(new BinaryDocValuesField(\"dv2\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedDocValuesField(\"dv3\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedSetDocValuesField(\"dv4\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedSetDocValuesField(\"dv4\", new BytesRef(Integer.toString(i-1))));\n          doc.add(new SortedNumericDocValuesField(\"dv5\", i));\n          doc.add(new SortedNumericDocValuesField(\"dv5\", i-1));\n          doc.add(newTextField(\"text1\", TestUtil.randomAnalysisString(random(), 20, true), Field.Store.NO));\n          // ensure we store something\n          doc.add(new StoredField(\"stored1\", \"foo\"));\n          doc.add(new StoredField(\"stored1\", \"bar\"));    \n          // ensure we get some payloads\n          doc.add(newTextField(\"text_payloads\", TestUtil.randomAnalysisString(random(), 6, true), Field.Store.NO));\n          // ensure we get some vectors\n          FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n          ft.setStoreTermVectors(true);\n          doc.add(newField(\"text_vectors\", TestUtil.randomAnalysisString(random(), 6, true), ft));\n          \n          if (random().nextInt(10) > 0) {\n            // single doc\n            try {\n              iw.addDocument(doc);\n              // we made it, sometimes delete our doc, or update a dv\n              int thingToDo = random().nextInt(4);\n              if (thingToDo == 0) {\n                iw.deleteDocuments(new Term(\"id\", Integer.toString(i)));\n              } else if (thingToDo == 1) {\n                iw.updateNumericDocValue(new Term(\"id\", Integer.toString(i)), \"dv\", i+1L);\n              } else if (thingToDo == 2) {\n                iw.updateBinaryDocValue(new Term(\"id\", Integer.toString(i)), \"dv2\", new BytesRef(Integer.toString(i+1)));\n              }\n            } catch (OutOfMemoryError e) {\n              if (e.getMessage() != null && e.getMessage().startsWith(\"Fake OutOfMemoryError\")) {\n                exceptionStream.println(\"\\nTEST: got expected fake exc:\" + e.getMessage());\n                e.printStackTrace(exceptionStream);\n                try {\n                  iw.rollback();\n                } catch (Throwable t) {}\n                continue STARTOVER;\n              } else {\n                Rethrow.rethrow(e);\n              }\n            }\n          } else {\n            // block docs\n            Document doc2 = new Document();\n            doc2.add(newStringField(\"id\", Integer.toString(-i), Field.Store.NO));\n            doc2.add(newTextField(\"text1\", TestUtil.randomAnalysisString(random(), 20, true), Field.Store.NO));\n            doc2.add(new StoredField(\"stored1\", \"foo\"));\n            doc2.add(new StoredField(\"stored1\", \"bar\"));\n            doc2.add(newField(\"text_vectors\", TestUtil.randomAnalysisString(random(), 6, true), ft));\n            \n            try {\n              iw.addDocuments(Arrays.asList(doc, doc2));\n              // we made it, sometimes delete our docs\n              if (random().nextBoolean()) {\n                iw.deleteDocuments(new Term(\"id\", Integer.toString(i)), new Term(\"id\", Integer.toString(-i)));\n              }\n            } catch (OutOfMemoryError e) {\n              if (e.getMessage() != null && e.getMessage().startsWith(\"Fake OutOfMemoryError\")) {\n                exceptionStream.println(\"\\nTEST: got expected fake exc:\" + e.getMessage());\n                e.printStackTrace(exceptionStream);\n              } else {\n                Rethrow.rethrow(e);\n              }\n              try {\n                iw.rollback();\n              } catch (Throwable t) {}\n              continue STARTOVER;\n            }\n          }\n          \n          if (random().nextInt(10) == 0) {\n            // trigger flush:\n            try {\n              if (random().nextBoolean()) {\n                DirectoryReader ir = null;\n                try {\n                  ir = DirectoryReader.open(iw, random().nextBoolean());\n                  TestUtil.checkReader(ir);\n                } finally {\n                  IOUtils.closeWhileHandlingException(ir);\n                }\n              } else {\n                iw.commit();\n              }\n              if (DirectoryReader.indexExists(dir)) {\n                TestUtil.checkIndex(dir);\n              }\n            } catch (OutOfMemoryError e) {\n              if (e.getMessage() != null && e.getMessage().startsWith(\"Fake OutOfMemoryError\")) {\n                exceptionStream.println(\"\\nTEST: got expected fake exc:\" + e.getMessage());\n                e.printStackTrace(exceptionStream);\n              } else {\n                Rethrow.rethrow(e);\n              }\n              try {\n                iw.rollback();\n              } catch (Throwable t) {}\n              continue STARTOVER;\n            }\n          }\n        }\n        \n        try {\n          iw.close();\n        } catch (OutOfMemoryError e) {\n          if (e.getMessage() != null && e.getMessage().startsWith(\"Fake OutOfMemoryError\")) {\n            exceptionStream.println(\"\\nTEST: got expected fake exc:\" + e.getMessage());\n            e.printStackTrace(exceptionStream);\n            try {\n              iw.rollback();\n            } catch (Throwable t) {}\n            continue STARTOVER;\n          } else {\n            Rethrow.rethrow(e);\n          }\n        }\n      } catch (Throwable t) {\n        System.out.println(\"Unexpected exception: dumping fake-exception-log:...\");\n        exceptionStream.flush();\n        System.out.println(exceptionLog.toString(\"UTF-8\"));\n        System.out.flush();\n        Rethrow.rethrow(t);\n      }\n    }\n    dir.close();\n    if (VERBOSE) {\n      System.out.println(\"TEST PASSED: dumping fake-exception-log:...\");\n      System.out.println(exceptionLog.toString(\"UTF-8\"));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"faca0a5ba7f0b5cf87d9ebc28a943ec3dfd38458","date":1416851862,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOutOfMemory#doTest(MockDirectoryWrapper.Failure).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOutOfMemory#doTest(MockDirectoryWrapper.Failure).mjava","sourceNew":"  // just one thread, serial merge policy, hopefully debuggable\n  private void doTest(MockDirectoryWrapper.Failure failOn) throws Exception {   \n    // log all exceptions we hit, in case we fail (for debugging)\n    ByteArrayOutputStream exceptionLog = new ByteArrayOutputStream();\n    PrintStream exceptionStream = new PrintStream(exceptionLog, true, \"UTF-8\");\n    //PrintStream exceptionStream = System.out;\n    \n    final long analyzerSeed = random().nextLong();\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // we are gonna make it angry\n        TokenStream stream = tokenizer;\n        // emit some payloads\n        if (fieldName.contains(\"payloads\")) {\n          stream = new MockVariableLengthPayloadFilter(new Random(analyzerSeed), stream);\n        }\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    \n    MockDirectoryWrapper dir = null;\n    \n    final int numIterations = TEST_NIGHTLY ? atLeast(100) : atLeast(20);\n    \n    STARTOVER:\n    for (int iter = 0; iter < numIterations; iter++) {\n      try {\n        // close from last run\n        if (dir != null) {\n          dir.close();\n        }\n        // disable slow things: we don't rely upon sleeps here.\n        dir = newMockDirectory();\n        dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n        dir.setUseSlowOpenClosers(false);\n      \n        IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n        // just for now, try to keep this test reproducible\n        conf.setMergeScheduler(new SerialMergeScheduler());\n      \n        // test never makes it this far...\n        int numDocs = atLeast(2000);\n      \n        IndexWriter iw = new IndexWriter(dir, conf);\n        iw.commit(); // ensure there is always a commit\n\n        dir.failOn(failOn);\n        \n        for (int i = 0; i < numDocs; i++) {\n          Document doc = new Document();\n          doc.add(newStringField(\"id\", Integer.toString(i), Field.Store.NO));\n          doc.add(new NumericDocValuesField(\"dv\", i));\n          doc.add(new BinaryDocValuesField(\"dv2\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedDocValuesField(\"dv3\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedSetDocValuesField(\"dv4\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedSetDocValuesField(\"dv4\", new BytesRef(Integer.toString(i-1))));\n          doc.add(new SortedNumericDocValuesField(\"dv5\", i));\n          doc.add(new SortedNumericDocValuesField(\"dv5\", i-1));\n          doc.add(newTextField(\"text1\", TestUtil.randomAnalysisString(random(), 20, true), Field.Store.NO));\n          // ensure we store something\n          doc.add(new StoredField(\"stored1\", \"foo\"));\n          doc.add(new StoredField(\"stored1\", \"bar\"));    \n          // ensure we get some payloads\n          doc.add(newTextField(\"text_payloads\", TestUtil.randomAnalysisString(random(), 6, true), Field.Store.NO));\n          // ensure we get some vectors\n          FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n          ft.setStoreTermVectors(true);\n          doc.add(newField(\"text_vectors\", TestUtil.randomAnalysisString(random(), 6, true), ft));\n          \n          if (random().nextInt(10) > 0) {\n            // single doc\n            try {\n              iw.addDocument(doc);\n              // we made it, sometimes delete our doc, or update a dv\n              int thingToDo = random().nextInt(4);\n              if (thingToDo == 0) {\n                iw.deleteDocuments(new Term(\"id\", Integer.toString(i)));\n              } else if (thingToDo == 1) {\n                iw.updateNumericDocValue(new Term(\"id\", Integer.toString(i)), \"dv\", i+1L);\n              } else if (thingToDo == 2) {\n                iw.updateBinaryDocValue(new Term(\"id\", Integer.toString(i)), \"dv2\", new BytesRef(Integer.toString(i+1)));\n              }\n            } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n              getOOM(disaster, iw, exceptionStream);\n              continue STARTOVER;\n            }\n          } else {\n            // block docs\n            Document doc2 = new Document();\n            doc2.add(newStringField(\"id\", Integer.toString(-i), Field.Store.NO));\n            doc2.add(newTextField(\"text1\", TestUtil.randomAnalysisString(random(), 20, true), Field.Store.NO));\n            doc2.add(new StoredField(\"stored1\", \"foo\"));\n            doc2.add(new StoredField(\"stored1\", \"bar\"));\n            doc2.add(newField(\"text_vectors\", TestUtil.randomAnalysisString(random(), 6, true), ft));\n            \n            try {\n              iw.addDocuments(Arrays.asList(doc, doc2));\n              // we made it, sometimes delete our docs\n              if (random().nextBoolean()) {\n                iw.deleteDocuments(new Term(\"id\", Integer.toString(i)), new Term(\"id\", Integer.toString(-i)));\n              }\n            } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n              getOOM(disaster, iw, exceptionStream);\n              continue STARTOVER;\n            }\n          }\n          \n          if (random().nextInt(10) == 0) {\n            // trigger flush:\n            try {\n              if (random().nextBoolean()) {\n                DirectoryReader ir = null;\n                try {\n                  ir = DirectoryReader.open(iw, random().nextBoolean());\n                  TestUtil.checkReader(ir);\n                } finally {\n                  IOUtils.closeWhileHandlingException(ir);\n                }\n              } else {\n                iw.commit();\n              }\n              if (DirectoryReader.indexExists(dir)) {\n                TestUtil.checkIndex(dir);\n              }\n            } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n              getOOM(disaster, iw, exceptionStream);\n              continue STARTOVER;\n            }\n          }\n        }\n        \n        try {\n          iw.close();\n        } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n          getOOM(disaster, iw, exceptionStream);\n          continue STARTOVER;\n        }\n      } catch (Throwable t) {\n        System.out.println(\"Unexpected exception: dumping fake-exception-log:...\");\n        exceptionStream.flush();\n        System.out.println(exceptionLog.toString(\"UTF-8\"));\n        System.out.flush();\n        Rethrow.rethrow(t);\n      }\n    }\n    dir.close();\n    if (VERBOSE) {\n      System.out.println(\"TEST PASSED: dumping fake-exception-log:...\");\n      System.out.println(exceptionLog.toString(\"UTF-8\"));\n    }\n  }\n\n","sourceOld":"  // just one thread, serial merge policy, hopefully debuggable\n  private void doTest(MockDirectoryWrapper.Failure failOn) throws Exception {   \n    // log all exceptions we hit, in case we fail (for debugging)\n    ByteArrayOutputStream exceptionLog = new ByteArrayOutputStream();\n    PrintStream exceptionStream = new PrintStream(exceptionLog, true, \"UTF-8\");\n    //PrintStream exceptionStream = System.out;\n    \n    final long analyzerSeed = random().nextLong();\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // we are gonna make it angry\n        TokenStream stream = tokenizer;\n        // emit some payloads\n        if (fieldName.contains(\"payloads\")) {\n          stream = new MockVariableLengthPayloadFilter(new Random(analyzerSeed), stream);\n        }\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    \n    MockDirectoryWrapper dir = null;\n    \n    final int numIterations = TEST_NIGHTLY ? atLeast(500) : atLeast(20);\n    \n    STARTOVER:\n    for (int iter = 0; iter < numIterations; iter++) {\n      try {\n        // close from last run\n        if (dir != null) {\n          dir.close();\n        }\n        // disable slow things: we don't rely upon sleeps here.\n        dir = newMockDirectory();\n        dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n        dir.setUseSlowOpenClosers(false);\n      \n        IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n        // just for now, try to keep this test reproducible\n        conf.setMergeScheduler(new SerialMergeScheduler());\n      \n        // test never makes it this far...\n        int numDocs = atLeast(2000);\n      \n        IndexWriter iw = new IndexWriter(dir, conf);\n        iw.commit(); // ensure there is always a commit\n\n        dir.failOn(failOn);\n        \n        for (int i = 0; i < numDocs; i++) {\n          Document doc = new Document();\n          doc.add(newStringField(\"id\", Integer.toString(i), Field.Store.NO));\n          doc.add(new NumericDocValuesField(\"dv\", i));\n          doc.add(new BinaryDocValuesField(\"dv2\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedDocValuesField(\"dv3\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedSetDocValuesField(\"dv4\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedSetDocValuesField(\"dv4\", new BytesRef(Integer.toString(i-1))));\n          doc.add(new SortedNumericDocValuesField(\"dv5\", i));\n          doc.add(new SortedNumericDocValuesField(\"dv5\", i-1));\n          doc.add(newTextField(\"text1\", TestUtil.randomAnalysisString(random(), 20, true), Field.Store.NO));\n          // ensure we store something\n          doc.add(new StoredField(\"stored1\", \"foo\"));\n          doc.add(new StoredField(\"stored1\", \"bar\"));    \n          // ensure we get some payloads\n          doc.add(newTextField(\"text_payloads\", TestUtil.randomAnalysisString(random(), 6, true), Field.Store.NO));\n          // ensure we get some vectors\n          FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n          ft.setStoreTermVectors(true);\n          doc.add(newField(\"text_vectors\", TestUtil.randomAnalysisString(random(), 6, true), ft));\n          \n          if (random().nextInt(10) > 0) {\n            // single doc\n            try {\n              iw.addDocument(doc);\n              // we made it, sometimes delete our doc, or update a dv\n              int thingToDo = random().nextInt(4);\n              if (thingToDo == 0) {\n                iw.deleteDocuments(new Term(\"id\", Integer.toString(i)));\n              } else if (thingToDo == 1) {\n                iw.updateNumericDocValue(new Term(\"id\", Integer.toString(i)), \"dv\", i+1L);\n              } else if (thingToDo == 2) {\n                iw.updateBinaryDocValue(new Term(\"id\", Integer.toString(i)), \"dv2\", new BytesRef(Integer.toString(i+1)));\n              }\n            } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n              getOOM(disaster, iw, exceptionStream);\n              continue STARTOVER;\n            }\n          } else {\n            // block docs\n            Document doc2 = new Document();\n            doc2.add(newStringField(\"id\", Integer.toString(-i), Field.Store.NO));\n            doc2.add(newTextField(\"text1\", TestUtil.randomAnalysisString(random(), 20, true), Field.Store.NO));\n            doc2.add(new StoredField(\"stored1\", \"foo\"));\n            doc2.add(new StoredField(\"stored1\", \"bar\"));\n            doc2.add(newField(\"text_vectors\", TestUtil.randomAnalysisString(random(), 6, true), ft));\n            \n            try {\n              iw.addDocuments(Arrays.asList(doc, doc2));\n              // we made it, sometimes delete our docs\n              if (random().nextBoolean()) {\n                iw.deleteDocuments(new Term(\"id\", Integer.toString(i)), new Term(\"id\", Integer.toString(-i)));\n              }\n            } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n              getOOM(disaster, iw, exceptionStream);\n              continue STARTOVER;\n            }\n          }\n          \n          if (random().nextInt(10) == 0) {\n            // trigger flush:\n            try {\n              if (random().nextBoolean()) {\n                DirectoryReader ir = null;\n                try {\n                  ir = DirectoryReader.open(iw, random().nextBoolean());\n                  TestUtil.checkReader(ir);\n                } finally {\n                  IOUtils.closeWhileHandlingException(ir);\n                }\n              } else {\n                iw.commit();\n              }\n              if (DirectoryReader.indexExists(dir)) {\n                TestUtil.checkIndex(dir);\n              }\n            } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n              getOOM(disaster, iw, exceptionStream);\n              continue STARTOVER;\n            }\n          }\n        }\n        \n        try {\n          iw.close();\n        } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n          getOOM(disaster, iw, exceptionStream);\n          continue STARTOVER;\n        }\n      } catch (Throwable t) {\n        System.out.println(\"Unexpected exception: dumping fake-exception-log:...\");\n        exceptionStream.flush();\n        System.out.println(exceptionLog.toString(\"UTF-8\"));\n        System.out.flush();\n        Rethrow.rethrow(t);\n      }\n    }\n    dir.close();\n    if (VERBOSE) {\n      System.out.println(\"TEST PASSED: dumping fake-exception-log:...\");\n      System.out.println(exceptionLog.toString(\"UTF-8\"));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a87ce200bba7d88024e2f1c4012212072ce8a5ae","date":1417031281,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOutOfMemory#doTest(MockDirectoryWrapper.Failure).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOutOfMemory#doTest(MockDirectoryWrapper.Failure).mjava","sourceNew":"  // just one thread, serial merge policy, hopefully debuggable\n  private void doTest(MockDirectoryWrapper.Failure failOn) throws Exception {   \n    // log all exceptions we hit, in case we fail (for debugging)\n    ByteArrayOutputStream exceptionLog = new ByteArrayOutputStream();\n    PrintStream exceptionStream = new PrintStream(exceptionLog, true, \"UTF-8\");\n    //PrintStream exceptionStream = System.out;\n    \n    final long analyzerSeed = random().nextLong();\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // we are gonna make it angry\n        TokenStream stream = tokenizer;\n        // emit some payloads\n        if (fieldName.contains(\"payloads\")) {\n          stream = new MockVariableLengthPayloadFilter(new Random(analyzerSeed), stream);\n        }\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    \n    MockDirectoryWrapper dir = null;\n    \n    final int numIterations = TEST_NIGHTLY ? atLeast(100) : atLeast(5);\n    \n    STARTOVER:\n    for (int iter = 0; iter < numIterations; iter++) {\n      try {\n        // close from last run\n        if (dir != null) {\n          dir.close();\n        }\n        // disable slow things: we don't rely upon sleeps here.\n        dir = newMockDirectory();\n        dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n        dir.setUseSlowOpenClosers(false);\n      \n        IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n        // just for now, try to keep this test reproducible\n        conf.setMergeScheduler(new SerialMergeScheduler());\n      \n        // test never makes it this far...\n        int numDocs = atLeast(2000);\n      \n        IndexWriter iw = new IndexWriter(dir, conf);\n        iw.commit(); // ensure there is always a commit\n\n        dir.failOn(failOn);\n        \n        for (int i = 0; i < numDocs; i++) {\n          Document doc = new Document();\n          doc.add(newStringField(\"id\", Integer.toString(i), Field.Store.NO));\n          doc.add(new NumericDocValuesField(\"dv\", i));\n          doc.add(new BinaryDocValuesField(\"dv2\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedDocValuesField(\"dv3\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedSetDocValuesField(\"dv4\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedSetDocValuesField(\"dv4\", new BytesRef(Integer.toString(i-1))));\n          doc.add(new SortedNumericDocValuesField(\"dv5\", i));\n          doc.add(new SortedNumericDocValuesField(\"dv5\", i-1));\n          doc.add(newTextField(\"text1\", TestUtil.randomAnalysisString(random(), 20, true), Field.Store.NO));\n          // ensure we store something\n          doc.add(new StoredField(\"stored1\", \"foo\"));\n          doc.add(new StoredField(\"stored1\", \"bar\"));    \n          // ensure we get some payloads\n          doc.add(newTextField(\"text_payloads\", TestUtil.randomAnalysisString(random(), 6, true), Field.Store.NO));\n          // ensure we get some vectors\n          FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n          ft.setStoreTermVectors(true);\n          doc.add(newField(\"text_vectors\", TestUtil.randomAnalysisString(random(), 6, true), ft));\n          \n          if (random().nextInt(10) > 0) {\n            // single doc\n            try {\n              iw.addDocument(doc);\n              // we made it, sometimes delete our doc, or update a dv\n              int thingToDo = random().nextInt(4);\n              if (thingToDo == 0) {\n                iw.deleteDocuments(new Term(\"id\", Integer.toString(i)));\n              } else if (thingToDo == 1) {\n                iw.updateNumericDocValue(new Term(\"id\", Integer.toString(i)), \"dv\", i+1L);\n              } else if (thingToDo == 2) {\n                iw.updateBinaryDocValue(new Term(\"id\", Integer.toString(i)), \"dv2\", new BytesRef(Integer.toString(i+1)));\n              }\n            } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n              getOOM(disaster, iw, exceptionStream);\n              continue STARTOVER;\n            }\n          } else {\n            // block docs\n            Document doc2 = new Document();\n            doc2.add(newStringField(\"id\", Integer.toString(-i), Field.Store.NO));\n            doc2.add(newTextField(\"text1\", TestUtil.randomAnalysisString(random(), 20, true), Field.Store.NO));\n            doc2.add(new StoredField(\"stored1\", \"foo\"));\n            doc2.add(new StoredField(\"stored1\", \"bar\"));\n            doc2.add(newField(\"text_vectors\", TestUtil.randomAnalysisString(random(), 6, true), ft));\n            \n            try {\n              iw.addDocuments(Arrays.asList(doc, doc2));\n              // we made it, sometimes delete our docs\n              if (random().nextBoolean()) {\n                iw.deleteDocuments(new Term(\"id\", Integer.toString(i)), new Term(\"id\", Integer.toString(-i)));\n              }\n            } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n              getOOM(disaster, iw, exceptionStream);\n              continue STARTOVER;\n            }\n          }\n          \n          if (random().nextInt(10) == 0) {\n            // trigger flush:\n            try {\n              if (random().nextBoolean()) {\n                DirectoryReader ir = null;\n                try {\n                  ir = DirectoryReader.open(iw, random().nextBoolean());\n                  TestUtil.checkReader(ir);\n                } finally {\n                  IOUtils.closeWhileHandlingException(ir);\n                }\n              } else {\n                iw.commit();\n              }\n              if (DirectoryReader.indexExists(dir)) {\n                TestUtil.checkIndex(dir);\n              }\n            } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n              getOOM(disaster, iw, exceptionStream);\n              continue STARTOVER;\n            }\n          }\n        }\n        \n        try {\n          iw.close();\n        } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n          getOOM(disaster, iw, exceptionStream);\n          continue STARTOVER;\n        }\n      } catch (Throwable t) {\n        System.out.println(\"Unexpected exception: dumping fake-exception-log:...\");\n        exceptionStream.flush();\n        System.out.println(exceptionLog.toString(\"UTF-8\"));\n        System.out.flush();\n        Rethrow.rethrow(t);\n      }\n    }\n    dir.close();\n    if (VERBOSE) {\n      System.out.println(\"TEST PASSED: dumping fake-exception-log:...\");\n      System.out.println(exceptionLog.toString(\"UTF-8\"));\n    }\n  }\n\n","sourceOld":"  // just one thread, serial merge policy, hopefully debuggable\n  private void doTest(MockDirectoryWrapper.Failure failOn) throws Exception {   \n    // log all exceptions we hit, in case we fail (for debugging)\n    ByteArrayOutputStream exceptionLog = new ByteArrayOutputStream();\n    PrintStream exceptionStream = new PrintStream(exceptionLog, true, \"UTF-8\");\n    //PrintStream exceptionStream = System.out;\n    \n    final long analyzerSeed = random().nextLong();\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // we are gonna make it angry\n        TokenStream stream = tokenizer;\n        // emit some payloads\n        if (fieldName.contains(\"payloads\")) {\n          stream = new MockVariableLengthPayloadFilter(new Random(analyzerSeed), stream);\n        }\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    \n    MockDirectoryWrapper dir = null;\n    \n    final int numIterations = TEST_NIGHTLY ? atLeast(100) : atLeast(20);\n    \n    STARTOVER:\n    for (int iter = 0; iter < numIterations; iter++) {\n      try {\n        // close from last run\n        if (dir != null) {\n          dir.close();\n        }\n        // disable slow things: we don't rely upon sleeps here.\n        dir = newMockDirectory();\n        dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n        dir.setUseSlowOpenClosers(false);\n      \n        IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n        // just for now, try to keep this test reproducible\n        conf.setMergeScheduler(new SerialMergeScheduler());\n      \n        // test never makes it this far...\n        int numDocs = atLeast(2000);\n      \n        IndexWriter iw = new IndexWriter(dir, conf);\n        iw.commit(); // ensure there is always a commit\n\n        dir.failOn(failOn);\n        \n        for (int i = 0; i < numDocs; i++) {\n          Document doc = new Document();\n          doc.add(newStringField(\"id\", Integer.toString(i), Field.Store.NO));\n          doc.add(new NumericDocValuesField(\"dv\", i));\n          doc.add(new BinaryDocValuesField(\"dv2\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedDocValuesField(\"dv3\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedSetDocValuesField(\"dv4\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedSetDocValuesField(\"dv4\", new BytesRef(Integer.toString(i-1))));\n          doc.add(new SortedNumericDocValuesField(\"dv5\", i));\n          doc.add(new SortedNumericDocValuesField(\"dv5\", i-1));\n          doc.add(newTextField(\"text1\", TestUtil.randomAnalysisString(random(), 20, true), Field.Store.NO));\n          // ensure we store something\n          doc.add(new StoredField(\"stored1\", \"foo\"));\n          doc.add(new StoredField(\"stored1\", \"bar\"));    \n          // ensure we get some payloads\n          doc.add(newTextField(\"text_payloads\", TestUtil.randomAnalysisString(random(), 6, true), Field.Store.NO));\n          // ensure we get some vectors\n          FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n          ft.setStoreTermVectors(true);\n          doc.add(newField(\"text_vectors\", TestUtil.randomAnalysisString(random(), 6, true), ft));\n          \n          if (random().nextInt(10) > 0) {\n            // single doc\n            try {\n              iw.addDocument(doc);\n              // we made it, sometimes delete our doc, or update a dv\n              int thingToDo = random().nextInt(4);\n              if (thingToDo == 0) {\n                iw.deleteDocuments(new Term(\"id\", Integer.toString(i)));\n              } else if (thingToDo == 1) {\n                iw.updateNumericDocValue(new Term(\"id\", Integer.toString(i)), \"dv\", i+1L);\n              } else if (thingToDo == 2) {\n                iw.updateBinaryDocValue(new Term(\"id\", Integer.toString(i)), \"dv2\", new BytesRef(Integer.toString(i+1)));\n              }\n            } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n              getOOM(disaster, iw, exceptionStream);\n              continue STARTOVER;\n            }\n          } else {\n            // block docs\n            Document doc2 = new Document();\n            doc2.add(newStringField(\"id\", Integer.toString(-i), Field.Store.NO));\n            doc2.add(newTextField(\"text1\", TestUtil.randomAnalysisString(random(), 20, true), Field.Store.NO));\n            doc2.add(new StoredField(\"stored1\", \"foo\"));\n            doc2.add(new StoredField(\"stored1\", \"bar\"));\n            doc2.add(newField(\"text_vectors\", TestUtil.randomAnalysisString(random(), 6, true), ft));\n            \n            try {\n              iw.addDocuments(Arrays.asList(doc, doc2));\n              // we made it, sometimes delete our docs\n              if (random().nextBoolean()) {\n                iw.deleteDocuments(new Term(\"id\", Integer.toString(i)), new Term(\"id\", Integer.toString(-i)));\n              }\n            } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n              getOOM(disaster, iw, exceptionStream);\n              continue STARTOVER;\n            }\n          }\n          \n          if (random().nextInt(10) == 0) {\n            // trigger flush:\n            try {\n              if (random().nextBoolean()) {\n                DirectoryReader ir = null;\n                try {\n                  ir = DirectoryReader.open(iw, random().nextBoolean());\n                  TestUtil.checkReader(ir);\n                } finally {\n                  IOUtils.closeWhileHandlingException(ir);\n                }\n              } else {\n                iw.commit();\n              }\n              if (DirectoryReader.indexExists(dir)) {\n                TestUtil.checkIndex(dir);\n              }\n            } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n              getOOM(disaster, iw, exceptionStream);\n              continue STARTOVER;\n            }\n          }\n        }\n        \n        try {\n          iw.close();\n        } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n          getOOM(disaster, iw, exceptionStream);\n          continue STARTOVER;\n        }\n      } catch (Throwable t) {\n        System.out.println(\"Unexpected exception: dumping fake-exception-log:...\");\n        exceptionStream.flush();\n        System.out.println(exceptionLog.toString(\"UTF-8\"));\n        System.out.flush();\n        Rethrow.rethrow(t);\n      }\n    }\n    dir.close();\n    if (VERBOSE) {\n      System.out.println(\"TEST PASSED: dumping fake-exception-log:...\");\n      System.out.println(exceptionLog.toString(\"UTF-8\"));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c48871ed951104729f5e17a8ee1091b43fa18980","date":1446564542,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnVMError#doTest(MockDirectoryWrapper.Failure).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOutOfMemory#doTest(MockDirectoryWrapper.Failure).mjava","sourceNew":"  // just one thread, serial merge policy, hopefully debuggable\n  private void doTest(MockDirectoryWrapper.Failure failOn) throws Exception {   \n    // log all exceptions we hit, in case we fail (for debugging)\n    ByteArrayOutputStream exceptionLog = new ByteArrayOutputStream();\n    PrintStream exceptionStream = new PrintStream(exceptionLog, true, \"UTF-8\");\n    //PrintStream exceptionStream = System.out;\n    \n    final long analyzerSeed = random().nextLong();\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // we are gonna make it angry\n        TokenStream stream = tokenizer;\n        // emit some payloads\n        if (fieldName.contains(\"payloads\")) {\n          stream = new MockVariableLengthPayloadFilter(new Random(analyzerSeed), stream);\n        }\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    \n    MockDirectoryWrapper dir = null;\n    \n    final int numIterations = TEST_NIGHTLY ? atLeast(100) : atLeast(5);\n    \n    STARTOVER:\n    for (int iter = 0; iter < numIterations; iter++) {\n      try {\n        // close from last run\n        if (dir != null) {\n          dir.close();\n        }\n        // disable slow things: we don't rely upon sleeps here.\n        dir = newMockDirectory();\n        dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n        dir.setUseSlowOpenClosers(false);\n      \n        IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n        // just for now, try to keep this test reproducible\n        conf.setMergeScheduler(new SerialMergeScheduler());\n      \n        // test never makes it this far...\n        int numDocs = atLeast(2000);\n      \n        IndexWriter iw = new IndexWriter(dir, conf);\n        iw.commit(); // ensure there is always a commit\n\n        dir.failOn(failOn);\n        \n        for (int i = 0; i < numDocs; i++) {\n          Document doc = new Document();\n          doc.add(newStringField(\"id\", Integer.toString(i), Field.Store.NO));\n          doc.add(new NumericDocValuesField(\"dv\", i));\n          doc.add(new BinaryDocValuesField(\"dv2\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedDocValuesField(\"dv3\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedSetDocValuesField(\"dv4\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedSetDocValuesField(\"dv4\", new BytesRef(Integer.toString(i-1))));\n          doc.add(new SortedNumericDocValuesField(\"dv5\", i));\n          doc.add(new SortedNumericDocValuesField(\"dv5\", i-1));\n          doc.add(newTextField(\"text1\", TestUtil.randomAnalysisString(random(), 20, true), Field.Store.NO));\n          // ensure we store something\n          doc.add(new StoredField(\"stored1\", \"foo\"));\n          doc.add(new StoredField(\"stored1\", \"bar\"));    \n          // ensure we get some payloads\n          doc.add(newTextField(\"text_payloads\", TestUtil.randomAnalysisString(random(), 6, true), Field.Store.NO));\n          // ensure we get some vectors\n          FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n          ft.setStoreTermVectors(true);\n          doc.add(newField(\"text_vectors\", TestUtil.randomAnalysisString(random(), 6, true), ft));\n          \n          if (random().nextInt(10) > 0) {\n            // single doc\n            try {\n              iw.addDocument(doc);\n              // we made it, sometimes delete our doc, or update a dv\n              int thingToDo = random().nextInt(4);\n              if (thingToDo == 0) {\n                iw.deleteDocuments(new Term(\"id\", Integer.toString(i)));\n              } else if (thingToDo == 1) {\n                iw.updateNumericDocValue(new Term(\"id\", Integer.toString(i)), \"dv\", i+1L);\n              } else if (thingToDo == 2) {\n                iw.updateBinaryDocValue(new Term(\"id\", Integer.toString(i)), \"dv2\", new BytesRef(Integer.toString(i+1)));\n              }\n            } catch (VirtualMachineError | AlreadyClosedException disaster) {\n              getTragedy(disaster, iw, exceptionStream);\n              continue STARTOVER;\n            }\n          } else {\n            // block docs\n            Document doc2 = new Document();\n            doc2.add(newStringField(\"id\", Integer.toString(-i), Field.Store.NO));\n            doc2.add(newTextField(\"text1\", TestUtil.randomAnalysisString(random(), 20, true), Field.Store.NO));\n            doc2.add(new StoredField(\"stored1\", \"foo\"));\n            doc2.add(new StoredField(\"stored1\", \"bar\"));\n            doc2.add(newField(\"text_vectors\", TestUtil.randomAnalysisString(random(), 6, true), ft));\n            \n            try {\n              iw.addDocuments(Arrays.asList(doc, doc2));\n              // we made it, sometimes delete our docs\n              if (random().nextBoolean()) {\n                iw.deleteDocuments(new Term(\"id\", Integer.toString(i)), new Term(\"id\", Integer.toString(-i)));\n              }\n            } catch (VirtualMachineError | AlreadyClosedException disaster) {\n              getTragedy(disaster, iw, exceptionStream);\n              continue STARTOVER;\n            }\n          }\n          \n          if (random().nextInt(10) == 0) {\n            // trigger flush:\n            try {\n              if (random().nextBoolean()) {\n                DirectoryReader ir = null;\n                try {\n                  ir = DirectoryReader.open(iw, random().nextBoolean());\n                  TestUtil.checkReader(ir);\n                } finally {\n                  IOUtils.closeWhileHandlingException(ir);\n                }\n              } else {\n                iw.commit();\n              }\n              if (DirectoryReader.indexExists(dir)) {\n                TestUtil.checkIndex(dir);\n              }\n            } catch (VirtualMachineError | AlreadyClosedException disaster) {\n              getTragedy(disaster, iw, exceptionStream);\n              continue STARTOVER;\n            }\n          }\n        }\n        \n        try {\n          iw.close();\n        } catch (VirtualMachineError | AlreadyClosedException disaster) {\n          getTragedy(disaster, iw, exceptionStream);\n          continue STARTOVER;\n        }\n      } catch (Throwable t) {\n        System.out.println(\"Unexpected exception: dumping fake-exception-log:...\");\n        exceptionStream.flush();\n        System.out.println(exceptionLog.toString(\"UTF-8\"));\n        System.out.flush();\n        Rethrow.rethrow(t);\n      }\n    }\n    dir.close();\n    if (VERBOSE) {\n      System.out.println(\"TEST PASSED: dumping fake-exception-log:...\");\n      System.out.println(exceptionLog.toString(\"UTF-8\"));\n    }\n  }\n\n","sourceOld":"  // just one thread, serial merge policy, hopefully debuggable\n  private void doTest(MockDirectoryWrapper.Failure failOn) throws Exception {   \n    // log all exceptions we hit, in case we fail (for debugging)\n    ByteArrayOutputStream exceptionLog = new ByteArrayOutputStream();\n    PrintStream exceptionStream = new PrintStream(exceptionLog, true, \"UTF-8\");\n    //PrintStream exceptionStream = System.out;\n    \n    final long analyzerSeed = random().nextLong();\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // we are gonna make it angry\n        TokenStream stream = tokenizer;\n        // emit some payloads\n        if (fieldName.contains(\"payloads\")) {\n          stream = new MockVariableLengthPayloadFilter(new Random(analyzerSeed), stream);\n        }\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    \n    MockDirectoryWrapper dir = null;\n    \n    final int numIterations = TEST_NIGHTLY ? atLeast(100) : atLeast(5);\n    \n    STARTOVER:\n    for (int iter = 0; iter < numIterations; iter++) {\n      try {\n        // close from last run\n        if (dir != null) {\n          dir.close();\n        }\n        // disable slow things: we don't rely upon sleeps here.\n        dir = newMockDirectory();\n        dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n        dir.setUseSlowOpenClosers(false);\n      \n        IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n        // just for now, try to keep this test reproducible\n        conf.setMergeScheduler(new SerialMergeScheduler());\n      \n        // test never makes it this far...\n        int numDocs = atLeast(2000);\n      \n        IndexWriter iw = new IndexWriter(dir, conf);\n        iw.commit(); // ensure there is always a commit\n\n        dir.failOn(failOn);\n        \n        for (int i = 0; i < numDocs; i++) {\n          Document doc = new Document();\n          doc.add(newStringField(\"id\", Integer.toString(i), Field.Store.NO));\n          doc.add(new NumericDocValuesField(\"dv\", i));\n          doc.add(new BinaryDocValuesField(\"dv2\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedDocValuesField(\"dv3\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedSetDocValuesField(\"dv4\", new BytesRef(Integer.toString(i))));\n          doc.add(new SortedSetDocValuesField(\"dv4\", new BytesRef(Integer.toString(i-1))));\n          doc.add(new SortedNumericDocValuesField(\"dv5\", i));\n          doc.add(new SortedNumericDocValuesField(\"dv5\", i-1));\n          doc.add(newTextField(\"text1\", TestUtil.randomAnalysisString(random(), 20, true), Field.Store.NO));\n          // ensure we store something\n          doc.add(new StoredField(\"stored1\", \"foo\"));\n          doc.add(new StoredField(\"stored1\", \"bar\"));    \n          // ensure we get some payloads\n          doc.add(newTextField(\"text_payloads\", TestUtil.randomAnalysisString(random(), 6, true), Field.Store.NO));\n          // ensure we get some vectors\n          FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n          ft.setStoreTermVectors(true);\n          doc.add(newField(\"text_vectors\", TestUtil.randomAnalysisString(random(), 6, true), ft));\n          \n          if (random().nextInt(10) > 0) {\n            // single doc\n            try {\n              iw.addDocument(doc);\n              // we made it, sometimes delete our doc, or update a dv\n              int thingToDo = random().nextInt(4);\n              if (thingToDo == 0) {\n                iw.deleteDocuments(new Term(\"id\", Integer.toString(i)));\n              } else if (thingToDo == 1) {\n                iw.updateNumericDocValue(new Term(\"id\", Integer.toString(i)), \"dv\", i+1L);\n              } else if (thingToDo == 2) {\n                iw.updateBinaryDocValue(new Term(\"id\", Integer.toString(i)), \"dv2\", new BytesRef(Integer.toString(i+1)));\n              }\n            } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n              getOOM(disaster, iw, exceptionStream);\n              continue STARTOVER;\n            }\n          } else {\n            // block docs\n            Document doc2 = new Document();\n            doc2.add(newStringField(\"id\", Integer.toString(-i), Field.Store.NO));\n            doc2.add(newTextField(\"text1\", TestUtil.randomAnalysisString(random(), 20, true), Field.Store.NO));\n            doc2.add(new StoredField(\"stored1\", \"foo\"));\n            doc2.add(new StoredField(\"stored1\", \"bar\"));\n            doc2.add(newField(\"text_vectors\", TestUtil.randomAnalysisString(random(), 6, true), ft));\n            \n            try {\n              iw.addDocuments(Arrays.asList(doc, doc2));\n              // we made it, sometimes delete our docs\n              if (random().nextBoolean()) {\n                iw.deleteDocuments(new Term(\"id\", Integer.toString(i)), new Term(\"id\", Integer.toString(-i)));\n              }\n            } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n              getOOM(disaster, iw, exceptionStream);\n              continue STARTOVER;\n            }\n          }\n          \n          if (random().nextInt(10) == 0) {\n            // trigger flush:\n            try {\n              if (random().nextBoolean()) {\n                DirectoryReader ir = null;\n                try {\n                  ir = DirectoryReader.open(iw, random().nextBoolean());\n                  TestUtil.checkReader(ir);\n                } finally {\n                  IOUtils.closeWhileHandlingException(ir);\n                }\n              } else {\n                iw.commit();\n              }\n              if (DirectoryReader.indexExists(dir)) {\n                TestUtil.checkIndex(dir);\n              }\n            } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n              getOOM(disaster, iw, exceptionStream);\n              continue STARTOVER;\n            }\n          }\n        }\n        \n        try {\n          iw.close();\n        } catch (OutOfMemoryError | AlreadyClosedException disaster) {\n          getOOM(disaster, iw, exceptionStream);\n          continue STARTOVER;\n        }\n      } catch (Throwable t) {\n        System.out.println(\"Unexpected exception: dumping fake-exception-log:...\");\n        exceptionStream.flush();\n        System.out.println(exceptionLog.toString(\"UTF-8\"));\n        System.out.flush();\n        Rethrow.rethrow(t);\n      }\n    }\n    dir.close();\n    if (VERBOSE) {\n      System.out.println(\"TEST PASSED: dumping fake-exception-log:...\");\n      System.out.println(exceptionLog.toString(\"UTF-8\"));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"30310b71978c10ec44d08c346837a2f4bfe7dfed":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a87ce200bba7d88024e2f1c4012212072ce8a5ae":["faca0a5ba7f0b5cf87d9ebc28a943ec3dfd38458"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"faca0a5ba7f0b5cf87d9ebc28a943ec3dfd38458":["949847c0040cd70a68222d526cb0da7bf6cbb3c2"],"949847c0040cd70a68222d526cb0da7bf6cbb3c2":["30310b71978c10ec44d08c346837a2f4bfe7dfed"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c48871ed951104729f5e17a8ee1091b43fa18980"],"c48871ed951104729f5e17a8ee1091b43fa18980":["a87ce200bba7d88024e2f1c4012212072ce8a5ae"]},"commit2Childs":{"30310b71978c10ec44d08c346837a2f4bfe7dfed":["949847c0040cd70a68222d526cb0da7bf6cbb3c2"],"a87ce200bba7d88024e2f1c4012212072ce8a5ae":["c48871ed951104729f5e17a8ee1091b43fa18980"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["30310b71978c10ec44d08c346837a2f4bfe7dfed"],"faca0a5ba7f0b5cf87d9ebc28a943ec3dfd38458":["a87ce200bba7d88024e2f1c4012212072ce8a5ae"],"949847c0040cd70a68222d526cb0da7bf6cbb3c2":["faca0a5ba7f0b5cf87d9ebc28a943ec3dfd38458"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"c48871ed951104729f5e17a8ee1091b43fa18980":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}