{"path":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","commits":[{"id":"69e6520a21709190413a63084ed135271aab1a7c","date":1556607462,"type":1,"author":"Mikhail Khludnev","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    String collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      t = timings.sub(\"fillRanges\");\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        syncRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\");\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        shardRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\");\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        shardRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        shardRequestTracker.processResponses(results, shardHandler, true,\n            \"SPLITSHARD failed while asking sub shard leaders\" +\n                \" to apply buffered updates\");\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        syncRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\");\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      t = timings.sub(\"finalCommit\");\n      ocmh.commit(results, slice.get(), parentShardLeader);\n      t.stop();\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    String collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      t = timings.sub(\"fillRanges\");\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.debug(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      t = timings.sub(\"finalCommit\");\n      ocmh.commit(results, slice.get(), parentShardLeader);\n      t.stop();\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d20dc32a40c1bd7b8b8e2354e6b0b9f956ef758c","date":1562161956,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      t = timings.sub(\"fillRanges\");\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        syncRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\");\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        shardRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\");\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        shardRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        shardRequestTracker.processResponses(results, shardHandler, true,\n            \"SPLITSHARD failed while asking sub shard leaders\" +\n                \" to apply buffered updates\");\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        syncRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\");\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      t = timings.sub(\"finalCommit\");\n      ocmh.commit(results, slice.get(), parentShardLeader);\n      t.stop();\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    String collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      t = timings.sub(\"fillRanges\");\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        syncRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\");\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        shardRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\");\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        shardRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        shardRequestTracker.processResponses(results, shardHandler, true,\n            \"SPLITSHARD failed while asking sub shard leaders\" +\n                \" to apply buffered updates\");\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        syncRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\");\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      t = timings.sub(\"finalCommit\");\n      ocmh.commit(results, slice.get(), parentShardLeader);\n      t.stop();\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a05f3f5161c62339ec5560b8f6958f3df8483618","date":1563550501,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, true)) {\n        t = timings.sub(\"getRanges\");\n\n        log.info(\"Requesting split ranges from replica \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n            + collectionName + \" on \" + parentShardLeader);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\");\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split range to be used is \" + splits);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        syncRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\");\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        shardRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\");\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        shardRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        shardRequestTracker.processResponses(results, shardHandler, true,\n            \"SPLITSHARD failed while asking sub shard leaders\" +\n                \" to apply buffered updates\");\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        syncRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\");\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      t = timings.sub(\"finalCommit\");\n      ocmh.commit(results, slice.get(), parentShardLeader);\n      t.stop();\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      t = timings.sub(\"fillRanges\");\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        syncRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\");\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        shardRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\");\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        shardRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        shardRequestTracker.processResponses(results, shardHandler, true,\n            \"SPLITSHARD failed while asking sub shard leaders\" +\n                \" to apply buffered updates\");\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        syncRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\");\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      t = timings.sub(\"finalCommit\");\n      ocmh.commit(results, slice.get(), parentShardLeader);\n      t.stop();\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["ebccdcfbac56fd8a05e781b2b8cf7a8f1d447e87","ebccdcfbac56fd8a05e781b2b8cf7a8f1d447e87"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"36bdabc04743acfe0e82c9cf8208b1111b2b193a","date":1565115020,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, true)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\");\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: \" + splits + \" slice=\" + slice + \" leader=\" + parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        syncRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\");\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        shardRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\");\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        shardRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        shardRequestTracker.processResponses(results, shardHandler, true,\n            \"SPLITSHARD failed while asking sub shard leaders\" +\n                \" to apply buffered updates\");\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        syncRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\");\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      t = timings.sub(\"finalCommit\");\n      ocmh.commit(results, slice.get(), parentShardLeader);\n      t.stop();\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, true)) {\n        t = timings.sub(\"getRanges\");\n\n        log.info(\"Requesting split ranges from replica \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n            + collectionName + \" on \" + parentShardLeader);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\");\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split range to be used is \" + splits);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        syncRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\");\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        shardRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\");\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        shardRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        shardRequestTracker.processResponses(results, shardHandler, true,\n            \"SPLITSHARD failed while asking sub shard leaders\" +\n                \" to apply buffered updates\");\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        syncRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\");\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      t = timings.sub(\"finalCommit\");\n      ocmh.commit(results, slice.get(), parentShardLeader);\n      t.stop();\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2f56a372e878021ee5712e3b5651d2bdd2945b07","date":1565355922,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\");\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: \" + splits + \" slice=\" + slice + \" leader=\" + parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        syncRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\");\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        shardRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\");\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        shardRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        shardRequestTracker.processResponses(results, shardHandler, true,\n            \"SPLITSHARD failed while asking sub shard leaders\" +\n                \" to apply buffered updates\");\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        syncRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\");\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      t = timings.sub(\"finalCommit\");\n      ocmh.commit(results, slice.get(), parentShardLeader);\n      t.stop();\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, true)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\");\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: \" + splits + \" slice=\" + slice + \" leader=\" + parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        syncRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\");\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        shardRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\");\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        shardRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        shardRequestTracker.processResponses(results, shardHandler, true,\n            \"SPLITSHARD failed while asking sub shard leaders\" +\n                \" to apply buffered updates\");\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        syncRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\");\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      t = timings.sub(\"finalCommit\");\n      ocmh.commit(results, slice.get(), parentShardLeader);\n      t.stop();\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7f125a1877bd8a597f490d08013d70f8f4df12e5","date":1567079860,"type":3,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n          handleFailureOnAsyncRequest(results, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: \" + splits + \" slice=\" + slice + \" leader=\" + parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      t = timings.sub(\"finalCommit\");\n      ocmh.commit(results, slice.get(), parentShardLeader);\n      t.stop();\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\");\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: \" + splits + \" slice=\" + slice + \" leader=\" + parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        syncRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\");\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        shardRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\");\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        shardRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        shardRequestTracker.processResponses(results, shardHandler, true,\n            \"SPLITSHARD failed while asking sub shard leaders\" +\n                \" to apply buffered updates\");\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        syncRequestTracker.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\");\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      t = timings.sub(\"finalCommit\");\n      ocmh.commit(results, slice.get(), parentShardLeader);\n      t.stop();\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["ebccdcfbac56fd8a05e781b2b8cf7a8f1d447e87","ebccdcfbac56fd8a05e781b2b8cf7a8f1d447e87"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1498cb4cd65cf2303ccde0e0d6edf5a5e1e8fd28","date":1571131262,"type":3,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n          handleFailureOnAsyncRequest(results, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: \" + splits + \" slice=\" + slice + \" leader=\" + parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      t = timings.sub(\"finalCommit\");\n      ocmh.commit(results, slice.get(), parentShardLeader);\n      t.stop();\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n          handleFailureOnAsyncRequest(results, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: \" + splits + \" slice=\" + slice + \" leader=\" + parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      t = timings.sub(\"finalCommit\");\n      ocmh.commit(results, slice.get(), parentShardLeader);\n      t.stop();\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","bugFix":["439c63ae5d22132fca810a0029a854e97d2c1a3e","66e0b82bd39567aa2bf534e5282d05fb4a4a2c76"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b0b597c65628ca9e73913a07e81691f8229bae35","date":1571224353,"type":3,"author":"jimczi","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n          handleFailureOnAsyncRequest(results, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: \" + splits + \" slice=\" + slice + \" leader=\" + parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      t = timings.sub(\"finalCommit\");\n      ocmh.commit(results, slice.get(), parentShardLeader);\n      t.stop();\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n          handleFailureOnAsyncRequest(results, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: \" + splits + \" slice=\" + slice + \" leader=\" + parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      t = timings.sub(\"finalCommit\");\n      ocmh.commit(results, slice.get(), parentShardLeader);\n      t.stop();\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8f79e6c597ec19b2f9669589153faac2f803462a","date":1575984097,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n          handleFailureOnAsyncRequest(results, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: \" + splits + \" slice=\" + slice + \" leader=\" + parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      t = timings.sub(\"finalCommit\");\n      ocmh.commit(results, slice.get(), parentShardLeader);\n      t.stop();\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n          handleFailureOnAsyncRequest(results, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: \" + splits + \" slice=\" + slice + \" leader=\" + parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      t = timings.sub(\"finalCommit\");\n      ocmh.commit(results, slice.get(), parentShardLeader);\n      t.stop();\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","bugFix":["685af99397b6da31116a2cac747ed255d217d080"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"993b0c7dda6341b437fe5685d35c6cc35eaac420","date":1575985950,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n          handleFailureOnAsyncRequest(results, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: \" + splits + \" slice=\" + slice + \" leader=\" + parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      t = timings.sub(\"finalCommit\");\n      ocmh.commit(results, slice.get(), parentShardLeader);\n      t.stop();\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n          handleFailureOnAsyncRequest(results, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: \" + splits + \" slice=\" + slice + \" leader=\" + parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      t = timings.sub(\"finalCommit\");\n      ocmh.commit(results, slice.get(), parentShardLeader);\n      t.stop();\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8a8f0700353755635a948651e84c152bff39d8dc","date":1576121314,"type":3,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n          handleFailureOnAsyncRequest(results, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: \" + splits + \" slice=\" + slice + \" leader=\" + parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n          handleFailureOnAsyncRequest(results, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: \" + splits + \" slice=\" + slice + \" leader=\" + parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      t = timings.sub(\"finalCommit\");\n      ocmh.commit(results, slice.get(), parentShardLeader);\n      t.stop();\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","bugFix":["20c968c14aace7cf49843bf2c1fafc7fd3845659","1d4bf9d5308dfef350829c28f2b3b2648df1e9b1"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"df724d84dab24a0cc54bec95a8680867adc7f171","date":1576156608,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n          handleFailureOnAsyncRequest(results, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: \" + splits + \" slice=\" + slice + \" leader=\" + parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n          handleFailureOnAsyncRequest(results, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: \" + splits + \" slice=\" + slice + \" leader=\" + parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      t = timings.sub(\"finalCommit\");\n      ocmh.commit(results, slice.get(), parentShardLeader);\n      t.stop();\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ebccdcfbac56fd8a05e781b2b8cf7a8f1d447e87","date":1576277705,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.syncRequestTracker();\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: \" + splits + \" slice=\" + slice + \" leader=\" + parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n          handleFailureOnAsyncRequest(results, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: \" + splits + \" slice=\" + slice + \" leader=\" + parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","bugFix":["7f125a1877bd8a597f490d08013d70f8f4df12e5","a05f3f5161c62339ec5560b8f6958f3df8483618"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"06595b0c22c7d3075c4104d3820cccf95d9d8a43","date":1576491645,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.syncRequestTracker();\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: \" + splits + \" slice=\" + slice + \" leader=\" + parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n          handleFailureOnAsyncRequest(results, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: \" + splits + \" slice=\" + slice + \" leader=\" + parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4abf821a41a2fdca2a1dea148999931d22e20529","date":1587749643,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.syncRequestTracker();\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: {} slice={} leader={}\", splits, slice, parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice {} of collection {} on {}\", subSlice, collectionName, nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica {} as part of slice {} of collection {} on {}\"\n            , subShardName, subSlice, collectionName, nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: {} to be alive on: {}\", subShardName, nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection {} parent shard: {} on: {}\"\n          , collectionName, slice, parentShardLeader);\n\n      if (log.isInfoEnabled()) {\n        log.info(\"Splitting shard {} as part of slice {} of collection {} on {}\"\n            , parentShardLeader.getName(), slice, collectionName, parentShardLeader);\n      }\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      if (log.isDebugEnabled()) {\n        log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n      }\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : {}\", subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : {}\", subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard {} as part of slice {} of collection {} on {}\"\n            , solrCoreName, sliceName, collectionName, subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices {}\", subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: {} parent shard: {}\", collectionName, slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.syncRequestTracker();\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: \" + splits + \" slice=\" + slice + \" leader=\" + parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"86968c6cf51846df861b8f29bd85b6d9a7c9f19c","date":1591481497,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","sourceNew":"  @SuppressWarnings({\"rawtypes\"})\n  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.syncRequestTracker();\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: {} slice={} leader={}\", splits, slice, parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice {} of collection {} on {}\", subSlice, collectionName, nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica {} as part of slice {} of collection {} on {}\"\n            , subShardName, subSlice, collectionName, nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: {} to be alive on: {}\", subShardName, nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection {} parent shard: {} on: {}\"\n          , collectionName, slice, parentShardLeader);\n\n      if (log.isInfoEnabled()) {\n        log.info(\"Splitting shard {} as part of slice {} of collection {} on {}\"\n            , parentShardLeader.getName(), slice, collectionName, parentShardLeader);\n      }\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      if (log.isDebugEnabled()) {\n        log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n      }\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : {}\", subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : {}\", subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard {} as part of slice {} of collection {} on {}\"\n            , solrCoreName, sliceName, collectionName, subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices {}\", subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: {} parent shard: {}\", collectionName, slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.syncRequestTracker();\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: {} slice={} leader={}\", splits, slice, parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice {} of collection {} on {}\", subSlice, collectionName, nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica {} as part of slice {} of collection {} on {}\"\n            , subShardName, subSlice, collectionName, nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: {} to be alive on: {}\", subShardName, nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection {} parent shard: {} on: {}\"\n          , collectionName, slice, parentShardLeader);\n\n      if (log.isInfoEnabled()) {\n        log.info(\"Splitting shard {} as part of slice {} of collection {} on {}\"\n            , parentShardLeader.getName(), slice, collectionName, parentShardLeader);\n      }\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      if (log.isDebugEnabled()) {\n        log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n      }\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : {}\", subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : {}\", subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard {} as part of slice {} of collection {} on {}\"\n            , solrCoreName, sliceName, collectionName, subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices {}\", subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: {} parent shard: {}\", collectionName, slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1964c059f45ae1de1877f9f0fe3ca327ea4218e8","date":1594088246,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","sourceNew":"  @SuppressWarnings({\"rawtypes\"})\n  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.syncRequestTracker();\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: {} slice={} leader={}\", splits, slice, parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice {} of collection {} on {}\", subSlice, collectionName, nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica {} as part of slice {} of collection {} on {}\"\n            , subShardName, subSlice, collectionName, nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: {} to be alive on: {}\", subShardName, nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection {} parent shard: {} on: {}\"\n          , collectionName, slice, parentShardLeader);\n\n      if (log.isInfoEnabled()) {\n        log.info(\"Splitting shard {} as part of slice {} of collection {} on {}\"\n            , parentShardLeader.getName(), slice, collectionName, parentShardLeader);\n      }\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      if (log.isDebugEnabled()) {\n        log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n      }\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : {}\", subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : {}\", subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard {} as part of slice {} of collection {} on {}\"\n            , solrCoreName, sliceName, collectionName, subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices {}\", subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: {} parent shard: {}\", collectionName, slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","sourceOld":"  @SuppressWarnings({\"rawtypes\"})\n  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler(ocmh.overseer.getCoreContainer().getUpdateShardHandler().getDefaultHttpClient());\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.syncRequestTracker();\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: {} slice={} leader={}\", splits, slice, parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice {} of collection {} on {}\", subSlice, collectionName, nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica {} as part of slice {} of collection {} on {}\"\n            , subShardName, subSlice, collectionName, nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: {} to be alive on: {}\", subShardName, nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection {} parent shard: {} on: {}\"\n          , collectionName, slice, parentShardLeader);\n\n      if (log.isInfoEnabled()) {\n        log.info(\"Splitting shard {} as part of slice {} of collection {} on {}\"\n            , parentShardLeader.getName(), slice, collectionName, parentShardLeader);\n      }\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      if (log.isDebugEnabled()) {\n        log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n      }\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : {}\", subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : {}\", subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard {} as part of slice {} of collection {} on {}\"\n            , solrCoreName, sliceName, collectionName, subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices {}\", subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: {} parent shard: {}\", collectionName, slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e","date":1594223844,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","sourceNew":"  @SuppressWarnings({\"rawtypes\"})\n  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.syncRequestTracker();\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: {} slice={} leader={}\", splits, slice, parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice {} of collection {} on {}\", subSlice, collectionName, nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica {} as part of slice {} of collection {} on {}\"\n            , subShardName, subSlice, collectionName, nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: {} to be alive on: {}\", subShardName, nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection {} parent shard: {} on: {}\"\n          , collectionName, slice, parentShardLeader);\n\n      if (log.isInfoEnabled()) {\n        log.info(\"Splitting shard {} as part of slice {} of collection {} on {}\"\n            , parentShardLeader.getName(), slice, collectionName, parentShardLeader);\n      }\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      if (log.isDebugEnabled()) {\n        log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n      }\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : {}\", subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : {}\", subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard {} as part of slice {} of collection {} on {}\"\n            , solrCoreName, sliceName, collectionName, subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices {}\", subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: {} parent shard: {}\", collectionName, slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","sourceOld":"  @SuppressWarnings({\"rawtypes\"})\n  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.syncRequestTracker();\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: {} slice={} leader={}\", splits, slice, parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice {} of collection {} on {}\", subSlice, collectionName, nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica {} as part of slice {} of collection {} on {}\"\n            , subShardName, subSlice, collectionName, nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: {} to be alive on: {}\", subShardName, nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection {} parent shard: {} on: {}\"\n          , collectionName, slice, parentShardLeader);\n\n      if (log.isInfoEnabled()) {\n        log.info(\"Splitting shard {} as part of slice {} of collection {} on {}\"\n            , parentShardLeader.getName(), slice, collectionName, parentShardLeader);\n      }\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      if (log.isDebugEnabled()) {\n        log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n      }\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : {}\", subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : {}\", subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard {} as part of slice {} of collection {} on {}\"\n            , solrCoreName, sliceName, collectionName, subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices {}\", subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: {} parent shard: {}\", collectionName, slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3f504512a03d978990cbff30db0522b354e846db","date":1595247421,"type":3,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","sourceNew":"  @SuppressWarnings({\"rawtypes\"})\n  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.syncRequestTracker();\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: {} slice={} leader={}\", splits, slice, parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice {} of collection {} on {}\", subSlice, collectionName, nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica {} as part of slice {} of collection {} on {}\"\n            , subShardName, subSlice, collectionName, nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: {} to be alive on: {}\", subShardName, nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection {} parent shard: {} on: {}\"\n          , collectionName, slice, parentShardLeader);\n\n      if (log.isInfoEnabled()) {\n        log.info(\"Splitting shard {} as part of slice {} of collection {} on {}\"\n            , parentShardLeader.getName(), slice, collectionName, parentShardLeader);\n      }\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      if (log.isDebugEnabled()) {\n        log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n      }\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : {}\", subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : {}\", subShardNames);\n\n      // Replica creation for the new Slices\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard {} as part of slice {} of collection {} on {}\"\n            , solrCoreName, sliceName, collectionName, subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices {}\", subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: {} parent shard: {}\", collectionName, slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","sourceOld":"  @SuppressWarnings({\"rawtypes\"})\n  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.syncRequestTracker();\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: {} slice={} leader={}\", splits, slice, parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice {} of collection {} on {}\", subSlice, collectionName, nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica {} as part of slice {} of collection {} on {}\"\n            , subShardName, subSlice, collectionName, nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: {} to be alive on: {}\", subShardName, nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection {} parent shard: {} on: {}\"\n          , collectionName, slice, parentShardLeader);\n\n      if (log.isInfoEnabled()) {\n        log.info(\"Splitting shard {} as part of slice {} of collection {} on {}\"\n            , parentShardLeader.getName(), slice, collectionName, parentShardLeader);\n      }\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      if (log.isDebugEnabled()) {\n        log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n      }\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : {}\", subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : {}\", subShardNames);\n\n      // Replica creation for the new Slices\n      // replica placement is controlled by the autoscaling policy framework\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard {} as part of slice {} of collection {} on {}\"\n            , solrCoreName, sliceName, collectionName, subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices {}\", subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: {} parent shard: {}\", collectionName, slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c526352db87264a72a7a9ad68c1b769b81e54305","date":1598780188,"type":5,"author":"Ilan Ginzburg","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,CloudConfig,ZkNodeProps,NamedList[Object]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","sourceNew":"  @SuppressWarnings({\"rawtypes\"})\n  public boolean split(ClusterState clusterState, CloudConfig cloudConfig, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.syncRequestTracker();\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: {} slice={} leader={}\", splits, slice, parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, cloudConfig, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice {} of collection {} on {}\", subSlice, collectionName, nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica {} as part of slice {} of collection {} on {}\"\n            , subShardName, subSlice, collectionName, nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, cloudConfig, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: {} to be alive on: {}\", subShardName, nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection {} parent shard: {} on: {}\"\n          , collectionName, slice, parentShardLeader);\n\n      if (log.isInfoEnabled()) {\n        log.info(\"Splitting shard {} as part of slice {} of collection {} on {}\"\n            , parentShardLeader.getName(), slice, collectionName, parentShardLeader);\n      }\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      if (log.isDebugEnabled()) {\n        log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n      }\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : {}\", subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : {}\", subShardNames);\n\n      // Replica creation for the new Slices\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, cloudConfig, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard {} as part of slice {} of collection {} on {}\"\n            , solrCoreName, sliceName, collectionName, subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, cloudConfig, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices {}\", subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: {} parent shard: {}\", collectionName, slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, cloudConfig, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","sourceOld":"  @SuppressWarnings({\"rawtypes\"})\n  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.syncRequestTracker();\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: {} slice={} leader={}\", splits, slice, parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice {} of collection {} on {}\", subSlice, collectionName, nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica {} as part of slice {} of collection {} on {}\"\n            , subShardName, subSlice, collectionName, nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: {} to be alive on: {}\", subShardName, nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection {} parent shard: {} on: {}\"\n          , collectionName, slice, parentShardLeader);\n\n      if (log.isInfoEnabled()) {\n        log.info(\"Splitting shard {} as part of slice {} of collection {} on {}\"\n            , parentShardLeader.getName(), slice, collectionName, parentShardLeader);\n      }\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      if (log.isDebugEnabled()) {\n        log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n      }\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : {}\", subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : {}\", subShardNames);\n\n      // Replica creation for the new Slices\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard {} as part of slice {} of collection {} on {}\"\n            , solrCoreName, sliceName, collectionName, subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices {}\", subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: {} parent shard: {}\", collectionName, slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7b17e79a71117668ecbf8d3417c876e41396565","date":1598973672,"type":1,"author":"Ilan Ginzburg","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,CloudConfig,ZkNodeProps,NamedList[Object]).mjava","sourceNew":"  @SuppressWarnings({\"rawtypes\"})\n  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.syncRequestTracker();\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: {} slice={} leader={}\", splits, slice, parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice {} of collection {} on {}\", subSlice, collectionName, nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica {} as part of slice {} of collection {} on {}\"\n            , subShardName, subSlice, collectionName, nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: {} to be alive on: {}\", subShardName, nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection {} parent shard: {} on: {}\"\n          , collectionName, slice, parentShardLeader);\n\n      if (log.isInfoEnabled()) {\n        log.info(\"Splitting shard {} as part of slice {} of collection {} on {}\"\n            , parentShardLeader.getName(), slice, collectionName, parentShardLeader);\n      }\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      if (log.isDebugEnabled()) {\n        log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n      }\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : {}\", subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : {}\", subShardNames);\n\n      // Replica creation for the new Slices\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard {} as part of slice {} of collection {} on {}\"\n            , solrCoreName, sliceName, collectionName, subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices {}\", subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: {} parent shard: {}\", collectionName, slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","sourceOld":"  @SuppressWarnings({\"rawtypes\"})\n  public boolean split(ClusterState clusterState, CloudConfig cloudConfig, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.syncRequestTracker();\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: {} slice={} leader={}\", splits, slice, parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, cloudConfig, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice {} of collection {} on {}\", subSlice, collectionName, nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica {} as part of slice {} of collection {} on {}\"\n            , subShardName, subSlice, collectionName, nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, cloudConfig, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: {} to be alive on: {}\", subShardName, nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection {} parent shard: {} on: {}\"\n          , collectionName, slice, parentShardLeader);\n\n      if (log.isInfoEnabled()) {\n        log.info(\"Splitting shard {} as part of slice {} of collection {} on {}\"\n            , parentShardLeader.getName(), slice, collectionName, parentShardLeader);\n      }\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      if (log.isDebugEnabled()) {\n        log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n      }\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : {}\", subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : {}\", subShardNames);\n\n      // Replica creation for the new Slices\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, cloudConfig, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard {} as part of slice {} of collection {} on {}\"\n            , solrCoreName, sliceName, collectionName, subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, cloudConfig, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices {}\", subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: {} parent shard: {}\", collectionName, slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, cloudConfig, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f5d9700b23e8e9b11b845fcecef89dbdf21373d9","date":1600294231,"type":3,"author":"Ilan Ginzburg","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","sourceNew":"  @SuppressWarnings({\"rawtypes\"})\n  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.syncRequestTracker();\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: {} slice={} leader={}\", splits, slice, parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice {} of collection {} on {}\", subSlice, collectionName, nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica {} as part of slice {} of collection {} on {}\"\n            , subShardName, subSlice, collectionName, nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: {} to be alive on: {}\", subShardName, nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection {} parent shard: {} on: {}\"\n          , collectionName, slice, parentShardLeader);\n\n      if (log.isInfoEnabled()) {\n        log.info(\"Splitting shard {} as part of slice {} of collection {} on {}\"\n            , parentShardLeader.getName(), slice, collectionName, parentShardLeader);\n      }\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      if (log.isDebugEnabled()) {\n        log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n      }\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : {}\", subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : {}\", subShardNames);\n\n      // Replica creation for the new Slices\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategy assignStrategy = Assign.createAssignStrategy(ocmh.cloudManager, clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard {} as part of slice {} of collection {} on {}\"\n            , solrCoreName, sliceName, collectionName, subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices {}\", subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: {} parent shard: {}\", collectionName, slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","sourceOld":"  @SuppressWarnings({\"rawtypes\"})\n  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.syncRequestTracker();\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: {} slice={} leader={}\", splits, slice, parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice {} of collection {} on {}\", subSlice, collectionName, nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica {} as part of slice {} of collection {} on {}\"\n            , subShardName, subSlice, collectionName, nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: {} to be alive on: {}\", subShardName, nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection {} parent shard: {} on: {}\"\n          , collectionName, slice, parentShardLeader);\n\n      if (log.isInfoEnabled()) {\n        log.info(\"Splitting shard {} as part of slice {} of collection {} on {}\"\n            , parentShardLeader.getName(), slice, collectionName, parentShardLeader);\n      }\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      if (log.isDebugEnabled()) {\n        log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n      }\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : {}\", subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : {}\", subShardNames);\n\n      // Replica creation for the new Slices\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard {} as part of slice {} of collection {} on {}\"\n            , solrCoreName, sliceName, collectionName, subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices {}\", subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: {} parent shard: {}\", collectionName, slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"65352f844eb9e9a677ec4eb2abced4404f08181d","date":1600297608,"type":3,"author":"noblepaul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList[Object]).mjava","sourceNew":"  @SuppressWarnings({\"rawtypes\"})\n  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.syncRequestTracker();\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: {} slice={} leader={}\", splits, slice, parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice {} of collection {} on {}\", subSlice, collectionName, nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica {} as part of slice {} of collection {} on {}\"\n            , subShardName, subSlice, collectionName, nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: {} to be alive on: {}\", subShardName, nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection {} parent shard: {} on: {}\"\n          , collectionName, slice, parentShardLeader);\n\n      if (log.isInfoEnabled()) {\n        log.info(\"Splitting shard {} as part of slice {} of collection {} on {}\"\n            , parentShardLeader.getName(), slice, collectionName, parentShardLeader);\n      }\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      if (log.isDebugEnabled()) {\n        log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n      }\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : {}\", subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : {}\", subShardNames);\n\n      // Replica creation for the new Slices\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategy assignStrategy = Assign.createAssignStrategy(ocmh.cloudManager, clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard {} as part of slice {} of collection {} on {}\"\n            , solrCoreName, sliceName, collectionName, subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices {}\", subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: {} parent shard: {}\", collectionName, slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","sourceOld":"  @SuppressWarnings({\"rawtypes\"})\n  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList<Object> results) throws Exception {\n    final String asyncId = message.getStr(ASYNC);\n\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String methodStr = message.getStr(CommonAdminParams.SPLIT_METHOD, SolrIndexSplitter.SplitMethod.REWRITE.toLower());\n    SolrIndexSplitter.SplitMethod splitMethod = SolrIndexSplitter.SplitMethod.get(methodStr);\n    if (splitMethod == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown value '\" + CommonAdminParams.SPLIT_METHOD +\n          \": \" + methodStr);\n    }\n    boolean withTiming = message.getBool(CommonParams.TIMING, false);\n\n    String extCollectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collectionName;\n    if (followAliases) {\n      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);\n    } else {\n      collectionName = extCollectionName;\n    }\n\n    log.debug(\"Split shard invoked: {}\", message);\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n    Set<String> offlineSlices = new HashSet<>();\n    RTimerTree timings = new RTimerTree();\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n    if (parentSlice.getState() != Slice.State.ACTIVE) {\n      throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Parent slice is not active: \" +\n          collectionName + \"/ \" + parentSlice.getName() + \", state=\" + parentSlice.getState());\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Interrupted.\");\n    }\n\n    RTimerTree t = timings.sub(\"checkDiskSpace\");\n    checkDiskSpace(collectionName, slice.get(), parentShardLeader, splitMethod, ocmh.cloudManager);\n    t.stop();\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    // reproduce the currently existing number of replicas per type\n    AtomicInteger numNrt = new AtomicInteger();\n    AtomicInteger numTlog = new AtomicInteger();\n    AtomicInteger numPull = new AtomicInteger();\n    parentSlice.getReplicas().forEach(r -> {\n      switch (r.getType()) {\n        case NRT:\n          numNrt.incrementAndGet();\n          break;\n        case TLOG:\n          numTlog.incrementAndGet();\n          break;\n        case PULL:\n          numPull.incrementAndGet();\n      }\n    });\n    int repFactor = numNrt.get() + numTlog.get() + numPull.get();\n\n    boolean success = false;\n    try {\n      // type of the first subreplica will be the same as leader\n      boolean firstNrtReplica = parentShardLeader.getType() == Replica.Type.NRT;\n      // verify that we indeed have the right number of correct replica types\n      if ((firstNrtReplica && numNrt.get() < 1) || (!firstNrtReplica && numTlog.get() < 1)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"aborting split - inconsistent replica types in collection \" + collectionName +\n            \": nrt=\" + numNrt.get() + \", tlog=\" + numTlog.get() + \", pull=\" + numPull.get() + \", shard leader type is \" +\n            parentShardLeader.getType());\n      }\n\n      // check for the lock\n      if (!lockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName())) {\n        // mark as success to avoid clearing the lock in the \"finally\" block\n        success = true;\n        throw new SolrException(SolrException.ErrorCode.INVALID_STATE, \"Can't lock parent slice for splitting (another split operation running?): \" +\n            collectionName + \"/\" + parentSlice.getName());\n      }\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      @SuppressWarnings(\"deprecation\")\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n\n      if (message.getBool(CommonAdminParams.SPLIT_BY_PREFIX, false)) {\n        t = timings.sub(\"getRanges\");\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.GET_RANGES, \"true\");\n        params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        // Only 2 is currently supported\n        // int numSubShards = message.getInt(NUM_SUB_SHARDS, DEFAULT_NUM_SUB_SHARDS);\n        // params.set(NUM_SUB_SHARDS, Integer.toString(numSubShards));\n\n        {\n          final ShardRequestTracker shardRequestTracker = ocmh.syncRequestTracker();\n          shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n          SimpleOrderedMap<Object> getRangesResults = new SimpleOrderedMap<>();\n          String msgOnError = \"SPLITSHARD failed to invoke SPLIT.getRanges core admin command\";\n          shardRequestTracker.processResponses(getRangesResults, shardHandler, true, msgOnError);\n\n          // Extract the recommended splits from the shard response (if it exists)\n          // example response: getRangesResults={success={127.0.0.1:62086_solr={responseHeader={status=0,QTime=1},ranges=10-20,3a-3f}}}\n          NamedList successes = (NamedList)getRangesResults.get(\"success\");\n          if (successes != null && successes.size() > 0) {\n            NamedList shardRsp = (NamedList)successes.getVal(0);\n            String splits = (String)shardRsp.get(CoreAdminParams.RANGES);\n            if (splits != null) {\n              log.info(\"Resulting split ranges to be used: {} slice={} leader={}\", splits, slice, parentShardLeader);\n              // change the message to use the recommended split ranges\n              message = message.plus(CoreAdminParams.RANGES, splits);\n            }\n          }\n        }\n\n        t.stop();\n      }\n\n\n      t = timings.sub(\"fillRanges\");\n\n      String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, firstNrtReplica);\n      t.stop();\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      String nodeName = parentShardLeader.getNodeName();\n\n      t = timings.sub(\"createSubSlicesAndLeadersInState\");\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.debug(\"Creating slice {} of collection {} on {}\", subSlice, collectionName, nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", nodeName);\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state and refresh the local view of the cluster state\n        clusterState = ocmh.waitForNewShard(collectionName, subSlice);\n\n        log.debug(\"Adding first replica {} as part of slice {} of collection {} on {}\"\n            , subShardName, subSlice, collectionName, nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(REPLICA_TYPE, firstNrtReplica ? Replica.Type.NRT.toString() : Replica.Type.TLOG.toString());\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard leaders\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n      t = timings.sub(\"waitForSubSliceLeadersAlive\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.debug(\"Asking parent leader to wait for: {} to be alive on: {}\", subShardName, nodeName);\n          String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          shardRequestTracker.sendShardRequest(nodeName, p, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD timed out waiting for subshard leaders to come up\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully created all sub-shards for collection {} parent shard: {} on: {}\"\n          , collectionName, slice, parentShardLeader);\n\n      if (log.isInfoEnabled()) {\n        log.info(\"Splitting shard {} as part of slice {} of collection {} on {}\"\n            , parentShardLeader.getName(), slice, collectionName, parentShardLeader);\n      }\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      t = timings.sub(\"splitParentCore\");\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n        shardRequestTracker.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n        String msgOnError = \"SPLITSHARD failed to invoke SPLIT core admin command\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      if (log.isDebugEnabled()) {\n        log.debug(\"Index on shard: {} split into {} successfully\", nodeName, subShardNames.size());\n      }\n\n      t = timings.sub(\"applyBufferedUpdates\");\n      // apply buffered updates on sub-shards\n      {\n        final ShardRequestTracker shardRequestTracker = ocmh.asyncRequestTracker(asyncId);\n\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.debug(\"Applying buffered updates on : {}\", subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          shardRequestTracker.sendShardRequest(nodeName, params, shardHandler);\n        }\n\n        String msgOnError = \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\";\n        shardRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.debug(\"Successfully applied buffered updates on : {}\", subShardNames);\n\n      // Replica creation for the new Slices\n\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      // we have already created one subReplica for each subShard on the parent node.\n      // identify locations for the remaining replicas\n      if (firstNrtReplica) {\n        numNrt.decrementAndGet();\n      } else {\n        numTlog.decrementAndGet();\n      }\n\n      t = timings.sub(\"identifyNodesForReplicas\");\n      Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n          .forCollection(collectionName)\n          .forShard(subSlices)\n          .assignNrtReplicas(numNrt.get())\n          .assignTlogReplicas(numTlog.get())\n          .assignPullReplicas(numPull.get())\n          .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n          .build();\n      Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(ocmh.cloudManager);\n      Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n      List<ReplicaPosition> replicaPositions = assignStrategy.assign(ocmh.cloudManager, assignRequest);\n      t.stop();\n\n      t = timings.sub(\"createReplicaPlaceholders\");\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = Assign.buildSolrCoreName(collectionName, sliceName, replicaPosition.type, replicaPosition.index);\n\n        log.debug(\"Creating replica shard {} as part of slice {} of collection {} on {}\"\n            , solrCoreName, sliceName, collectionName, subShardNodeName);\n\n        // we first create all replicas in DOWN state without actually creating their cores in order to\n        // avoid a race condition where Overseer may prematurely activate the new sub-slices (and deactivate\n        // the parent slice) before all new replicas are added. This situation may lead to a loss of performance\n        // because the new shards will be activated with possibly many fewer replicas.\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.REPLICA_TYPE, replicaPosition.type.name(),\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(REPLICA_TYPE, replicaPosition.type.name());\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n      t.stop();\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside ReplicaMutator to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // A commit is needed so that documents are visible when the sub-shard replicas come up\n        // (Note: This commit used to be after the state switch, but was brought here before the state switch\n        //  as per SOLR-13945 so that sub shards don't come up empty, momentarily, after being marked active) \n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));\n      }\n\n      t = timings.sub(\"createCoresForReplicas\");\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      assert TestInjection.injectSplitFailureAfterReplicaCreation();\n\n      {\n        final ShardRequestTracker syncRequestTracker = ocmh.syncRequestTracker();\n        String msgOnError = \"SPLITSHARD failed to create subshard replicas\";\n        syncRequestTracker.processResponses(results, shardHandler, true, msgOnError);\n        handleFailureOnAsyncRequest(results, msgOnError);\n      }\n      t.stop();\n\n      log.info(\"Successfully created all replica shards for all sub-slices {}\", subSlices);\n\n      // The final commit was added in SOLR-4997 so that documents are visible\n      // when the sub-shard replicas come up\n      if (repFactor > 1) {\n        t = timings.sub(\"finalCommit\");\n        ocmh.commit(results, slice.get(), parentShardLeader);\n        t.stop();\n      }\n\n      if (withTiming) {\n        results.add(CommonParams.TIMING, timings.asNamedList());\n      }\n      success = true;\n      // don't unlock the shard yet - only do this if the final switch-over in\n      // ReplicaMutator succeeds (or fails)\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: {} parent shard: {}\", collectionName, slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (!success) {\n        cleanupAfterFailure(zkStateReader, collectionName, parentSlice.getName(), subSlices, offlineSlices);\n        unlockForSplit(ocmh.cloudManager, collectionName, parentSlice.getName());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"1964c059f45ae1de1877f9f0fe3ca327ea4218e8":["86968c6cf51846df861b8f29bd85b6d9a7c9f19c"],"f5d9700b23e8e9b11b845fcecef89dbdf21373d9":["e7b17e79a71117668ecbf8d3417c876e41396565"],"993b0c7dda6341b437fe5685d35c6cc35eaac420":["1498cb4cd65cf2303ccde0e0d6edf5a5e1e8fd28","8f79e6c597ec19b2f9669589153faac2f803462a"],"c526352db87264a72a7a9ad68c1b769b81e54305":["3f504512a03d978990cbff30db0522b354e846db"],"2f56a372e878021ee5712e3b5651d2bdd2945b07":["36bdabc04743acfe0e82c9cf8208b1111b2b193a"],"8f79e6c597ec19b2f9669589153faac2f803462a":["1498cb4cd65cf2303ccde0e0d6edf5a5e1e8fd28"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["1964c059f45ae1de1877f9f0fe3ca327ea4218e8"],"d20dc32a40c1bd7b8b8e2354e6b0b9f956ef758c":["69e6520a21709190413a63084ed135271aab1a7c"],"1498cb4cd65cf2303ccde0e0d6edf5a5e1e8fd28":["7f125a1877bd8a597f490d08013d70f8f4df12e5"],"df724d84dab24a0cc54bec95a8680867adc7f171":["993b0c7dda6341b437fe5685d35c6cc35eaac420","8a8f0700353755635a948651e84c152bff39d8dc"],"7f125a1877bd8a597f490d08013d70f8f4df12e5":["2f56a372e878021ee5712e3b5651d2bdd2945b07"],"ebccdcfbac56fd8a05e781b2b8cf7a8f1d447e87":["8a8f0700353755635a948651e84c152bff39d8dc"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"36bdabc04743acfe0e82c9cf8208b1111b2b193a":["a05f3f5161c62339ec5560b8f6958f3df8483618"],"e7b17e79a71117668ecbf8d3417c876e41396565":["c526352db87264a72a7a9ad68c1b769b81e54305"],"3f504512a03d978990cbff30db0522b354e846db":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"],"65352f844eb9e9a677ec4eb2abced4404f08181d":["e7b17e79a71117668ecbf8d3417c876e41396565","f5d9700b23e8e9b11b845fcecef89dbdf21373d9"],"a05f3f5161c62339ec5560b8f6958f3df8483618":["d20dc32a40c1bd7b8b8e2354e6b0b9f956ef758c"],"4abf821a41a2fdca2a1dea148999931d22e20529":["ebccdcfbac56fd8a05e781b2b8cf7a8f1d447e87"],"86968c6cf51846df861b8f29bd85b6d9a7c9f19c":["4abf821a41a2fdca2a1dea148999931d22e20529"],"69e6520a21709190413a63084ed135271aab1a7c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"06595b0c22c7d3075c4104d3820cccf95d9d8a43":["df724d84dab24a0cc54bec95a8680867adc7f171","ebccdcfbac56fd8a05e781b2b8cf7a8f1d447e87"],"8a8f0700353755635a948651e84c152bff39d8dc":["8f79e6c597ec19b2f9669589153faac2f803462a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["65352f844eb9e9a677ec4eb2abced4404f08181d"],"b0b597c65628ca9e73913a07e81691f8229bae35":["7f125a1877bd8a597f490d08013d70f8f4df12e5","1498cb4cd65cf2303ccde0e0d6edf5a5e1e8fd28"]},"commit2Childs":{"1964c059f45ae1de1877f9f0fe3ca327ea4218e8":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"],"f5d9700b23e8e9b11b845fcecef89dbdf21373d9":["65352f844eb9e9a677ec4eb2abced4404f08181d"],"993b0c7dda6341b437fe5685d35c6cc35eaac420":["df724d84dab24a0cc54bec95a8680867adc7f171"],"c526352db87264a72a7a9ad68c1b769b81e54305":["e7b17e79a71117668ecbf8d3417c876e41396565"],"2f56a372e878021ee5712e3b5651d2bdd2945b07":["7f125a1877bd8a597f490d08013d70f8f4df12e5"],"8f79e6c597ec19b2f9669589153faac2f803462a":["993b0c7dda6341b437fe5685d35c6cc35eaac420","8a8f0700353755635a948651e84c152bff39d8dc"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["3f504512a03d978990cbff30db0522b354e846db"],"d20dc32a40c1bd7b8b8e2354e6b0b9f956ef758c":["a05f3f5161c62339ec5560b8f6958f3df8483618"],"1498cb4cd65cf2303ccde0e0d6edf5a5e1e8fd28":["993b0c7dda6341b437fe5685d35c6cc35eaac420","8f79e6c597ec19b2f9669589153faac2f803462a","b0b597c65628ca9e73913a07e81691f8229bae35"],"df724d84dab24a0cc54bec95a8680867adc7f171":["06595b0c22c7d3075c4104d3820cccf95d9d8a43"],"7f125a1877bd8a597f490d08013d70f8f4df12e5":["1498cb4cd65cf2303ccde0e0d6edf5a5e1e8fd28","b0b597c65628ca9e73913a07e81691f8229bae35"],"ebccdcfbac56fd8a05e781b2b8cf7a8f1d447e87":["4abf821a41a2fdca2a1dea148999931d22e20529","06595b0c22c7d3075c4104d3820cccf95d9d8a43"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["69e6520a21709190413a63084ed135271aab1a7c"],"36bdabc04743acfe0e82c9cf8208b1111b2b193a":["2f56a372e878021ee5712e3b5651d2bdd2945b07"],"e7b17e79a71117668ecbf8d3417c876e41396565":["f5d9700b23e8e9b11b845fcecef89dbdf21373d9","65352f844eb9e9a677ec4eb2abced4404f08181d"],"3f504512a03d978990cbff30db0522b354e846db":["c526352db87264a72a7a9ad68c1b769b81e54305"],"65352f844eb9e9a677ec4eb2abced4404f08181d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a05f3f5161c62339ec5560b8f6958f3df8483618":["36bdabc04743acfe0e82c9cf8208b1111b2b193a"],"4abf821a41a2fdca2a1dea148999931d22e20529":["86968c6cf51846df861b8f29bd85b6d9a7c9f19c"],"86968c6cf51846df861b8f29bd85b6d9a7c9f19c":["1964c059f45ae1de1877f9f0fe3ca327ea4218e8"],"69e6520a21709190413a63084ed135271aab1a7c":["d20dc32a40c1bd7b8b8e2354e6b0b9f956ef758c"],"06595b0c22c7d3075c4104d3820cccf95d9d8a43":[],"8a8f0700353755635a948651e84c152bff39d8dc":["df724d84dab24a0cc54bec95a8680867adc7f171","ebccdcfbac56fd8a05e781b2b8cf7a8f1d447e87"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"b0b597c65628ca9e73913a07e81691f8229bae35":[]},"heads":["06595b0c22c7d3075c4104d3820cccf95d9d8a43","cd5edd1f2b162a5cfa08efd17851a07373a96817","b0b597c65628ca9e73913a07e81691f8229bae35"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}