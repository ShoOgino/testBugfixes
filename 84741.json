{"path":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,String,ZkCoreNodeProps,boolean).mjava","commits":[{"id":"14d5815ecbef89580f5c48990bcd433f04f8563a","date":1399564106,"type":0,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,String,ZkCoreNodeProps,boolean).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * \n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(final String collection, \n      final String shardId, final String replicaUrl, final ZkCoreNodeProps replicaCoreProps, boolean forcePublishState) \n          throws KeeperException, InterruptedException \n  {\n    \n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {     \n        if (!forcePublishState) {\n          return false; // already in this recovery process\n        }\n      }\n      \n      // if the replica's state is not DOWN right now, make it so ...        \n      String replicaNodeName = replicaCoreProps.getNodeName();      \n      String replicaCoreName = replicaCoreProps.getCoreName();      \n      assert replicaCoreName != null : \"No core name for replica \"+replicaNodeName;\n      \n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        replicasInLeaderInitiatedRecovery.put(replicaUrl, \n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreName));          \n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreName, ZkStateReader.DOWN);\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n      }      \n    }    \n    \n    String replicaCoreName = replicaCoreProps.getCoreName();    \n    ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\", \n        ZkStateReader.STATE_PROP, ZkStateReader.DOWN, \n        ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(), \n        ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n        ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n        ZkStateReader.SHARD_ID_PROP, shardId,\n        ZkStateReader.COLLECTION_PROP, collection);\n    log.warn(\"Leader is publishing core={} state={} on behalf of un-reachable replica {}; forcePublishState? \"+forcePublishState,\n        replicaCoreName, ZkStateReader.DOWN, replicaUrl);\n    overseerJobQueue.offer(ZkStateReader.toJSON(m));\n    \n    return nodeIsLive;\n  }  \n\n","sourceOld":null,"bugFix":null,"bugIntro":["b20a501ed59b81e03d60934d4a28f51d3d075395","ed16f67b6471c8066bb85c9f45ec7e01a41a7cbc"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"aea1d78da2c058b98e64569bcd37981c733b52a8","date":1400551646,"type":3,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,String,ZkCoreNodeProps,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,String,ZkCoreNodeProps,boolean).mjava","sourceNew":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * \n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(final String collection, \n      final String shardId, final String replicaUrl, final ZkCoreNodeProps replicaCoreProps, boolean forcePublishState) \n          throws KeeperException, InterruptedException \n  {    \n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n    \n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n    \n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {     \n        if (!forcePublishState) {\n          return false; // already in this recovery process\n        }\n      }\n      \n      // if the replica's state is not DOWN right now, make it so ...        \n      String replicaNodeName = replicaCoreProps.getNodeName();      \n      String replicaCoreName = replicaCoreProps.getCoreName();      \n      assert replicaCoreName != null : \"No core name for replica \"+replicaNodeName;\n      \n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        replicasInLeaderInitiatedRecovery.put(replicaUrl, \n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreName));          \n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreName, ZkStateReader.DOWN);\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n      }      \n    }    \n    \n    String replicaCoreName = replicaCoreProps.getCoreName();    \n    ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\", \n        ZkStateReader.STATE_PROP, ZkStateReader.DOWN, \n        ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(), \n        ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n        ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n        ZkStateReader.SHARD_ID_PROP, shardId,\n        ZkStateReader.COLLECTION_PROP, collection);\n    log.warn(\"Leader is publishing core={} state={} on behalf of un-reachable replica {}; forcePublishState? \"+forcePublishState,\n        replicaCoreName, ZkStateReader.DOWN, replicaUrl);\n    overseerJobQueue.offer(ZkStateReader.toJSON(m));\n    \n    return nodeIsLive;\n  }  \n\n","sourceOld":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * \n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(final String collection, \n      final String shardId, final String replicaUrl, final ZkCoreNodeProps replicaCoreProps, boolean forcePublishState) \n          throws KeeperException, InterruptedException \n  {\n    \n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {     \n        if (!forcePublishState) {\n          return false; // already in this recovery process\n        }\n      }\n      \n      // if the replica's state is not DOWN right now, make it so ...        \n      String replicaNodeName = replicaCoreProps.getNodeName();      \n      String replicaCoreName = replicaCoreProps.getCoreName();      \n      assert replicaCoreName != null : \"No core name for replica \"+replicaNodeName;\n      \n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        replicasInLeaderInitiatedRecovery.put(replicaUrl, \n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreName));          \n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreName, ZkStateReader.DOWN);\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n      }      \n    }    \n    \n    String replicaCoreName = replicaCoreProps.getCoreName();    \n    ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\", \n        ZkStateReader.STATE_PROP, ZkStateReader.DOWN, \n        ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(), \n        ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n        ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n        ZkStateReader.SHARD_ID_PROP, shardId,\n        ZkStateReader.COLLECTION_PROP, collection);\n    log.warn(\"Leader is publishing core={} state={} on behalf of un-reachable replica {}; forcePublishState? \"+forcePublishState,\n        replicaCoreName, ZkStateReader.DOWN, replicaUrl);\n    overseerJobQueue.offer(ZkStateReader.toJSON(m));\n    \n    return nodeIsLive;\n  }  \n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7605579001505896d48b07160075a5c8b8e128e","date":1400758727,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,String,ZkCoreNodeProps,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,String,ZkCoreNodeProps,boolean).mjava","sourceNew":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * \n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(final String collection, \n      final String shardId, final String replicaUrl, final ZkCoreNodeProps replicaCoreProps, boolean forcePublishState) \n          throws KeeperException, InterruptedException \n  {    \n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n    \n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n    \n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {     \n        if (!forcePublishState) {\n          return false; // already in this recovery process\n        }\n      }\n      \n      // if the replica's state is not DOWN right now, make it so ...        \n      String replicaNodeName = replicaCoreProps.getNodeName();      \n      String replicaCoreName = replicaCoreProps.getCoreName();      \n      assert replicaCoreName != null : \"No core name for replica \"+replicaNodeName;\n      \n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        replicasInLeaderInitiatedRecovery.put(replicaUrl, \n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreName));          \n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreName, ZkStateReader.DOWN);\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n      }      \n    }    \n    \n    String replicaCoreName = replicaCoreProps.getCoreName();    \n    ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\", \n        ZkStateReader.STATE_PROP, ZkStateReader.DOWN, \n        ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(), \n        ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n        ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n        ZkStateReader.SHARD_ID_PROP, shardId,\n        ZkStateReader.COLLECTION_PROP, collection);\n    log.warn(\"Leader is publishing core={} state={} on behalf of un-reachable replica {}; forcePublishState? \"+forcePublishState,\n        replicaCoreName, ZkStateReader.DOWN, replicaUrl);\n    overseerJobQueue.offer(ZkStateReader.toJSON(m));\n    \n    return nodeIsLive;\n  }  \n\n","sourceOld":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * \n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(final String collection, \n      final String shardId, final String replicaUrl, final ZkCoreNodeProps replicaCoreProps, boolean forcePublishState) \n          throws KeeperException, InterruptedException \n  {\n    \n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {     \n        if (!forcePublishState) {\n          return false; // already in this recovery process\n        }\n      }\n      \n      // if the replica's state is not DOWN right now, make it so ...        \n      String replicaNodeName = replicaCoreProps.getNodeName();      \n      String replicaCoreName = replicaCoreProps.getCoreName();      \n      assert replicaCoreName != null : \"No core name for replica \"+replicaNodeName;\n      \n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        replicasInLeaderInitiatedRecovery.put(replicaUrl, \n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreName));          \n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreName, ZkStateReader.DOWN);\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n      }      \n    }    \n    \n    String replicaCoreName = replicaCoreProps.getCoreName();    \n    ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\", \n        ZkStateReader.STATE_PROP, ZkStateReader.DOWN, \n        ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(), \n        ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n        ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n        ZkStateReader.SHARD_ID_PROP, shardId,\n        ZkStateReader.COLLECTION_PROP, collection);\n    log.warn(\"Leader is publishing core={} state={} on behalf of un-reachable replica {}; forcePublishState? \"+forcePublishState,\n        replicaCoreName, ZkStateReader.DOWN, replicaUrl);\n    overseerJobQueue.offer(ZkStateReader.toJSON(m));\n    \n    return nodeIsLive;\n  }  \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b20a501ed59b81e03d60934d4a28f51d3d075395","date":1403628473,"type":3,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,String,ZkCoreNodeProps,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,String,ZkCoreNodeProps,boolean).mjava","sourceNew":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * \n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(final String collection, \n      final String shardId, final String replicaUrl, final ZkCoreNodeProps replicaCoreProps, boolean forcePublishState) \n          throws KeeperException, InterruptedException \n  {    \n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n    \n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n    \n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    boolean publishDownState = false;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {     \n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n      \n      // if the replica's state is not DOWN right now, make it so ...        \n      String replicaNodeName = replicaCoreProps.getNodeName();      \n      String replicaCoreName = replicaCoreProps.getCoreName();      \n      assert replicaCoreName != null : \"No core name for replica \"+replicaNodeName;\n      \n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        replicasInLeaderInitiatedRecovery.put(replicaUrl, \n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreName));          \n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreName, ZkStateReader.DOWN);\n        log.info(\"Put replica \"+replicaCoreName+\" on \"+\n          replicaNodeName+\" into leader-initiated recovery.\");\n        publishDownState = true;        \n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \"+replicaNodeName+\n          \" is not live, so skipping leader-initiated recovery for replica: \"+\n          replicaCoreName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }      \n    }    \n    \n    if (publishDownState || forcePublishState) {\n      String replicaCoreName = replicaCoreProps.getCoreName();    \n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\", \n          ZkStateReader.STATE_PROP, ZkStateReader.DOWN, \n          ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(), \n          ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n          ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n          ZkStateReader.SHARD_ID_PROP, shardId,\n          ZkStateReader.COLLECTION_PROP, collection);\n      log.warn(\"Leader is publishing core={} state={} on behalf of un-reachable replica {}; forcePublishState? \"+forcePublishState,\n          replicaCoreName, ZkStateReader.DOWN, replicaUrl);\n      overseerJobQueue.offer(ZkStateReader.toJSON(m));      \n    }\n    \n    return nodeIsLive;\n  }  \n\n","sourceOld":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * \n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(final String collection, \n      final String shardId, final String replicaUrl, final ZkCoreNodeProps replicaCoreProps, boolean forcePublishState) \n          throws KeeperException, InterruptedException \n  {    \n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n    \n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n    \n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {     \n        if (!forcePublishState) {\n          return false; // already in this recovery process\n        }\n      }\n      \n      // if the replica's state is not DOWN right now, make it so ...        \n      String replicaNodeName = replicaCoreProps.getNodeName();      \n      String replicaCoreName = replicaCoreProps.getCoreName();      \n      assert replicaCoreName != null : \"No core name for replica \"+replicaNodeName;\n      \n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        replicasInLeaderInitiatedRecovery.put(replicaUrl, \n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreName));          \n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreName, ZkStateReader.DOWN);\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n      }      \n    }    \n    \n    String replicaCoreName = replicaCoreProps.getCoreName();    \n    ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\", \n        ZkStateReader.STATE_PROP, ZkStateReader.DOWN, \n        ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(), \n        ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n        ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n        ZkStateReader.SHARD_ID_PROP, shardId,\n        ZkStateReader.COLLECTION_PROP, collection);\n    log.warn(\"Leader is publishing core={} state={} on behalf of un-reachable replica {}; forcePublishState? \"+forcePublishState,\n        replicaCoreName, ZkStateReader.DOWN, replicaUrl);\n    overseerJobQueue.offer(ZkStateReader.toJSON(m));\n    \n    return nodeIsLive;\n  }  \n\n","bugFix":["14d5815ecbef89580f5c48990bcd433f04f8563a"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"07c824e7f6927860d366e4888be45e4db8c9e03b","date":1405193679,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,String,ZkCoreNodeProps,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,String,ZkCoreNodeProps,boolean).mjava","sourceNew":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * \n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(final String collection, \n      final String shardId, final String replicaUrl, final ZkCoreNodeProps replicaCoreProps, boolean forcePublishState) \n          throws KeeperException, InterruptedException \n  {    \n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n    \n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n    \n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    boolean publishDownState = false;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica)replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \"+replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {     \n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // if the replica's state is not DOWN right now, make it so ...\n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        replicasInLeaderInitiatedRecovery.put(replicaUrl, \n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName, ZkStateReader.DOWN);\n        log.info(\"Put replica core={} coreNodeName={} on \"+\n          replicaNodeName+\" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        publishDownState = true;        \n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }      \n    }    \n    \n    if (publishDownState || forcePublishState) {\n      String replicaCoreName = replicaCoreProps.getCoreName();    \n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\", \n          ZkStateReader.STATE_PROP, ZkStateReader.DOWN, \n          ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(), \n          ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n          ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n          ZkStateReader.SHARD_ID_PROP, shardId,\n          ZkStateReader.COLLECTION_PROP, collection);\n      log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}; forcePublishState? \"+forcePublishState,\n          replicaCoreName, replicaCoreNodeName, ZkStateReader.DOWN, replicaUrl);\n      overseerJobQueue.offer(ZkStateReader.toJSON(m));      \n    }\n    \n    return nodeIsLive;\n  }  \n\n","sourceOld":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * \n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(final String collection, \n      final String shardId, final String replicaUrl, final ZkCoreNodeProps replicaCoreProps, boolean forcePublishState) \n          throws KeeperException, InterruptedException \n  {    \n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n    \n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n    \n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    boolean publishDownState = false;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {     \n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n      \n      // if the replica's state is not DOWN right now, make it so ...        \n      String replicaNodeName = replicaCoreProps.getNodeName();      \n      String replicaCoreName = replicaCoreProps.getCoreName();      \n      assert replicaCoreName != null : \"No core name for replica \"+replicaNodeName;\n      \n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        replicasInLeaderInitiatedRecovery.put(replicaUrl, \n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreName));          \n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreName, ZkStateReader.DOWN);\n        log.info(\"Put replica \"+replicaCoreName+\" on \"+\n          replicaNodeName+\" into leader-initiated recovery.\");\n        publishDownState = true;        \n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \"+replicaNodeName+\n          \" is not live, so skipping leader-initiated recovery for replica: \"+\n          replicaCoreName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }      \n    }    \n    \n    if (publishDownState || forcePublishState) {\n      String replicaCoreName = replicaCoreProps.getCoreName();    \n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\", \n          ZkStateReader.STATE_PROP, ZkStateReader.DOWN, \n          ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(), \n          ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n          ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n          ZkStateReader.SHARD_ID_PROP, shardId,\n          ZkStateReader.COLLECTION_PROP, collection);\n      log.warn(\"Leader is publishing core={} state={} on behalf of un-reachable replica {}; forcePublishState? \"+forcePublishState,\n          replicaCoreName, ZkStateReader.DOWN, replicaUrl);\n      overseerJobQueue.offer(ZkStateReader.toJSON(m));      \n    }\n    \n    return nodeIsLive;\n  }  \n\n","bugFix":null,"bugIntro":["ed16f67b6471c8066bb85c9f45ec7e01a41a7cbc"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ed16f67b6471c8066bb85c9f45ec7e01a41a7cbc","date":1415656344,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,String,ZkCoreNodeProps,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,String,ZkCoreNodeProps,boolean).mjava","sourceNew":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * \n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(final String collection, \n      final String shardId, final String replicaUrl, final ZkCoreNodeProps replicaCoreProps, boolean forcePublishState) \n          throws KeeperException, InterruptedException \n  {    \n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n    \n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n    \n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    boolean publishDownState = false;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica)replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \"+replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {     \n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // if the replica's state is not DOWN right now, make it so ...\n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName, ZkStateReader.DOWN);\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \"+\n          replicaNodeName+\" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        publishDownState = true;        \n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }      \n    }    \n    \n    if (publishDownState || forcePublishState) {\n      String replicaCoreName = replicaCoreProps.getCoreName();    \n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\", \n          ZkStateReader.STATE_PROP, ZkStateReader.DOWN, \n          ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(), \n          ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n          ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n          ZkStateReader.SHARD_ID_PROP, shardId,\n          ZkStateReader.COLLECTION_PROP, collection);\n      log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}; forcePublishState? \"+forcePublishState,\n          replicaCoreName, replicaCoreNodeName, ZkStateReader.DOWN, replicaUrl);\n      overseerJobQueue.offer(ZkStateReader.toJSON(m));      \n    }\n    \n    return nodeIsLive;\n  }\n\n","sourceOld":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * \n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(final String collection, \n      final String shardId, final String replicaUrl, final ZkCoreNodeProps replicaCoreProps, boolean forcePublishState) \n          throws KeeperException, InterruptedException \n  {    \n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n    \n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n    \n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    boolean publishDownState = false;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica)replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \"+replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {     \n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // if the replica's state is not DOWN right now, make it so ...\n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        replicasInLeaderInitiatedRecovery.put(replicaUrl, \n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName, ZkStateReader.DOWN);\n        log.info(\"Put replica core={} coreNodeName={} on \"+\n          replicaNodeName+\" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        publishDownState = true;        \n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }      \n    }    \n    \n    if (publishDownState || forcePublishState) {\n      String replicaCoreName = replicaCoreProps.getCoreName();    \n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\", \n          ZkStateReader.STATE_PROP, ZkStateReader.DOWN, \n          ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(), \n          ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n          ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n          ZkStateReader.SHARD_ID_PROP, shardId,\n          ZkStateReader.COLLECTION_PROP, collection);\n      log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}; forcePublishState? \"+forcePublishState,\n          replicaCoreName, replicaCoreNodeName, ZkStateReader.DOWN, replicaUrl);\n      overseerJobQueue.offer(ZkStateReader.toJSON(m));      \n    }\n    \n    return nodeIsLive;\n  }  \n\n","bugFix":["07c824e7f6927860d366e4888be45e4db8c9e03b","14d5815ecbef89580f5c48990bcd433f04f8563a"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc","date":1426444850,"type":5,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,ZkCoreNodeProps,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,String,ZkCoreNodeProps,boolean).mjava","sourceNew":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * \n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(final String collection, \n      final String shardId, final ZkCoreNodeProps replicaCoreProps, boolean forcePublishState, String leaderCoreNodeName)\n          throws KeeperException, InterruptedException \n  {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n    \n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n    \n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    boolean publishDownState = false;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica)replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \"+replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {     \n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // if the replica's state is not DOWN right now, make it so ...\n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName, ZkStateReader.DOWN, leaderCoreNodeName);\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \"+\n          replicaNodeName+\" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        publishDownState = true;        \n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }      \n    }    \n    \n    if (publishDownState || forcePublishState) {\n      String replicaCoreName = replicaCoreProps.getCoreName();    \n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\", \n          ZkStateReader.STATE_PROP, ZkStateReader.DOWN, \n          ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(), \n          ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n          ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n          ZkStateReader.SHARD_ID_PROP, shardId,\n          ZkStateReader.COLLECTION_PROP, collection);\n      log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}; forcePublishState? \"+forcePublishState,\n          replicaCoreName, replicaCoreNodeName, ZkStateReader.DOWN, replicaUrl);\n      overseerJobQueue.offer(ZkStateReader.toJSON(m));      \n    }\n    \n    return nodeIsLive;\n  }\n\n","sourceOld":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * \n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(final String collection, \n      final String shardId, final String replicaUrl, final ZkCoreNodeProps replicaCoreProps, boolean forcePublishState) \n          throws KeeperException, InterruptedException \n  {    \n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n    \n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n    \n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    boolean publishDownState = false;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica)replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \"+replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {     \n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // if the replica's state is not DOWN right now, make it so ...\n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName, ZkStateReader.DOWN);\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \"+\n          replicaNodeName+\" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        publishDownState = true;        \n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }      \n    }    \n    \n    if (publishDownState || forcePublishState) {\n      String replicaCoreName = replicaCoreProps.getCoreName();    \n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\", \n          ZkStateReader.STATE_PROP, ZkStateReader.DOWN, \n          ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(), \n          ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n          ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n          ZkStateReader.SHARD_ID_PROP, shardId,\n          ZkStateReader.COLLECTION_PROP, collection);\n      log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}; forcePublishState? \"+forcePublishState,\n          replicaCoreName, replicaCoreNodeName, ZkStateReader.DOWN, replicaUrl);\n      overseerJobQueue.offer(ZkStateReader.toJSON(m));      \n    }\n    \n    return nodeIsLive;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":4,"author":"Ryan Ernst","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,String,ZkCoreNodeProps,boolean).mjava","sourceNew":null,"sourceOld":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * \n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(final String collection, \n      final String shardId, final String replicaUrl, final ZkCoreNodeProps replicaCoreProps, boolean forcePublishState) \n          throws KeeperException, InterruptedException \n  {    \n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n    \n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n    \n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    boolean publishDownState = false;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica)replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \"+replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {     \n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // if the replica's state is not DOWN right now, make it so ...\n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName, ZkStateReader.DOWN);\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \"+\n          replicaNodeName+\" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        publishDownState = true;        \n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }      \n    }    \n    \n    if (publishDownState || forcePublishState) {\n      String replicaCoreName = replicaCoreProps.getCoreName();    \n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\", \n          ZkStateReader.STATE_PROP, ZkStateReader.DOWN, \n          ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(), \n          ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n          ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n          ZkStateReader.SHARD_ID_PROP, shardId,\n          ZkStateReader.COLLECTION_PROP, collection);\n      log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}; forcePublishState? \"+forcePublishState,\n          replicaCoreName, replicaCoreNodeName, ZkStateReader.DOWN, replicaUrl);\n      overseerJobQueue.offer(ZkStateReader.toJSON(m));      \n    }\n    \n    return nodeIsLive;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc":["ed16f67b6471c8066bb85c9f45ec7e01a41a7cbc"],"ed16f67b6471c8066bb85c9f45ec7e01a41a7cbc":["07c824e7f6927860d366e4888be45e4db8c9e03b"],"b20a501ed59b81e03d60934d4a28f51d3d075395":["aea1d78da2c058b98e64569bcd37981c733b52a8"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["ed16f67b6471c8066bb85c9f45ec7e01a41a7cbc","dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc"],"07c824e7f6927860d366e4888be45e4db8c9e03b":["b20a501ed59b81e03d60934d4a28f51d3d075395"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"14d5815ecbef89580f5c48990bcd433f04f8563a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"aea1d78da2c058b98e64569bcd37981c733b52a8":["14d5815ecbef89580f5c48990bcd433f04f8563a"],"b7605579001505896d48b07160075a5c8b8e128e":["14d5815ecbef89580f5c48990bcd433f04f8563a","aea1d78da2c058b98e64569bcd37981c733b52a8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc"]},"commit2Childs":{"dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ed16f67b6471c8066bb85c9f45ec7e01a41a7cbc":["dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"b20a501ed59b81e03d60934d4a28f51d3d075395":["07c824e7f6927860d366e4888be45e4db8c9e03b"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"07c824e7f6927860d366e4888be45e4db8c9e03b":["ed16f67b6471c8066bb85c9f45ec7e01a41a7cbc"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["14d5815ecbef89580f5c48990bcd433f04f8563a"],"14d5815ecbef89580f5c48990bcd433f04f8563a":["aea1d78da2c058b98e64569bcd37981c733b52a8","b7605579001505896d48b07160075a5c8b8e128e"],"aea1d78da2c058b98e64569bcd37981c733b52a8":["b20a501ed59b81e03d60934d4a28f51d3d075395","b7605579001505896d48b07160075a5c8b8e128e"],"b7605579001505896d48b07160075a5c8b8e128e":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b7605579001505896d48b07160075a5c8b8e128e","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}