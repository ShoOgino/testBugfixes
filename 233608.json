{"path":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","commits":[{"id":"a7e4907084808af8fdb14b9809e6dceaccf6867b","date":1343473006,"type":1,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    StorableField[] docFields = doc.getFields(fieldName);\n    List<String> listFields = new ArrayList<String>();\n    for (StorableField field : docFields) {\n      listFields.add(field.stringValue());\n    }\n\n    String[] docTexts = (String[]) listFields.toArray(new String[listFields.size()]);\n   \n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    IndexableField[] docFields = doc.getFields(fieldName);\n    List<String> listFields = new ArrayList<String>();\n    for (IndexableField field : docFields) {\n      listFields.add(field.stringValue());\n    }\n\n    String[] docTexts = (String[]) listFields.toArray(new String[listFields.size()]);\n   \n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1d028314cced5858683a1bb4741423d0f934257b","date":1346596535,"type":1,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    StorableField[] docFields = doc.getFields(fieldName);\n    List<String> listFields = new ArrayList<String>();\n    for (StorableField field : docFields) {\n      listFields.add(field.stringValue());\n    }\n\n    String[] docTexts = (String[]) listFields.toArray(new String[listFields.size()]);\n   \n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    IndexableField[] docFields = doc.getFields(fieldName);\n    List<String> listFields = new ArrayList<String>();\n    for (IndexableField field : docFields) {\n      listFields.add(field.stringValue());\n    }\n\n    String[] docTexts = (String[]) listFields.toArray(new String[listFields.size()]);\n   \n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"66902e88b46bfaaa034e1a881715d5cde7c01146","date":1349210754,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    StorableField[] docFields = doc.getFields(fieldName);\n    List<String> listFields = new ArrayList<String>();\n    for (StorableField field : docFields) {\n      listFields.add(field.stringValue());\n    }\n\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    String[] docTexts = (String[]) listFields.toArray(new String[listFields.size()]);\n   \n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if (preserveMulti) {\n            if (bestTextFragments[k] != null) {\n              frags.add(bestTextFragments[k]);\n            }\n          } else {\n            if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n              frags.add(bestTextFragments[k]);\n            }\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n     if(!preserveMulti){\n        Collections.sort(frags, new Comparator<TextFragment>() {\n                public int compare(TextFragment arg0, TextFragment arg1) {\n                 return Math.round(arg1.getScore() - arg0.getScore());\n        }\n        });\n     }\n\n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if (preserveMulti) {\n          if (fragment != null) {\n            fragTexts.add(fragment.toString());\n          }\n        } else {\n          if ((fragment != null) && (fragment.getScore() > 0)) {\n            fragTexts.add(fragment.toString());\n          }\n        }\n\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    StorableField[] docFields = doc.getFields(fieldName);\n    List<String> listFields = new ArrayList<String>();\n    for (StorableField field : docFields) {\n      listFields.add(field.stringValue());\n    }\n\n    String[] docTexts = (String[]) listFields.toArray(new String[listFields.size()]);\n   \n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cf27653a0ff79ebec6be4f231cadb121fafae33","date":1349816608,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    StorableField[] docFields = doc.getFields(fieldName);\n    List<String> listFields = new ArrayList<String>();\n    for (StorableField field : docFields) {\n      listFields.add(field.stringValue());\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    String[] docTexts = (String[]) listFields.toArray(new String[listFields.size()]);\n   \n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if (preserveMulti) {\n            if (bestTextFragments[k] != null) {\n              frags.add(bestTextFragments[k]);\n            }\n          } else {\n            if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n              frags.add(bestTextFragments[k]);\n            }\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n     if(!preserveMulti){\n        Collections.sort(frags, new Comparator<TextFragment>() {\n                public int compare(TextFragment arg0, TextFragment arg1) {\n                 return Math.round(arg1.getScore() - arg0.getScore());\n        }\n        });\n     }\n\n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if (preserveMulti) {\n          if (fragment != null) {\n            fragTexts.add(fragment.toString());\n          }\n        } else {\n          if ((fragment != null) && (fragment.getScore() > 0)) {\n            fragTexts.add(fragment.toString());\n          }\n        }\n\n        if (fragTexts.size() >= numFragments && !preserveMulti) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    StorableField[] docFields = doc.getFields(fieldName);\n    List<String> listFields = new ArrayList<String>();\n    for (StorableField field : docFields) {\n      listFields.add(field.stringValue());\n    }\n\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    String[] docTexts = (String[]) listFields.toArray(new String[listFields.size()]);\n   \n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if (preserveMulti) {\n            if (bestTextFragments[k] != null) {\n              frags.add(bestTextFragments[k]);\n            }\n          } else {\n            if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n              frags.add(bestTextFragments[k]);\n            }\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n     if(!preserveMulti){\n        Collections.sort(frags, new Comparator<TextFragment>() {\n                public int compare(TextFragment arg0, TextFragment arg1) {\n                 return Math.round(arg1.getScore() - arg0.getScore());\n        }\n        });\n     }\n\n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if (preserveMulti) {\n          if (fragment != null) {\n            fragTexts.add(fragment.toString());\n          }\n        } else {\n          if ((fragment != null) && (fragment.getScore() > 0)) {\n            fragTexts.add(fragment.toString());\n          }\n        }\n\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"815287248ca7a77db68038baad5698c5767f36a7","date":1350761762,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    StorableField[] docFields = doc.getFields(fieldName);\n    List<String> listFields = new ArrayList<String>();\n    for (StorableField field : docFields) {\n      listFields.add(field.stringValue());\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    String[] docTexts = (String[]) listFields.toArray(new String[listFields.size()]);\n   \n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    TokenStream tvStream = TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    if (tvStream != null) {\n      tots = new TermOffsetsTokenStream(tvStream);\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if (preserveMulti) {\n            if (bestTextFragments[k] != null) {\n              frags.add(bestTextFragments[k]);\n            }\n          } else {\n            if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n              frags.add(bestTextFragments[k]);\n            }\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n     if(!preserveMulti){\n        Collections.sort(frags, new Comparator<TextFragment>() {\n                public int compare(TextFragment arg0, TextFragment arg1) {\n                 return Math.round(arg1.getScore() - arg0.getScore());\n        }\n        });\n     }\n\n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if (preserveMulti) {\n          if (fragment != null) {\n            fragTexts.add(fragment.toString());\n          }\n        } else {\n          if ((fragment != null) && (fragment.getScore() > 0)) {\n            fragTexts.add(fragment.toString());\n          }\n        }\n\n        if (fragTexts.size() >= numFragments && !preserveMulti) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    StorableField[] docFields = doc.getFields(fieldName);\n    List<String> listFields = new ArrayList<String>();\n    for (StorableField field : docFields) {\n      listFields.add(field.stringValue());\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    String[] docTexts = (String[]) listFields.toArray(new String[listFields.size()]);\n   \n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if (preserveMulti) {\n            if (bestTextFragments[k] != null) {\n              frags.add(bestTextFragments[k]);\n            }\n          } else {\n            if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n              frags.add(bestTextFragments[k]);\n            }\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n     if(!preserveMulti){\n        Collections.sort(frags, new Comparator<TextFragment>() {\n                public int compare(TextFragment arg0, TextFragment arg1) {\n                 return Math.round(arg1.getScore() - arg0.getScore());\n        }\n        });\n     }\n\n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if (preserveMulti) {\n          if (fragment != null) {\n            fragTexts.add(fragment.toString());\n          }\n        } else {\n          if ((fragment != null) && (fragment.getScore() > 0)) {\n            fragTexts.add(fragment.toString());\n          }\n        }\n\n        if (fragTexts.size() >= numFragments && !preserveMulti) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":["c220849f876de24a79f756f65b3eb045db59f63f","5df1793b9dbc0f17ba1d1dddb8a15748fdc3aaf3"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"62e52115b56781006682fd92c6938efaf174304d","date":1351014780,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    StorableField[] docFields = doc.getFields(fieldName);\n    List<String> listFields = new ArrayList<String>();\n    for (StorableField field : docFields) {\n      listFields.add(field.stringValue());\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    String[] docTexts = (String[]) listFields.toArray(new String[listFields.size()]);\n   \n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    TokenStream tvStream = TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    if (tvStream != null) {\n      tots = new TermOffsetsTokenStream(tvStream);\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if (preserveMulti) {\n            if (bestTextFragments[k] != null) {\n              frags.add(bestTextFragments[k]);\n            }\n          } else {\n            if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n              frags.add(bestTextFragments[k]);\n            }\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n     if(!preserveMulti){\n        Collections.sort(frags, new Comparator<TextFragment>() {\n                public int compare(TextFragment arg0, TextFragment arg1) {\n                 return Math.round(arg1.getScore() - arg0.getScore());\n        }\n        });\n     }\n\n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if (preserveMulti) {\n          if (fragment != null) {\n            fragTexts.add(fragment.toString());\n          }\n        } else {\n          if ((fragment != null) && (fragment.getScore() > 0)) {\n            fragTexts.add(fragment.toString());\n          }\n        }\n\n        if (fragTexts.size() >= numFragments && !preserveMulti) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    StorableField[] docFields = doc.getFields(fieldName);\n    List<String> listFields = new ArrayList<String>();\n    for (StorableField field : docFields) {\n      listFields.add(field.stringValue());\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    String[] docTexts = (String[]) listFields.toArray(new String[listFields.size()]);\n   \n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if (preserveMulti) {\n            if (bestTextFragments[k] != null) {\n              frags.add(bestTextFragments[k]);\n            }\n          } else {\n            if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n              frags.add(bestTextFragments[k]);\n            }\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n     if(!preserveMulti){\n        Collections.sort(frags, new Comparator<TextFragment>() {\n                public int compare(TextFragment arg0, TextFragment arg1) {\n                 return Math.round(arg1.getScore() - arg0.getScore());\n        }\n        });\n     }\n\n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if (preserveMulti) {\n          if (fragment != null) {\n            fragTexts.add(fragment.toString());\n          }\n        } else {\n          if ((fragment != null) && (fragment.getScore() > 0)) {\n            fragTexts.add(fragment.toString());\n          }\n        }\n\n        if (fragTexts.size() >= numFragments && !preserveMulti) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7530de27b87b961b51f01bd1299b7004d46e8823","date":1355236261,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    StorableField[] docFields = doc.getFields(fieldName);\n    List<String> listFields = new ArrayList<String>();\n    for (StorableField field : docFields) {\n      listFields.add(field.stringValue());\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    String[] docTexts = (String[]) listFields.toArray(new String[listFields.size()]);\n   \n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    TokenStream tvStream = TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    if (tvStream != null) {\n      tots = new TermOffsetsTokenStream(tvStream);\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if (preserveMulti) {\n            if (bestTextFragments[k] != null) {\n              frags.add(bestTextFragments[k]);\n            }\n          } else {\n            if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n              frags.add(bestTextFragments[k]);\n            }\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n     if(!preserveMulti){\n        Collections.sort(frags, new Comparator<TextFragment>() {\n                @Override\n                public int compare(TextFragment arg0, TextFragment arg1) {\n                 return Math.round(arg1.getScore() - arg0.getScore());\n        }\n        });\n     }\n\n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if (preserveMulti) {\n          if (fragment != null) {\n            fragTexts.add(fragment.toString());\n          }\n        } else {\n          if ((fragment != null) && (fragment.getScore() > 0)) {\n            fragTexts.add(fragment.toString());\n          }\n        }\n\n        if (fragTexts.size() >= numFragments && !preserveMulti) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    StorableField[] docFields = doc.getFields(fieldName);\n    List<String> listFields = new ArrayList<String>();\n    for (StorableField field : docFields) {\n      listFields.add(field.stringValue());\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    String[] docTexts = (String[]) listFields.toArray(new String[listFields.size()]);\n   \n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    TokenStream tvStream = TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    if (tvStream != null) {\n      tots = new TermOffsetsTokenStream(tvStream);\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if (preserveMulti) {\n            if (bestTextFragments[k] != null) {\n              frags.add(bestTextFragments[k]);\n            }\n          } else {\n            if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n              frags.add(bestTextFragments[k]);\n            }\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n     if(!preserveMulti){\n        Collections.sort(frags, new Comparator<TextFragment>() {\n                public int compare(TextFragment arg0, TextFragment arg1) {\n                 return Math.round(arg1.getScore() - arg0.getScore());\n        }\n        });\n     }\n\n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if (preserveMulti) {\n          if (fragment != null) {\n            fragTexts.add(fragment.toString());\n          }\n        } else {\n          if ((fragment != null) && (fragment.getScore() > 0)) {\n            fragTexts.add(fragment.toString());\n          }\n        }\n\n        if (fragTexts.size() >= numFragments && !preserveMulti) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    StorableField[] docFields = doc.getFields(fieldName);\n    List<String> listFields = new ArrayList<String>();\n    for (StorableField field : docFields) {\n      listFields.add(field.stringValue());\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    String[] docTexts = (String[]) listFields.toArray(new String[listFields.size()]);\n   \n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    TokenStream tvStream = TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    if (tvStream != null) {\n      tots = new TermOffsetsTokenStream(tvStream);\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if (preserveMulti) {\n            if (bestTextFragments[k] != null) {\n              frags.add(bestTextFragments[k]);\n            }\n          } else {\n            if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n              frags.add(bestTextFragments[k]);\n            }\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n     if(!preserveMulti){\n        Collections.sort(frags, new Comparator<TextFragment>() {\n                @Override\n                public int compare(TextFragment arg0, TextFragment arg1) {\n                 return Math.round(arg1.getScore() - arg0.getScore());\n        }\n        });\n     }\n\n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if (preserveMulti) {\n          if (fragment != null) {\n            fragTexts.add(fragment.toString());\n          }\n        } else {\n          if ((fragment != null) && (fragment.getScore() > 0)) {\n            fragTexts.add(fragment.toString());\n          }\n        }\n\n        if (fragTexts.size() >= numFragments && !preserveMulti) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    StorableField[] docFields = doc.getFields(fieldName);\n    List<String> listFields = new ArrayList<String>();\n    for (StorableField field : docFields) {\n      listFields.add(field.stringValue());\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    String[] docTexts = (String[]) listFields.toArray(new String[listFields.size()]);\n   \n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    TokenStream tvStream = TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    if (tvStream != null) {\n      tots = new TermOffsetsTokenStream(tvStream);\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if (preserveMulti) {\n            if (bestTextFragments[k] != null) {\n              frags.add(bestTextFragments[k]);\n            }\n          } else {\n            if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n              frags.add(bestTextFragments[k]);\n            }\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n     if(!preserveMulti){\n        Collections.sort(frags, new Comparator<TextFragment>() {\n                public int compare(TextFragment arg0, TextFragment arg1) {\n                 return Math.round(arg1.getScore() - arg0.getScore());\n        }\n        });\n     }\n\n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if (preserveMulti) {\n          if (fragment != null) {\n            fragTexts.add(fragment.toString());\n          }\n        } else {\n          if ((fragment != null) && (fragment.getScore() > 0)) {\n            fragTexts.add(fragment.toString());\n          }\n        }\n\n        if (fragTexts.size() >= numFragments && !preserveMulti) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17197618d443fdba7cdd03b52a61e09814931638","date":1364910298,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams();\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    List<StorableField> allFields = doc.getFields();\n    if (allFields != null && allFields.size() == 0) return; // No explicit contract that getFields returns != null,\n                                                            // although currently it can't.\n\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    TokenStream tvStream = TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    if (tvStream != null) {\n      tots = new TermOffsetsTokenStream(tvStream);\n    }\n    int mvToExamine = Integer.parseInt(req.getParams().get(HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n        Integer.toString(Integer.MAX_VALUE)));\n    int mvToMatch = Integer.parseInt(req.getParams().get(HighlightParams.MAX_MULTIVALUED_TO_MATCH,\n        Integer.toString(Integer.MAX_VALUE)));\n\n    for (StorableField thisField : allFields) {\n      if (mvToExamine <= 0 || mvToMatch <= 0) break;\n\n      if (! thisField.name().equals(fieldName)) continue; // Is there a better way to do this?\n\n      --mvToExamine;\n      String thisText = thisField.stringValue();\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( thisText.length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(thisText.length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if (preserveMulti) {\n            if (bestTextFragments[k] != null) {\n              frags.add(bestTextFragments[k]);\n              --mvToMatch;\n            }\n          } else {\n            if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n              frags.add(bestTextFragments[k]);\n              --mvToMatch;\n            }\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n     if(!preserveMulti){\n        Collections.sort(frags, new Comparator<TextFragment>() {\n                @Override\n                public int compare(TextFragment arg0, TextFragment arg1) {\n                 return Math.round(arg1.getScore() - arg0.getScore());\n        }\n        });\n     }\n\n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if (preserveMulti) {\n          if (fragment != null) {\n            fragTexts.add(fragment.toString());\n          }\n        } else {\n          if ((fragment != null) && (fragment.getScore() > 0)) {\n            fragTexts.add(fragment.toString());\n          }\n        }\n\n        if (fragTexts.size() >= numFragments && !preserveMulti) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    StorableField[] docFields = doc.getFields(fieldName);\n    List<String> listFields = new ArrayList<String>();\n    for (StorableField field : docFields) {\n      listFields.add(field.stringValue());\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    String[] docTexts = (String[]) listFields.toArray(new String[listFields.size()]);\n   \n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    TokenStream tvStream = TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    if (tvStream != null) {\n      tots = new TermOffsetsTokenStream(tvStream);\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if (preserveMulti) {\n            if (bestTextFragments[k] != null) {\n              frags.add(bestTextFragments[k]);\n            }\n          } else {\n            if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n              frags.add(bestTextFragments[k]);\n            }\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n     if(!preserveMulti){\n        Collections.sort(frags, new Comparator<TextFragment>() {\n                @Override\n                public int compare(TextFragment arg0, TextFragment arg1) {\n                 return Math.round(arg1.getScore() - arg0.getScore());\n        }\n        });\n     }\n\n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if (preserveMulti) {\n          if (fragment != null) {\n            fragTexts.add(fragment.toString());\n          }\n        } else {\n          if ((fragment != null) && (fragment.getScore() > 0)) {\n            fragTexts.add(fragment.toString());\n          }\n        }\n\n        if (fragTexts.size() >= numFragments && !preserveMulti) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams();\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    List<StorableField> allFields = doc.getFields();\n    if (allFields != null && allFields.size() == 0) return; // No explicit contract that getFields returns != null,\n                                                            // although currently it can't.\n\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    TokenStream tvStream = TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    if (tvStream != null) {\n      tots = new TermOffsetsTokenStream(tvStream);\n    }\n    int mvToExamine = Integer.parseInt(req.getParams().get(HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n        Integer.toString(Integer.MAX_VALUE)));\n    int mvToMatch = Integer.parseInt(req.getParams().get(HighlightParams.MAX_MULTIVALUED_TO_MATCH,\n        Integer.toString(Integer.MAX_VALUE)));\n\n    for (StorableField thisField : allFields) {\n      if (mvToExamine <= 0 || mvToMatch <= 0) break;\n\n      if (! thisField.name().equals(fieldName)) continue; // Is there a better way to do this?\n\n      --mvToExamine;\n      String thisText = thisField.stringValue();\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( thisText.length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(thisText.length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if (preserveMulti) {\n            if (bestTextFragments[k] != null) {\n              frags.add(bestTextFragments[k]);\n              --mvToMatch;\n            }\n          } else {\n            if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n              frags.add(bestTextFragments[k]);\n              --mvToMatch;\n            }\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n     if(!preserveMulti){\n        Collections.sort(frags, new Comparator<TextFragment>() {\n                @Override\n                public int compare(TextFragment arg0, TextFragment arg1) {\n                 return Math.round(arg1.getScore() - arg0.getScore());\n        }\n        });\n     }\n\n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<>();\n      for (TextFragment fragment: frags) {\n        if (preserveMulti) {\n          if (fragment != null) {\n            fragTexts.add(fragment.toString());\n          }\n        } else {\n          if ((fragment != null) && (fragment.getScore() > 0)) {\n            fragTexts.add(fragment.toString());\n          }\n        }\n\n        if (fragTexts.size() >= numFragments && !preserveMulti) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams();\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    List<StorableField> allFields = doc.getFields();\n    if (allFields != null && allFields.size() == 0) return; // No explicit contract that getFields returns != null,\n                                                            // although currently it can't.\n\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    TokenStream tvStream = TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    if (tvStream != null) {\n      tots = new TermOffsetsTokenStream(tvStream);\n    }\n    int mvToExamine = Integer.parseInt(req.getParams().get(HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n        Integer.toString(Integer.MAX_VALUE)));\n    int mvToMatch = Integer.parseInt(req.getParams().get(HighlightParams.MAX_MULTIVALUED_TO_MATCH,\n        Integer.toString(Integer.MAX_VALUE)));\n\n    for (StorableField thisField : allFields) {\n      if (mvToExamine <= 0 || mvToMatch <= 0) break;\n\n      if (! thisField.name().equals(fieldName)) continue; // Is there a better way to do this?\n\n      --mvToExamine;\n      String thisText = thisField.stringValue();\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( thisText.length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(thisText.length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if (preserveMulti) {\n            if (bestTextFragments[k] != null) {\n              frags.add(bestTextFragments[k]);\n              --mvToMatch;\n            }\n          } else {\n            if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n              frags.add(bestTextFragments[k]);\n              --mvToMatch;\n            }\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n     if(!preserveMulti){\n        Collections.sort(frags, new Comparator<TextFragment>() {\n                @Override\n                public int compare(TextFragment arg0, TextFragment arg1) {\n                 return Math.round(arg1.getScore() - arg0.getScore());\n        }\n        });\n     }\n\n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if (preserveMulti) {\n          if (fragment != null) {\n            fragTexts.add(fragment.toString());\n          }\n        } else {\n          if ((fragment != null) && (fragment.getScore() > 0)) {\n            fragTexts.add(fragment.toString());\n          }\n        }\n\n        if (fragTexts.size() >= numFragments && !preserveMulti) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cad4d876350c7790594c78dc3c3fb1718bf06dc7","date":1417321437,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && schemaField.getType() instanceof org.apache.solr.schema.TrieField) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams();\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    List<StorableField> allFields = doc.getFields();\n    if (allFields == null || allFields.isEmpty()) return; // No explicit contract that getFields returns != null,\n                                                            // although currently it can't.\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization (multi-valued)\n    TokenStream tvStream = TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    if (tvStream != null && schemaField.multiValued() && isActuallyMultiValued(allFields, fieldName)) {\n      tots = new TermOffsetsTokenStream(tvStream);\n    }\n    int mvToExamine = Integer.parseInt(req.getParams().get(HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n        Integer.toString(Integer.MAX_VALUE)));\n    int mvToMatch = Integer.parseInt(req.getParams().get(HighlightParams.MAX_MULTIVALUED_TO_MATCH,\n        Integer.toString(Integer.MAX_VALUE)));\n\n    for (StorableField thisField : allFields) {\n      if (mvToExamine <= 0 || mvToMatch <= 0) break;\n\n      if (! thisField.name().equals(fieldName)) continue; // Is there a better way to do this?\n\n      --mvToExamine;\n      String thisText = thisField.stringValue();\n      TokenStream tstream;\n      if (tots != null) {\n        // if we're using TermOffsets optimization (multi-valued field with term vectors), then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze < 0) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n         \n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter)tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(thisText.length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (preserveMulti) {\n            if (bestTextFragment != null) {\n              frags.add(bestTextFragment);\n              --mvToMatch;\n            }\n          } else {\n            if ((bestTextFragment != null) && (bestTextFragment.getScore() > 0)) {\n              frags.add(bestTextFragment);\n              --mvToMatch;\n            }\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // sort such that the fragments with the highest score come first\n    if (!preserveMulti) {\n      Collections.sort(frags, new Comparator<TextFragment>() {\n        @Override\n        public int compare(TextFragment arg0, TextFragment arg1) {\n          return Math.round(arg1.getScore() - arg0.getScore());\n        }\n      });\n    }\n\n    // convert fragments back into text\n    // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<>();\n      for (TextFragment fragment: frags) {\n        if (preserveMulti) {\n          if (fragment != null) {\n            fragTexts.add(fragment.toString());\n          }\n        } else {\n          if ((fragment != null) && (fragment.getScore() > 0)) {\n            fragTexts.add(fragment.toString());\n          }\n        }\n\n        if (fragTexts.size() >= numFragments && !preserveMulti) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams();\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    List<StorableField> allFields = doc.getFields();\n    if (allFields != null && allFields.size() == 0) return; // No explicit contract that getFields returns != null,\n                                                            // although currently it can't.\n\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    TokenStream tvStream = TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    if (tvStream != null) {\n      tots = new TermOffsetsTokenStream(tvStream);\n    }\n    int mvToExamine = Integer.parseInt(req.getParams().get(HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n        Integer.toString(Integer.MAX_VALUE)));\n    int mvToMatch = Integer.parseInt(req.getParams().get(HighlightParams.MAX_MULTIVALUED_TO_MATCH,\n        Integer.toString(Integer.MAX_VALUE)));\n\n    for (StorableField thisField : allFields) {\n      if (mvToExamine <= 0 || mvToMatch <= 0) break;\n\n      if (! thisField.name().equals(fieldName)) continue; // Is there a better way to do this?\n\n      --mvToExamine;\n      String thisText = thisField.stringValue();\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( thisText.length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(thisText.length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if (preserveMulti) {\n            if (bestTextFragments[k] != null) {\n              frags.add(bestTextFragments[k]);\n              --mvToMatch;\n            }\n          } else {\n            if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n              frags.add(bestTextFragments[k]);\n              --mvToMatch;\n            }\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n     if(!preserveMulti){\n        Collections.sort(frags, new Comparator<TextFragment>() {\n                @Override\n                public int compare(TextFragment arg0, TextFragment arg1) {\n                 return Math.round(arg1.getScore() - arg0.getScore());\n        }\n        });\n     }\n\n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<>();\n      for (TextFragment fragment: frags) {\n        if (preserveMulti) {\n          if (fragment != null) {\n            fragTexts.add(fragment.toString());\n          }\n        } else {\n          if ((fragment != null) && (fragment.getScore() > 0)) {\n            fragTexts.add(fragment.toString());\n          }\n        }\n\n        if (fragTexts.size() >= numFragments && !preserveMulti) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bd19a665923f0acf59d3f056adfef60dbe7c1047","date":1419308867,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && schemaField.getType() instanceof org.apache.solr.schema.TrieField) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams();\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    List<StorableField> allFields = doc.getFields();\n    if (allFields == null || allFields.isEmpty()) return; // No explicit contract that getFields returns != null,\n                                                            // although currently it can't.\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    TokenStream tvStream = TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && schemaField.multiValued() && isActuallyMultiValued(allFields, fieldName)) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    int mvToExamine = Integer.parseInt(req.getParams().get(HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n        Integer.toString(Integer.MAX_VALUE)));\n    int mvToMatch = Integer.parseInt(req.getParams().get(HighlightParams.MAX_MULTIVALUED_TO_MATCH,\n        Integer.toString(Integer.MAX_VALUE)));\n\n    for (StorableField thisField : allFields) {\n      if (mvToExamine <= 0 || mvToMatch <= 0) break;\n\n      if (! thisField.name().equals(fieldName)) continue; // Is there a better way to do this?\n\n      --mvToExamine;\n      String thisText = thisField.stringValue();\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze < 0) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n         \n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter)tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(thisText.length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (preserveMulti) {\n            if (bestTextFragment != null) {\n              frags.add(bestTextFragment);\n              --mvToMatch;\n            }\n          } else {\n            if ((bestTextFragment != null) && (bestTextFragment.getScore() > 0)) {\n              frags.add(bestTextFragment);\n              --mvToMatch;\n            }\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // sort such that the fragments with the highest score come first\n    if (!preserveMulti) {\n      Collections.sort(frags, new Comparator<TextFragment>() {\n        @Override\n        public int compare(TextFragment arg0, TextFragment arg1) {\n          return Math.round(arg1.getScore() - arg0.getScore());\n        }\n      });\n    }\n\n    // convert fragments back into text\n    // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<>();\n      for (TextFragment fragment: frags) {\n        if (preserveMulti) {\n          if (fragment != null) {\n            fragTexts.add(fragment.toString());\n          }\n        } else {\n          if ((fragment != null) && (fragment.getScore() > 0)) {\n            fragTexts.add(fragment.toString());\n          }\n        }\n\n        if (fragTexts.size() >= numFragments && !preserveMulti) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && schemaField.getType() instanceof org.apache.solr.schema.TrieField) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams();\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    List<StorableField> allFields = doc.getFields();\n    if (allFields == null || allFields.isEmpty()) return; // No explicit contract that getFields returns != null,\n                                                            // although currently it can't.\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization (multi-valued)\n    TokenStream tvStream = TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    if (tvStream != null && schemaField.multiValued() && isActuallyMultiValued(allFields, fieldName)) {\n      tots = new TermOffsetsTokenStream(tvStream);\n    }\n    int mvToExamine = Integer.parseInt(req.getParams().get(HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n        Integer.toString(Integer.MAX_VALUE)));\n    int mvToMatch = Integer.parseInt(req.getParams().get(HighlightParams.MAX_MULTIVALUED_TO_MATCH,\n        Integer.toString(Integer.MAX_VALUE)));\n\n    for (StorableField thisField : allFields) {\n      if (mvToExamine <= 0 || mvToMatch <= 0) break;\n\n      if (! thisField.name().equals(fieldName)) continue; // Is there a better way to do this?\n\n      --mvToExamine;\n      String thisText = thisField.stringValue();\n      TokenStream tstream;\n      if (tots != null) {\n        // if we're using TermOffsets optimization (multi-valued field with term vectors), then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze < 0) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n         \n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter)tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(thisText.length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (preserveMulti) {\n            if (bestTextFragment != null) {\n              frags.add(bestTextFragment);\n              --mvToMatch;\n            }\n          } else {\n            if ((bestTextFragment != null) && (bestTextFragment.getScore() > 0)) {\n              frags.add(bestTextFragment);\n              --mvToMatch;\n            }\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // sort such that the fragments with the highest score come first\n    if (!preserveMulti) {\n      Collections.sort(frags, new Comparator<TextFragment>() {\n        @Override\n        public int compare(TextFragment arg0, TextFragment arg1) {\n          return Math.round(arg1.getScore() - arg0.getScore());\n        }\n      });\n    }\n\n    // convert fragments back into text\n    // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<>();\n      for (TextFragment fragment: frags) {\n        if (preserveMulti) {\n          if (fragment != null) {\n            fragTexts.add(fragment.toString());\n          }\n        } else {\n          if ((fragment != null) && (fragment.getScore() > 0)) {\n            fragTexts.add(fragment.toString());\n          }\n        }\n\n        if (fragTexts.size() >= numFragments && !preserveMulti) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4a7ab38c291565c0fabdbd2946cd2f614dea29ff","date":1421069415,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && schemaField.getType() instanceof org.apache.solr.schema.TrieField) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams();\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    List<StorableField> allFields = doc.getFields();\n    if (allFields == null || allFields.isEmpty()) return; // No explicit contract that getFields returns != null,\n                                                            // although currently it can't.\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    TokenStream tvStream = TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && schemaField.multiValued() && isActuallyMultiValued(allFields, fieldName)) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    int mvToExamine = Integer.parseInt(req.getParams().get(HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n        Integer.toString(Integer.MAX_VALUE)));\n    int mvToMatch = Integer.parseInt(req.getParams().get(HighlightParams.MAX_MULTIVALUED_TO_MATCH,\n        Integer.toString(Integer.MAX_VALUE)));\n\n    for (StorableField thisField : allFields) {\n      if (mvToExamine <= 0 || mvToMatch <= 0) break;\n\n      if (! thisField.name().equals(fieldName)) continue; // Is there a better way to do this?\n\n      --mvToExamine;\n      String thisText = thisField.stringValue();\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze < 0) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n         \n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter)tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(thisText.length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (preserveMulti) {\n            if (bestTextFragment != null) {\n              frags.add(bestTextFragment);\n              --mvToMatch;\n            }\n          } else {\n            if ((bestTextFragment != null) && (bestTextFragment.getScore() > 0)) {\n              frags.add(bestTextFragment);\n              --mvToMatch;\n            }\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // sort such that the fragments with the highest score come first\n    if (!preserveMulti) {\n      Collections.sort(frags, new Comparator<TextFragment>() {\n        @Override\n        public int compare(TextFragment arg0, TextFragment arg1) {\n          return Math.round(arg1.getScore() - arg0.getScore());\n        }\n      });\n    }\n\n    // convert fragments back into text\n    // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<>();\n      for (TextFragment fragment: frags) {\n        if (preserveMulti) {\n          if (fragment != null) {\n            fragTexts.add(fragment.toString());\n          }\n        } else {\n          if ((fragment != null) && (fragment.getScore() > 0)) {\n            fragTexts.add(fragment.toString());\n          }\n        }\n\n        if (fragTexts.size() >= numFragments && !preserveMulti) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summaries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && schemaField.getType() instanceof org.apache.solr.schema.TrieField) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams();\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    List<StorableField> allFields = doc.getFields();\n    if (allFields == null || allFields.isEmpty()) return; // No explicit contract that getFields returns != null,\n                                                            // although currently it can't.\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    TokenStream tvStream = TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && schemaField.multiValued() && isActuallyMultiValued(allFields, fieldName)) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    int mvToExamine = Integer.parseInt(req.getParams().get(HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n        Integer.toString(Integer.MAX_VALUE)));\n    int mvToMatch = Integer.parseInt(req.getParams().get(HighlightParams.MAX_MULTIVALUED_TO_MATCH,\n        Integer.toString(Integer.MAX_VALUE)));\n\n    for (StorableField thisField : allFields) {\n      if (mvToExamine <= 0 || mvToMatch <= 0) break;\n\n      if (! thisField.name().equals(fieldName)) continue; // Is there a better way to do this?\n\n      --mvToExamine;\n      String thisText = thisField.stringValue();\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze < 0) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n         \n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter)tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(thisText.length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (preserveMulti) {\n            if (bestTextFragment != null) {\n              frags.add(bestTextFragment);\n              --mvToMatch;\n            }\n          } else {\n            if ((bestTextFragment != null) && (bestTextFragment.getScore() > 0)) {\n              frags.add(bestTextFragment);\n              --mvToMatch;\n            }\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // sort such that the fragments with the highest score come first\n    if (!preserveMulti) {\n      Collections.sort(frags, new Comparator<TextFragment>() {\n        @Override\n        public int compare(TextFragment arg0, TextFragment arg1) {\n          return Math.round(arg1.getScore() - arg0.getScore());\n        }\n      });\n    }\n\n    // convert fragments back into text\n    // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<>();\n      for (TextFragment fragment: frags) {\n        if (preserveMulti) {\n          if (fragment != null) {\n            fragTexts.add(fragment.toString());\n          }\n        } else {\n          if ((fragment != null) && (fragment.getScore() > 0)) {\n            fragTexts.add(fragment.toString());\n          }\n        }\n\n        if (fragTexts.size() >= numFragments && !preserveMulti) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"12aef50f649e6f5f9689970748f96e3f1ca6104e","date":1428932135,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  protected void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    final SolrParams params = req.getParams();\n\n    // TODO: highlighting numeric fields is broken (Lucene) - so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && schemaField.getType() instanceof org.apache.solr.schema.TrieField) return;\n    // END: Hack\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            schemaField.multiValued() ? Integer.MAX_VALUE : 1);\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(req, doc, fieldName, mvToExamine, maxCharsToAnalyze);\n    if (fieldValues.isEmpty()) {\n      return;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    final TokenStream tvStream =\n        TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter)tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, new Comparator<TextFragment>() {//TODO make TextFragment Comparable\n          @Override\n          public int compare(TextFragment arg0, TextFragment arg1) {\n            return Math.round(arg1.getScore() - arg0.getScore());\n          }\n        });\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      docSummaries.add(fieldName, getResponseForFragments(frags, req));\n    } else {\n      // no summaries made, copy text from alternate field\n      alternateField(docSummaries, params, doc, fieldName);\n    }\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && schemaField.getType() instanceof org.apache.solr.schema.TrieField) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams();\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    List<StorableField> allFields = doc.getFields();\n    if (allFields == null || allFields.isEmpty()) return; // No explicit contract that getFields returns != null,\n                                                            // although currently it can't.\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    TokenStream tvStream = TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && schemaField.multiValued() && isActuallyMultiValued(allFields, fieldName)) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    int mvToExamine = Integer.parseInt(req.getParams().get(HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n        Integer.toString(Integer.MAX_VALUE)));\n    int mvToMatch = Integer.parseInt(req.getParams().get(HighlightParams.MAX_MULTIVALUED_TO_MATCH,\n        Integer.toString(Integer.MAX_VALUE)));\n\n    for (StorableField thisField : allFields) {\n      if (mvToExamine <= 0 || mvToMatch <= 0) break;\n\n      if (! thisField.name().equals(fieldName)) continue; // Is there a better way to do this?\n\n      --mvToExamine;\n      String thisText = thisField.stringValue();\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze < 0) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n         \n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter)tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(thisText.length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (preserveMulti) {\n            if (bestTextFragment != null) {\n              frags.add(bestTextFragment);\n              --mvToMatch;\n            }\n          } else {\n            if ((bestTextFragment != null) && (bestTextFragment.getScore() > 0)) {\n              frags.add(bestTextFragment);\n              --mvToMatch;\n            }\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // sort such that the fragments with the highest score come first\n    if (!preserveMulti) {\n      Collections.sort(frags, new Comparator<TextFragment>() {\n        @Override\n        public int compare(TextFragment arg0, TextFragment arg1) {\n          return Math.round(arg1.getScore() - arg0.getScore());\n        }\n      });\n    }\n\n    // convert fragments back into text\n    // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<>();\n      for (TextFragment fragment: frags) {\n        if (preserveMulti) {\n          if (fragment != null) {\n            fragTexts.add(fragment.toString());\n          }\n        } else {\n          if ((fragment != null) && (fragment.getScore() > 0)) {\n            fragTexts.add(fragment.toString());\n          }\n        }\n\n        if (fragTexts.size() >= numFragments && !preserveMulti) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summaries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c15fc5a614054769aee7b33d93ee5913e6397ac8","date":1428932937,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  protected void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    final SolrParams params = req.getParams();\n\n    // TODO: highlighting numeric fields is broken (Lucene) - so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && schemaField.getType() instanceof org.apache.solr.schema.TrieField) return;\n    // END: Hack\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            schemaField.multiValued() ? Integer.MAX_VALUE : 1);\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(req, doc, fieldName, mvToExamine, maxCharsToAnalyze);\n    if (fieldValues.isEmpty()) {\n      return;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    final TokenStream tvStream =\n        TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter)tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, new Comparator<TextFragment>() {//TODO make TextFragment Comparable\n          @Override\n          public int compare(TextFragment arg0, TextFragment arg1) {\n            return Float.compare(arg1.getScore(), arg0.getScore());\n          }\n        });\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      docSummaries.add(fieldName, getResponseForFragments(frags, req));\n    } else {\n      // no summaries made, copy text from alternate field\n      alternateField(docSummaries, params, doc, fieldName);\n    }\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  protected void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    final SolrParams params = req.getParams();\n\n    // TODO: highlighting numeric fields is broken (Lucene) - so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && schemaField.getType() instanceof org.apache.solr.schema.TrieField) return;\n    // END: Hack\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            schemaField.multiValued() ? Integer.MAX_VALUE : 1);\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(req, doc, fieldName, mvToExamine, maxCharsToAnalyze);\n    if (fieldValues.isEmpty()) {\n      return;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    final TokenStream tvStream =\n        TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter)tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, new Comparator<TextFragment>() {//TODO make TextFragment Comparable\n          @Override\n          public int compare(TextFragment arg0, TextFragment arg1) {\n            return Math.round(arg1.getScore() - arg0.getScore());\n          }\n        });\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      docSummaries.add(fieldName, getResponseForFragments(frags, req));\n    } else {\n      // no summaries made, copy text from alternate field\n      alternateField(docSummaries, params, doc, fieldName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"943562acd5eb31fa2fb7384927091158a85ce9fe","date":1428934087,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  protected void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n                                              int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    final SolrParams params = req.getParams();\n\n    // TODO: highlighting numeric fields is broken (Lucene) - so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && schemaField.getType() instanceof org.apache.solr.schema.TrieField) return;\n    // END: Hack\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            schemaField.multiValued() ? Integer.MAX_VALUE : 1);\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(req, doc, fieldName, mvToExamine, maxCharsToAnalyze);\n    if (fieldValues.isEmpty()) {\n      return;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    final TokenStream tvStream =\n        TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter)tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, new Comparator<TextFragment>() {//TODO make TextFragment Comparable\n          @Override\n          public int compare(TextFragment arg0, TextFragment arg1) {\n            return Float.compare(arg1.getScore(), arg0.getScore());\n          }\n        });\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      docSummaries.add(fieldName, getResponseForFragments(frags, req));\n    } else {\n      // no summaries made, copy text from alternate field\n      alternateField(docSummaries, params, doc, fieldName);\n    }\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  protected void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    final SolrParams params = req.getParams();\n\n    // TODO: highlighting numeric fields is broken (Lucene) - so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && schemaField.getType() instanceof org.apache.solr.schema.TrieField) return;\n    // END: Hack\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            schemaField.multiValued() ? Integer.MAX_VALUE : 1);\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(req, doc, fieldName, mvToExamine, maxCharsToAnalyze);\n    if (fieldValues.isEmpty()) {\n      return;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    final TokenStream tvStream =\n        TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter)tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, new Comparator<TextFragment>() {//TODO make TextFragment Comparable\n          @Override\n          public int compare(TextFragment arg0, TextFragment arg1) {\n            return Float.compare(arg1.getScore(), arg0.getScore());\n          }\n        });\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      docSummaries.add(fieldName, getResponseForFragments(frags, req));\n    } else {\n      // no summaries made, copy text from alternate field\n      alternateField(docSummaries, params, doc, fieldName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d1f66bb85f641eb4cf7699495d10e0ab50eae3ad","date":1428936836,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  protected void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n                                              int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    final SolrParams params = req.getParams();\n\n    // TODO: highlighting numeric fields is broken (Lucene) - so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && schemaField.getType() instanceof org.apache.solr.schema.TrieField) return;\n    // END: Hack\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            schemaField.multiValued() ? Integer.MAX_VALUE : 1);\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(req, doc, fieldName, mvToExamine, maxCharsToAnalyze);\n    if (fieldValues.isEmpty()) {\n      return;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    final TokenStream tvStream =\n        TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter)tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, new Comparator<TextFragment>() {//TODO make TextFragment Comparable\n          @Override\n          public int compare(TextFragment arg0, TextFragment arg1) {\n            return Float.compare(arg1.getScore(), arg0.getScore());\n          }\n        });\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      docSummaries.add(fieldName, getResponseForFragments(frags, req));\n    } else {\n      // no summaries made, copy text from alternate field\n      alternateField(docSummaries, params, doc, fieldName);\n    }\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  protected void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n                                              int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    final SolrParams params = req.getParams();\n\n    // TODO: highlighting numeric fields is broken (Lucene) - so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && schemaField.getType() instanceof org.apache.solr.schema.TrieField) return;\n    // END: Hack\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            schemaField.multiValued() ? Integer.MAX_VALUE : 1);\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(req, doc, fieldName, mvToExamine, maxCharsToAnalyze);\n    if (fieldValues.isEmpty()) {\n      return;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    final TokenStream tvStream =\n        TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter)tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, new Comparator<TextFragment>() {//TODO make TextFragment Comparable\n          @Override\n          public int compare(TextFragment arg0, TextFragment arg1) {\n            return Float.compare(arg1.getScore(), arg0.getScore());\n          }\n        });\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      docSummaries.add(fieldName, getResponseForFragments(frags, req));\n    } else {\n      // no summaries made, copy text from alternate field\n      alternateField(docSummaries, params, doc, fieldName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d25092e16fdff6540a9798c0f0f5af0d6d58ec1c","date":1428959500,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  protected void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n                                              int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    final SolrParams params = req.getParams();\n\n    // TODO: highlighting numeric fields is broken (Lucene) - so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && schemaField.getType() instanceof org.apache.solr.schema.TrieField) return;\n    // END: Hack\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField != null && schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(req, doc, fieldName, mvToExamine, maxCharsToAnalyze);\n    if (fieldValues.isEmpty()) {\n      return;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    final TokenStream tvStream =\n        TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter)tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, new Comparator<TextFragment>() {//TODO make TextFragment Comparable\n          @Override\n          public int compare(TextFragment arg0, TextFragment arg1) {\n            return Float.compare(arg1.getScore(), arg0.getScore());\n          }\n        });\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      docSummaries.add(fieldName, getResponseForFragments(frags, req));\n    } else {\n      // no summaries made, copy text from alternate field\n      alternateField(docSummaries, params, doc, fieldName);\n    }\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  protected void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n                                              int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    final SolrParams params = req.getParams();\n\n    // TODO: highlighting numeric fields is broken (Lucene) - so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && schemaField.getType() instanceof org.apache.solr.schema.TrieField) return;\n    // END: Hack\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            schemaField.multiValued() ? Integer.MAX_VALUE : 1);\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(req, doc, fieldName, mvToExamine, maxCharsToAnalyze);\n    if (fieldValues.isEmpty()) {\n      return;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    final TokenStream tvStream =\n        TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter)tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, new Comparator<TextFragment>() {//TODO make TextFragment Comparable\n          @Override\n          public int compare(TextFragment arg0, TextFragment arg1) {\n            return Float.compare(arg1.getScore(), arg0.getScore());\n          }\n        });\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      docSummaries.add(fieldName, getResponseForFragments(frags, req));\n    } else {\n      // no summaries made, copy text from alternate field\n      alternateField(docSummaries, params, doc, fieldName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5d62e4938659e263e96ae8188e11aea8a940aea5","date":1430230314,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  protected void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n                                              int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    final SolrParams params = req.getParams();\n\n    // TODO: highlighting numeric fields is broken (Lucene) - so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && schemaField.getType() instanceof org.apache.solr.schema.TrieField) return;\n    // END: Hack\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField != null && schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(req, doc, fieldName, mvToExamine, maxCharsToAnalyze);\n    if (fieldValues.isEmpty()) {\n      return;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    final Fields tvFields = searcher.getIndexReader().getTermVectors(docId); // TODO add as param; see SOLR-5855\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter)tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, new Comparator<TextFragment>() {//TODO make TextFragment Comparable\n          @Override\n          public int compare(TextFragment arg0, TextFragment arg1) {\n            return Float.compare(arg1.getScore(), arg0.getScore());\n          }\n        });\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      docSummaries.add(fieldName, getResponseForFragments(frags, req));\n    } else {\n      // no summaries made, copy text from alternate field\n      alternateField(docSummaries, params, doc, fieldName);\n    }\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  protected void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n                                              int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    final SolrParams params = req.getParams();\n\n    // TODO: highlighting numeric fields is broken (Lucene) - so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && schemaField.getType() instanceof org.apache.solr.schema.TrieField) return;\n    // END: Hack\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField != null && schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(req, doc, fieldName, mvToExamine, maxCharsToAnalyze);\n    if (fieldValues.isEmpty()) {\n      return;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    final TokenStream tvStream =\n        TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter)tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, new Comparator<TextFragment>() {//TODO make TextFragment Comparable\n          @Override\n          public int compare(TextFragment arg0, TextFragment arg1) {\n            return Float.compare(arg1.getScore(), arg0.getScore());\n          }\n        });\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      docSummaries.add(fieldName, getResponseForFragments(frags, req));\n    } else {\n      // no summaries made, copy text from alternate field\n      alternateField(docSummaries, params, doc, fieldName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1ba83a7997a13459d756c436cc76ee2570d2128f","date":1432215022,"type":5,"author":"David Wayne Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(StoredDocument,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","sourceNew":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(StoredDocument doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schemaField, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, new Comparator<TextFragment>() {//TODO make TextFragment Comparable\n          @Override\n          public int compare(TextFragment arg0, TextFragment arg1) {\n            return Float.compare(arg1.getScore(), arg0.getScore());\n          }\n        });\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  protected void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n                                              int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    final SolrParams params = req.getParams();\n\n    // TODO: highlighting numeric fields is broken (Lucene) - so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && schemaField.getType() instanceof org.apache.solr.schema.TrieField) return;\n    // END: Hack\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField != null && schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(req, doc, fieldName, mvToExamine, maxCharsToAnalyze);\n    if (fieldValues.isEmpty()) {\n      return;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    final Fields tvFields = searcher.getIndexReader().getTermVectors(docId); // TODO add as param; see SOLR-5855\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter)tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, new Comparator<TextFragment>() {//TODO make TextFragment Comparable\n          @Override\n          public int compare(TextFragment arg0, TextFragment arg1) {\n            return Float.compare(arg1.getScore(), arg0.getScore());\n          }\n        });\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      docSummaries.add(fieldName, getResponseForFragments(frags, req));\n    } else {\n      // no summaries made, copy text from alternate field\n      alternateField(docSummaries, params, doc, fieldName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["815287248ca7a77db68038baad5698c5767f36a7","7530de27b87b961b51f01bd1299b7004d46e8823"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["17197618d443fdba7cdd03b52a61e09814931638"],"d1f66bb85f641eb4cf7699495d10e0ab50eae3ad":["943562acd5eb31fa2fb7384927091158a85ce9fe"],"cad4d876350c7790594c78dc3c3fb1718bf06dc7":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"12aef50f649e6f5f9689970748f96e3f1ca6104e":["4a7ab38c291565c0fabdbd2946cd2f614dea29ff"],"5d62e4938659e263e96ae8188e11aea8a940aea5":["d25092e16fdff6540a9798c0f0f5af0d6d58ec1c"],"bd19a665923f0acf59d3f056adfef60dbe7c1047":["cad4d876350c7790594c78dc3c3fb1718bf06dc7"],"d25092e16fdff6540a9798c0f0f5af0d6d58ec1c":["d1f66bb85f641eb4cf7699495d10e0ab50eae3ad"],"c15fc5a614054769aee7b33d93ee5913e6397ac8":["12aef50f649e6f5f9689970748f96e3f1ca6104e"],"815287248ca7a77db68038baad5698c5767f36a7":["4cf27653a0ff79ebec6be4f231cadb121fafae33"],"4a7ab38c291565c0fabdbd2946cd2f614dea29ff":["bd19a665923f0acf59d3f056adfef60dbe7c1047"],"943562acd5eb31fa2fb7384927091158a85ce9fe":["c15fc5a614054769aee7b33d93ee5913e6397ac8"],"1d028314cced5858683a1bb4741423d0f934257b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","a7e4907084808af8fdb14b9809e6dceaccf6867b"],"4cf27653a0ff79ebec6be4f231cadb121fafae33":["66902e88b46bfaaa034e1a881715d5cde7c01146"],"66902e88b46bfaaa034e1a881715d5cde7c01146":["1d028314cced5858683a1bb4741423d0f934257b"],"62e52115b56781006682fd92c6938efaf174304d":["4cf27653a0ff79ebec6be4f231cadb121fafae33","815287248ca7a77db68038baad5698c5767f36a7"],"1ba83a7997a13459d756c436cc76ee2570d2128f":["5d62e4938659e263e96ae8188e11aea8a940aea5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17197618d443fdba7cdd03b52a61e09814931638":["7530de27b87b961b51f01bd1299b7004d46e8823"],"7530de27b87b961b51f01bd1299b7004d46e8823":["815287248ca7a77db68038baad5698c5767f36a7"],"a7e4907084808af8fdb14b9809e6dceaccf6867b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["1ba83a7997a13459d756c436cc76ee2570d2128f"]},"commit2Childs":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["cad4d876350c7790594c78dc3c3fb1718bf06dc7"],"d1f66bb85f641eb4cf7699495d10e0ab50eae3ad":["d25092e16fdff6540a9798c0f0f5af0d6d58ec1c"],"cad4d876350c7790594c78dc3c3fb1718bf06dc7":["bd19a665923f0acf59d3f056adfef60dbe7c1047"],"12aef50f649e6f5f9689970748f96e3f1ca6104e":["c15fc5a614054769aee7b33d93ee5913e6397ac8"],"5d62e4938659e263e96ae8188e11aea8a940aea5":["1ba83a7997a13459d756c436cc76ee2570d2128f"],"bd19a665923f0acf59d3f056adfef60dbe7c1047":["4a7ab38c291565c0fabdbd2946cd2f614dea29ff"],"d25092e16fdff6540a9798c0f0f5af0d6d58ec1c":["5d62e4938659e263e96ae8188e11aea8a940aea5"],"c15fc5a614054769aee7b33d93ee5913e6397ac8":["943562acd5eb31fa2fb7384927091158a85ce9fe"],"815287248ca7a77db68038baad5698c5767f36a7":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","62e52115b56781006682fd92c6938efaf174304d","7530de27b87b961b51f01bd1299b7004d46e8823"],"4a7ab38c291565c0fabdbd2946cd2f614dea29ff":["12aef50f649e6f5f9689970748f96e3f1ca6104e"],"943562acd5eb31fa2fb7384927091158a85ce9fe":["d1f66bb85f641eb4cf7699495d10e0ab50eae3ad"],"1d028314cced5858683a1bb4741423d0f934257b":["66902e88b46bfaaa034e1a881715d5cde7c01146"],"4cf27653a0ff79ebec6be4f231cadb121fafae33":["815287248ca7a77db68038baad5698c5767f36a7","62e52115b56781006682fd92c6938efaf174304d"],"66902e88b46bfaaa034e1a881715d5cde7c01146":["4cf27653a0ff79ebec6be4f231cadb121fafae33"],"62e52115b56781006682fd92c6938efaf174304d":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1d028314cced5858683a1bb4741423d0f934257b","a7e4907084808af8fdb14b9809e6dceaccf6867b"],"1ba83a7997a13459d756c436cc76ee2570d2128f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"7530de27b87b961b51f01bd1299b7004d46e8823":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","17197618d443fdba7cdd03b52a61e09814931638"],"17197618d443fdba7cdd03b52a61e09814931638":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"a7e4907084808af8fdb14b9809e6dceaccf6867b":["1d028314cced5858683a1bb4741423d0f934257b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","62e52115b56781006682fd92c6938efaf174304d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}