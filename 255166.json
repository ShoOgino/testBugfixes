{"path":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getIndexInfo(IndexReader,int,Map[String,TopTermQueue],Set[String]).mjava","commits":[{"id":"eb037ddbc4ef8b427189b9ca13486ea830d0c766","date":1325813112,"type":0,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getIndexInfo(IndexReader,int,Map[String,TopTermQueue],Set[String]).mjava","pathOld":"/dev/null","sourceNew":"  public static SimpleOrderedMap<Object> getIndexInfo( IndexReader reader, int numTerms,\n                                                       Map<String, TopTermQueue> topTerms,\n                                                       Set<String> fieldList) throws IOException {\n    Directory dir = reader.directory();\n    SimpleOrderedMap<Object> indexInfo = new SimpleOrderedMap<Object>();\n\n    indexInfo.add(\"numDocs\", reader.numDocs());\n    indexInfo.add(\"maxDoc\", reader.maxDoc());\n    final CharsRef spare = new CharsRef();\n    if( numTerms > 0 ) {\n      Fields fields = MultiFields.getFields(reader);\n      long totalTerms = 0;\n      if (fields != null) {\n        FieldsEnum fieldsEnum = fields.iterator();\n        String field;\n        while ((field = fieldsEnum.next()) != null) {\n          Terms terms = fieldsEnum.terms();\n          if (terms == null) {\n            continue;\n          }\n          totalTerms += terms.getUniqueTermCount();\n\n          if (fieldList != null && !fieldList.contains(field)) {\n            continue;\n          }\n\n          TermsEnum termsEnum = terms.iterator(null);\n          BytesRef text;\n          int[] buckets = new int[HIST_ARRAY_SIZE];\n          TopTermQueue tiq = topTerms.get(field);\n          if (tiq == null) {\n            tiq = new TopTermQueue(numTerms + 1);   // Allocating slots for the top N terms to collect freqs.\n            topTerms.put(field, tiq);\n          }\n          while ((text = termsEnum.next()) != null) {\n            int freq = termsEnum.docFreq();  // This calculation seems odd, but it gives the same results as it used to.\n            int slot = 32 - Integer.numberOfLeadingZeros(Math.max(0, freq - 1));\n            buckets[slot] = buckets[slot] + 1;\n            if (freq > tiq.minFreq) {\n              UnicodeUtil.UTF8toUTF16(text, spare);\n              String t = spare.toString();\n              tiq.distinctTerms = new Long(fieldsEnum.terms().getUniqueTermCount()).intValue();\n\n              tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n              if (tiq.size() > numTerms) { // if tiq full\n                tiq.pop(); // remove lowest in tiq\n                tiq.minFreq  = tiq.getTopTermInfo().docFreq;\n              }\n            }\n          }\n          tiq.histogram.add(buckets);\n        }\n      }\n      //Clumsy, but I'm tired.\n      indexInfo.add(\"numTerms\", (new Long(totalTerms)).intValue());\n\n    }\n    indexInfo.add(\"version\", reader.getVersion());  // TODO? Is this different then: IndexReader.getCurrentVersion( dir )?\n    indexInfo.add(\"segmentCount\", reader.getSequentialSubReaders().length);\n    indexInfo.add(\"current\", reader.isCurrent() );\n    indexInfo.add(\"hasDeletions\", reader.hasDeletions() );\n    indexInfo.add(\"directory\", dir );\n    indexInfo.add(\"lastModified\", new Date(IndexReader.lastModified(dir)) );\n    return indexInfo;\n  }\n  //////////////////////// SolrInfoMBeans methods //////////////////////\n\n","sourceOld":null,"bugFix":null,"bugIntro":["6b05ecbfa2ad21f9ba1f9f694d6c5f5c19e22e4a","6b05ecbfa2ad21f9ba1f9f694d6c5f5c19e22e4a","c01638f4dd94981c1d3d52c4f7991246a5a24aba"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c01638f4dd94981c1d3d52c4f7991246a5a24aba","date":1327876712,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getIndexInfo(IndexReader,int,Map[String,TopTermQueue],Set[String]).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getIndexInfo(IndexReader,int,Map[String,TopTermQueue],Set[String]).mjava","sourceNew":"  public static SimpleOrderedMap<Object> getIndexInfo( IndexReader reader, int numTerms,\n                                                       Map<String, TopTermQueue> topTerms,\n                                                       Set<String> fieldList) throws IOException {\n    Directory dir = reader.directory();\n    SimpleOrderedMap<Object> indexInfo = new SimpleOrderedMap<Object>();\n\n    indexInfo.add(\"numDocs\", reader.numDocs());\n    indexInfo.add(\"maxDoc\", reader.maxDoc());\n    final CharsRef spare = new CharsRef();\n    if( numTerms > 0 ) {\n      Fields fields = MultiFields.getFields(reader);\n      long totalTerms = 0;\n      if (fields != null) {\n        FieldsEnum fieldsEnum = fields.iterator();\n        String field;\n        while ((field = fieldsEnum.next()) != null) {\n          Terms terms = fieldsEnum.terms();\n          if (terms == null) {\n            continue;\n          }\n          totalTerms += terms.getUniqueTermCount();\n\n          if (fieldList != null && !fieldList.contains(field)) {\n            continue;\n          }\n\n          TermsEnum termsEnum = terms.iterator(null);\n          BytesRef text;\n          int[] buckets = new int[HIST_ARRAY_SIZE];\n          TopTermQueue tiq = topTerms.get(field);\n          if (tiq == null) {\n            tiq = new TopTermQueue(numTerms + 1);   // Allocating slots for the top N terms to collect freqs.\n            topTerms.put(field, tiq);\n          }\n          while ((text = termsEnum.next()) != null) {\n            int freq = termsEnum.docFreq();  // This calculation seems odd, but it gives the same results as it used to.\n            int slot = 32 - Integer.numberOfLeadingZeros(Math.max(0, freq - 1));\n            buckets[slot] = buckets[slot] + 1;\n            if (freq > tiq.minFreq) {\n              UnicodeUtil.UTF8toUTF16(text, spare);\n              String t = spare.toString();\n              tiq.distinctTerms = new Long(fieldsEnum.terms().getUniqueTermCount()).intValue();\n\n              tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n              if (tiq.size() > numTerms) { // if tiq full\n                tiq.pop(); // remove lowest in tiq\n                tiq.minFreq  = tiq.getTopTermInfo().docFreq;\n              }\n            }\n          }\n          tiq.histogram.add(buckets);\n        }\n      }\n      //Clumsy, but I'm tired.\n      indexInfo.add(\"numTerms\", (new Long(totalTerms)).intValue());\n\n    }\n    indexInfo.add(\"version\", reader.getVersion());  // TODO? Is this different then: IndexReader.getCurrentVersion( dir )?\n    indexInfo.add(\"segmentCount\", reader.getSequentialSubReaders().length);\n    indexInfo.add(\"current\", reader.isCurrent() );\n    indexInfo.add(\"hasDeletions\", reader.hasDeletions() );\n    indexInfo.add(\"directory\", dir );\n    String s = reader.getIndexCommit().getUserData().get(SolrIndexWriter.COMMIT_TIME_MSEC_KEY);\n    if (s != null) {\n      indexInfo.add(\"lastModified\", new Date(Long.parseLong(s)));\n    }\n    return indexInfo;\n  }\n  //////////////////////// SolrInfoMBeans methods //////////////////////\n\n","sourceOld":"  public static SimpleOrderedMap<Object> getIndexInfo( IndexReader reader, int numTerms,\n                                                       Map<String, TopTermQueue> topTerms,\n                                                       Set<String> fieldList) throws IOException {\n    Directory dir = reader.directory();\n    SimpleOrderedMap<Object> indexInfo = new SimpleOrderedMap<Object>();\n\n    indexInfo.add(\"numDocs\", reader.numDocs());\n    indexInfo.add(\"maxDoc\", reader.maxDoc());\n    final CharsRef spare = new CharsRef();\n    if( numTerms > 0 ) {\n      Fields fields = MultiFields.getFields(reader);\n      long totalTerms = 0;\n      if (fields != null) {\n        FieldsEnum fieldsEnum = fields.iterator();\n        String field;\n        while ((field = fieldsEnum.next()) != null) {\n          Terms terms = fieldsEnum.terms();\n          if (terms == null) {\n            continue;\n          }\n          totalTerms += terms.getUniqueTermCount();\n\n          if (fieldList != null && !fieldList.contains(field)) {\n            continue;\n          }\n\n          TermsEnum termsEnum = terms.iterator(null);\n          BytesRef text;\n          int[] buckets = new int[HIST_ARRAY_SIZE];\n          TopTermQueue tiq = topTerms.get(field);\n          if (tiq == null) {\n            tiq = new TopTermQueue(numTerms + 1);   // Allocating slots for the top N terms to collect freqs.\n            topTerms.put(field, tiq);\n          }\n          while ((text = termsEnum.next()) != null) {\n            int freq = termsEnum.docFreq();  // This calculation seems odd, but it gives the same results as it used to.\n            int slot = 32 - Integer.numberOfLeadingZeros(Math.max(0, freq - 1));\n            buckets[slot] = buckets[slot] + 1;\n            if (freq > tiq.minFreq) {\n              UnicodeUtil.UTF8toUTF16(text, spare);\n              String t = spare.toString();\n              tiq.distinctTerms = new Long(fieldsEnum.terms().getUniqueTermCount()).intValue();\n\n              tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n              if (tiq.size() > numTerms) { // if tiq full\n                tiq.pop(); // remove lowest in tiq\n                tiq.minFreq  = tiq.getTopTermInfo().docFreq;\n              }\n            }\n          }\n          tiq.histogram.add(buckets);\n        }\n      }\n      //Clumsy, but I'm tired.\n      indexInfo.add(\"numTerms\", (new Long(totalTerms)).intValue());\n\n    }\n    indexInfo.add(\"version\", reader.getVersion());  // TODO? Is this different then: IndexReader.getCurrentVersion( dir )?\n    indexInfo.add(\"segmentCount\", reader.getSequentialSubReaders().length);\n    indexInfo.add(\"current\", reader.isCurrent() );\n    indexInfo.add(\"hasDeletions\", reader.hasDeletions() );\n    indexInfo.add(\"directory\", dir );\n    indexInfo.add(\"lastModified\", new Date(IndexReader.lastModified(dir)) );\n    return indexInfo;\n  }\n  //////////////////////// SolrInfoMBeans methods //////////////////////\n\n","bugFix":["eb037ddbc4ef8b427189b9ca13486ea830d0c766"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"817882884229bace7dc5d1b75f6b0e4aa1e47122","date":1327879145,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getIndexInfo(IndexReader,int,Map[String,TopTermQueue],Set[String]).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getIndexInfo(IndexReader,int,Map[String,TopTermQueue],Set[String]).mjava","sourceNew":"  public static SimpleOrderedMap<Object> getIndexInfo( IndexReader reader, int numTerms,\n                                                       Map<String, TopTermQueue> topTerms,\n                                                       Set<String> fieldList) throws IOException {\n    Directory dir = reader.directory();\n    SimpleOrderedMap<Object> indexInfo = new SimpleOrderedMap<Object>();\n\n    indexInfo.add(\"numDocs\", reader.numDocs());\n    indexInfo.add(\"maxDoc\", reader.maxDoc());\n    final CharsRef spare = new CharsRef();\n    if( numTerms > 0 ) {\n      Fields fields = MultiFields.getFields(reader);\n      long totalTerms = 0;\n      if (fields != null) {\n        FieldsEnum fieldsEnum = fields.iterator();\n        String field;\n        while ((field = fieldsEnum.next()) != null) {\n          Terms terms = fieldsEnum.terms();\n          if (terms == null) {\n            continue;\n          }\n          totalTerms += terms.getUniqueTermCount();\n\n          if (fieldList != null && !fieldList.contains(field)) {\n            continue;\n          }\n\n          TermsEnum termsEnum = terms.iterator(null);\n          BytesRef text;\n          int[] buckets = new int[HIST_ARRAY_SIZE];\n          TopTermQueue tiq = topTerms.get(field);\n          if (tiq == null) {\n            tiq = new TopTermQueue(numTerms + 1);   // Allocating slots for the top N terms to collect freqs.\n            topTerms.put(field, tiq);\n          }\n          while ((text = termsEnum.next()) != null) {\n            int freq = termsEnum.docFreq();  // This calculation seems odd, but it gives the same results as it used to.\n            int slot = 32 - Integer.numberOfLeadingZeros(Math.max(0, freq - 1));\n            buckets[slot] = buckets[slot] + 1;\n            if (freq > tiq.minFreq) {\n              UnicodeUtil.UTF8toUTF16(text, spare);\n              String t = spare.toString();\n              tiq.distinctTerms = new Long(fieldsEnum.terms().getUniqueTermCount()).intValue();\n\n              tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n              if (tiq.size() > numTerms) { // if tiq full\n                tiq.pop(); // remove lowest in tiq\n                tiq.minFreq  = tiq.getTopTermInfo().docFreq;\n              }\n            }\n          }\n          tiq.histogram.add(buckets);\n        }\n      }\n      //Clumsy, but I'm tired.\n      indexInfo.add(\"numTerms\", (new Long(totalTerms)).intValue());\n\n    }\n    indexInfo.add(\"version\", reader.getVersion());  // TODO? Is this different then: IndexReader.getCurrentVersion( dir )?\n    indexInfo.add(\"segmentCount\", reader.getSequentialSubReaders().length);\n    indexInfo.add(\"current\", reader.isCurrent() );\n    indexInfo.add(\"hasDeletions\", reader.hasDeletions() );\n    indexInfo.add(\"directory\", dir );\n    String s = reader.getIndexCommit().getUserData().get(SolrIndexWriter.COMMIT_TIME_MSEC_KEY);\n    if (s != null) {\n      indexInfo.add(\"lastModified\", new Date(Long.parseLong(s)));\n    }\n    return indexInfo;\n  }\n  //////////////////////// SolrInfoMBeans methods //////////////////////\n\n","sourceOld":"  public static SimpleOrderedMap<Object> getIndexInfo( IndexReader reader, int numTerms,\n                                                       Map<String, TopTermQueue> topTerms,\n                                                       Set<String> fieldList) throws IOException {\n    Directory dir = reader.directory();\n    SimpleOrderedMap<Object> indexInfo = new SimpleOrderedMap<Object>();\n\n    indexInfo.add(\"numDocs\", reader.numDocs());\n    indexInfo.add(\"maxDoc\", reader.maxDoc());\n    final CharsRef spare = new CharsRef();\n    if( numTerms > 0 ) {\n      Fields fields = MultiFields.getFields(reader);\n      long totalTerms = 0;\n      if (fields != null) {\n        FieldsEnum fieldsEnum = fields.iterator();\n        String field;\n        while ((field = fieldsEnum.next()) != null) {\n          Terms terms = fieldsEnum.terms();\n          if (terms == null) {\n            continue;\n          }\n          totalTerms += terms.getUniqueTermCount();\n\n          if (fieldList != null && !fieldList.contains(field)) {\n            continue;\n          }\n\n          TermsEnum termsEnum = terms.iterator(null);\n          BytesRef text;\n          int[] buckets = new int[HIST_ARRAY_SIZE];\n          TopTermQueue tiq = topTerms.get(field);\n          if (tiq == null) {\n            tiq = new TopTermQueue(numTerms + 1);   // Allocating slots for the top N terms to collect freqs.\n            topTerms.put(field, tiq);\n          }\n          while ((text = termsEnum.next()) != null) {\n            int freq = termsEnum.docFreq();  // This calculation seems odd, but it gives the same results as it used to.\n            int slot = 32 - Integer.numberOfLeadingZeros(Math.max(0, freq - 1));\n            buckets[slot] = buckets[slot] + 1;\n            if (freq > tiq.minFreq) {\n              UnicodeUtil.UTF8toUTF16(text, spare);\n              String t = spare.toString();\n              tiq.distinctTerms = new Long(fieldsEnum.terms().getUniqueTermCount()).intValue();\n\n              tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n              if (tiq.size() > numTerms) { // if tiq full\n                tiq.pop(); // remove lowest in tiq\n                tiq.minFreq  = tiq.getTopTermInfo().docFreq;\n              }\n            }\n          }\n          tiq.histogram.add(buckets);\n        }\n      }\n      //Clumsy, but I'm tired.\n      indexInfo.add(\"numTerms\", (new Long(totalTerms)).intValue());\n\n    }\n    indexInfo.add(\"version\", reader.getVersion());  // TODO? Is this different then: IndexReader.getCurrentVersion( dir )?\n    indexInfo.add(\"segmentCount\", reader.getSequentialSubReaders().length);\n    indexInfo.add(\"current\", reader.isCurrent() );\n    indexInfo.add(\"hasDeletions\", reader.hasDeletions() );\n    indexInfo.add(\"directory\", dir );\n    indexInfo.add(\"lastModified\", new Date(IndexReader.lastModified(dir)) );\n    return indexInfo;\n  }\n  //////////////////////// SolrInfoMBeans methods //////////////////////\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"96d207426bd26fa5c1014e26d21d87603aea68b7","date":1327944562,"type":5,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getIndexInfo(DirectoryReader,int,Map[String,TopTermQueue],Set[String]).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getIndexInfo(IndexReader,int,Map[String,TopTermQueue],Set[String]).mjava","sourceNew":"  public static SimpleOrderedMap<Object> getIndexInfo( DirectoryReader reader, int numTerms,\n                                                       Map<String, TopTermQueue> topTerms,\n                                                       Set<String> fieldList) throws IOException {\n    Directory dir = reader.directory();\n    SimpleOrderedMap<Object> indexInfo = new SimpleOrderedMap<Object>();\n\n    indexInfo.add(\"numDocs\", reader.numDocs());\n    indexInfo.add(\"maxDoc\", reader.maxDoc());\n    final CharsRef spare = new CharsRef();\n    if( numTerms > 0 ) {\n      Fields fields = MultiFields.getFields(reader);\n      long totalTerms = 0;\n      if (fields != null) {\n        FieldsEnum fieldsEnum = fields.iterator();\n        String field;\n        while ((field = fieldsEnum.next()) != null) {\n          Terms terms = fieldsEnum.terms();\n          if (terms == null) {\n            continue;\n          }\n          totalTerms += terms.getUniqueTermCount();\n\n          if (fieldList != null && !fieldList.contains(field)) {\n            continue;\n          }\n\n          TermsEnum termsEnum = terms.iterator(null);\n          BytesRef text;\n          int[] buckets = new int[HIST_ARRAY_SIZE];\n          TopTermQueue tiq = topTerms.get(field);\n          if (tiq == null) {\n            tiq = new TopTermQueue(numTerms + 1);   // Allocating slots for the top N terms to collect freqs.\n            topTerms.put(field, tiq);\n          }\n          while ((text = termsEnum.next()) != null) {\n            int freq = termsEnum.docFreq();  // This calculation seems odd, but it gives the same results as it used to.\n            int slot = 32 - Integer.numberOfLeadingZeros(Math.max(0, freq - 1));\n            buckets[slot] = buckets[slot] + 1;\n            if (freq > tiq.minFreq) {\n              UnicodeUtil.UTF8toUTF16(text, spare);\n              String t = spare.toString();\n              tiq.distinctTerms = new Long(fieldsEnum.terms().getUniqueTermCount()).intValue();\n\n              tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n              if (tiq.size() > numTerms) { // if tiq full\n                tiq.pop(); // remove lowest in tiq\n                tiq.minFreq  = tiq.getTopTermInfo().docFreq;\n              }\n            }\n          }\n          tiq.histogram.add(buckets);\n        }\n      }\n      //Clumsy, but I'm tired.\n      indexInfo.add(\"numTerms\", (new Long(totalTerms)).intValue());\n\n    }\n    indexInfo.add(\"version\", reader.getVersion());  // TODO? Is this different then: IndexReader.getCurrentVersion( dir )?\n    indexInfo.add(\"segmentCount\", reader.getSequentialSubReaders().length);\n    indexInfo.add(\"current\", reader.isCurrent() );\n    indexInfo.add(\"hasDeletions\", reader.hasDeletions() );\n    indexInfo.add(\"directory\", dir );\n    String s = reader.getIndexCommit().getUserData().get(SolrIndexWriter.COMMIT_TIME_MSEC_KEY);\n    if (s != null) {\n      indexInfo.add(\"lastModified\", new Date(Long.parseLong(s)));\n    }\n    return indexInfo;\n  }\n  //////////////////////// SolrInfoMBeans methods //////////////////////\n\n","sourceOld":"  public static SimpleOrderedMap<Object> getIndexInfo( IndexReader reader, int numTerms,\n                                                       Map<String, TopTermQueue> topTerms,\n                                                       Set<String> fieldList) throws IOException {\n    Directory dir = reader.directory();\n    SimpleOrderedMap<Object> indexInfo = new SimpleOrderedMap<Object>();\n\n    indexInfo.add(\"numDocs\", reader.numDocs());\n    indexInfo.add(\"maxDoc\", reader.maxDoc());\n    final CharsRef spare = new CharsRef();\n    if( numTerms > 0 ) {\n      Fields fields = MultiFields.getFields(reader);\n      long totalTerms = 0;\n      if (fields != null) {\n        FieldsEnum fieldsEnum = fields.iterator();\n        String field;\n        while ((field = fieldsEnum.next()) != null) {\n          Terms terms = fieldsEnum.terms();\n          if (terms == null) {\n            continue;\n          }\n          totalTerms += terms.getUniqueTermCount();\n\n          if (fieldList != null && !fieldList.contains(field)) {\n            continue;\n          }\n\n          TermsEnum termsEnum = terms.iterator(null);\n          BytesRef text;\n          int[] buckets = new int[HIST_ARRAY_SIZE];\n          TopTermQueue tiq = topTerms.get(field);\n          if (tiq == null) {\n            tiq = new TopTermQueue(numTerms + 1);   // Allocating slots for the top N terms to collect freqs.\n            topTerms.put(field, tiq);\n          }\n          while ((text = termsEnum.next()) != null) {\n            int freq = termsEnum.docFreq();  // This calculation seems odd, but it gives the same results as it used to.\n            int slot = 32 - Integer.numberOfLeadingZeros(Math.max(0, freq - 1));\n            buckets[slot] = buckets[slot] + 1;\n            if (freq > tiq.minFreq) {\n              UnicodeUtil.UTF8toUTF16(text, spare);\n              String t = spare.toString();\n              tiq.distinctTerms = new Long(fieldsEnum.terms().getUniqueTermCount()).intValue();\n\n              tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n              if (tiq.size() > numTerms) { // if tiq full\n                tiq.pop(); // remove lowest in tiq\n                tiq.minFreq  = tiq.getTopTermInfo().docFreq;\n              }\n            }\n          }\n          tiq.histogram.add(buckets);\n        }\n      }\n      //Clumsy, but I'm tired.\n      indexInfo.add(\"numTerms\", (new Long(totalTerms)).intValue());\n\n    }\n    indexInfo.add(\"version\", reader.getVersion());  // TODO? Is this different then: IndexReader.getCurrentVersion( dir )?\n    indexInfo.add(\"segmentCount\", reader.getSequentialSubReaders().length);\n    indexInfo.add(\"current\", reader.isCurrent() );\n    indexInfo.add(\"hasDeletions\", reader.hasDeletions() );\n    indexInfo.add(\"directory\", dir );\n    String s = reader.getIndexCommit().getUserData().get(SolrIndexWriter.COMMIT_TIME_MSEC_KEY);\n    if (s != null) {\n      indexInfo.add(\"lastModified\", new Date(Long.parseLong(s)));\n    }\n    return indexInfo;\n  }\n  //////////////////////// SolrInfoMBeans methods //////////////////////\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5b6fdfce35d0adb18836cf8711abe487a934df33","date":1327946200,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getIndexInfo(IndexReader,int,Map[String,TopTermQueue],Set[String]).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getIndexInfo(IndexReader,int,Map[String,TopTermQueue],Set[String]).mjava","sourceNew":"  public static SimpleOrderedMap<Object> getIndexInfo( IndexReader reader, int numTerms,\n                                                       Map<String, TopTermQueue> topTerms,\n                                                       Set<String> fieldList) throws IOException {\n    Directory dir = reader.directory();\n    SimpleOrderedMap<Object> indexInfo = new SimpleOrderedMap<Object>();\n\n    indexInfo.add(\"numDocs\", reader.numDocs());\n    indexInfo.add(\"maxDoc\", reader.maxDoc());\n    final CharsRef spare = new CharsRef();\n    if( numTerms > 0 ) {\n      Fields fields = MultiFields.getFields(reader);\n      long totalTerms = 0;\n      if (fields != null) {\n        FieldsEnum fieldsEnum = fields.iterator();\n        String field;\n        while ((field = fieldsEnum.next()) != null) {\n          Terms terms = fieldsEnum.terms();\n          if (terms == null) {\n            continue;\n          }\n          totalTerms += terms.getUniqueTermCount();\n\n          if (fieldList != null && !fieldList.contains(field)) {\n            continue;\n          }\n\n          TermsEnum termsEnum = terms.iterator(null);\n          BytesRef text;\n          int[] buckets = new int[HIST_ARRAY_SIZE];\n          TopTermQueue tiq = topTerms.get(field);\n          if (tiq == null) {\n            tiq = new TopTermQueue(numTerms + 1);   // Allocating slots for the top N terms to collect freqs.\n            topTerms.put(field, tiq);\n          }\n          while ((text = termsEnum.next()) != null) {\n            int freq = termsEnum.docFreq();  // This calculation seems odd, but it gives the same results as it used to.\n            int slot = 32 - Integer.numberOfLeadingZeros(Math.max(0, freq - 1));\n            buckets[slot] = buckets[slot] + 1;\n            if (freq > tiq.minFreq) {\n              UnicodeUtil.UTF8toUTF16(text, spare);\n              String t = spare.toString();\n              tiq.distinctTerms = new Long(fieldsEnum.terms().getUniqueTermCount()).intValue();\n\n              tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n              if (tiq.size() > numTerms) { // if tiq full\n                tiq.pop(); // remove lowest in tiq\n                tiq.minFreq  = tiq.getTopTermInfo().docFreq;\n              }\n            }\n          }\n          tiq.histogram.add(buckets);\n        }\n      }\n      //Clumsy, but I'm tired.\n      indexInfo.add(\"numTerms\", (new Long(totalTerms)).intValue());\n\n    }\n    indexInfo.add(\"version\", reader.getVersion());  // TODO? Is this different then: IndexReader.getCurrentVersion( dir )?\n    indexInfo.add(\"segmentCount\", reader.getSequentialSubReaders().length);\n    indexInfo.add(\"current\", reader.isCurrent() );\n    indexInfo.add(\"hasDeletions\", reader.hasDeletions() );\n    indexInfo.add(\"directory\", dir );\n    String s = reader.getIndexCommit().getUserData().get(SolrIndexWriter.COMMIT_TIME_MSEC_KEY);\n    if (s != null) {\n      indexInfo.add(\"lastModified\", new Date(Long.parseLong(s)));\n    }\n    return indexInfo;\n  }\n  //////////////////////// SolrInfoMBeans methods //////////////////////\n\n","sourceOld":"  public static SimpleOrderedMap<Object> getIndexInfo( IndexReader reader, int numTerms,\n                                                       Map<String, TopTermQueue> topTerms,\n                                                       Set<String> fieldList) throws IOException {\n    Directory dir = reader.directory();\n    SimpleOrderedMap<Object> indexInfo = new SimpleOrderedMap<Object>();\n\n    indexInfo.add(\"numDocs\", reader.numDocs());\n    indexInfo.add(\"maxDoc\", reader.maxDoc());\n    final CharsRef spare = new CharsRef();\n    if( numTerms > 0 ) {\n      Fields fields = MultiFields.getFields(reader);\n      long totalTerms = 0;\n      if (fields != null) {\n        FieldsEnum fieldsEnum = fields.iterator();\n        String field;\n        while ((field = fieldsEnum.next()) != null) {\n          Terms terms = fieldsEnum.terms();\n          if (terms == null) {\n            continue;\n          }\n          totalTerms += terms.getUniqueTermCount();\n\n          if (fieldList != null && !fieldList.contains(field)) {\n            continue;\n          }\n\n          TermsEnum termsEnum = terms.iterator(null);\n          BytesRef text;\n          int[] buckets = new int[HIST_ARRAY_SIZE];\n          TopTermQueue tiq = topTerms.get(field);\n          if (tiq == null) {\n            tiq = new TopTermQueue(numTerms + 1);   // Allocating slots for the top N terms to collect freqs.\n            topTerms.put(field, tiq);\n          }\n          while ((text = termsEnum.next()) != null) {\n            int freq = termsEnum.docFreq();  // This calculation seems odd, but it gives the same results as it used to.\n            int slot = 32 - Integer.numberOfLeadingZeros(Math.max(0, freq - 1));\n            buckets[slot] = buckets[slot] + 1;\n            if (freq > tiq.minFreq) {\n              UnicodeUtil.UTF8toUTF16(text, spare);\n              String t = spare.toString();\n              tiq.distinctTerms = new Long(fieldsEnum.terms().getUniqueTermCount()).intValue();\n\n              tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n              if (tiq.size() > numTerms) { // if tiq full\n                tiq.pop(); // remove lowest in tiq\n                tiq.minFreq  = tiq.getTopTermInfo().docFreq;\n              }\n            }\n          }\n          tiq.histogram.add(buckets);\n        }\n      }\n      //Clumsy, but I'm tired.\n      indexInfo.add(\"numTerms\", (new Long(totalTerms)).intValue());\n\n    }\n    indexInfo.add(\"version\", reader.getVersion());  // TODO? Is this different then: IndexReader.getCurrentVersion( dir )?\n    indexInfo.add(\"segmentCount\", reader.getSequentialSubReaders().length);\n    indexInfo.add(\"current\", reader.isCurrent() );\n    indexInfo.add(\"hasDeletions\", reader.hasDeletions() );\n    indexInfo.add(\"directory\", dir );\n    indexInfo.add(\"lastModified\", new Date(IndexReader.lastModified(dir)) );\n    return indexInfo;\n  }\n  //////////////////////// SolrInfoMBeans methods //////////////////////\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":5,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getIndexInfo(DirectoryReader,int,Map[String,TopTermQueue],Set[String]).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getIndexInfo(IndexReader,int,Map[String,TopTermQueue],Set[String]).mjava","sourceNew":"  public static SimpleOrderedMap<Object> getIndexInfo( DirectoryReader reader, int numTerms,\n                                                       Map<String, TopTermQueue> topTerms,\n                                                       Set<String> fieldList) throws IOException {\n    Directory dir = reader.directory();\n    SimpleOrderedMap<Object> indexInfo = new SimpleOrderedMap<Object>();\n\n    indexInfo.add(\"numDocs\", reader.numDocs());\n    indexInfo.add(\"maxDoc\", reader.maxDoc());\n    final CharsRef spare = new CharsRef();\n    if( numTerms > 0 ) {\n      Fields fields = MultiFields.getFields(reader);\n      long totalTerms = 0;\n      if (fields != null) {\n        FieldsEnum fieldsEnum = fields.iterator();\n        String field;\n        while ((field = fieldsEnum.next()) != null) {\n          Terms terms = fieldsEnum.terms();\n          if (terms == null) {\n            continue;\n          }\n          totalTerms += terms.getUniqueTermCount();\n\n          if (fieldList != null && !fieldList.contains(field)) {\n            continue;\n          }\n\n          TermsEnum termsEnum = terms.iterator(null);\n          BytesRef text;\n          int[] buckets = new int[HIST_ARRAY_SIZE];\n          TopTermQueue tiq = topTerms.get(field);\n          if (tiq == null) {\n            tiq = new TopTermQueue(numTerms + 1);   // Allocating slots for the top N terms to collect freqs.\n            topTerms.put(field, tiq);\n          }\n          while ((text = termsEnum.next()) != null) {\n            int freq = termsEnum.docFreq();  // This calculation seems odd, but it gives the same results as it used to.\n            int slot = 32 - Integer.numberOfLeadingZeros(Math.max(0, freq - 1));\n            buckets[slot] = buckets[slot] + 1;\n            if (freq > tiq.minFreq) {\n              UnicodeUtil.UTF8toUTF16(text, spare);\n              String t = spare.toString();\n              tiq.distinctTerms = new Long(fieldsEnum.terms().getUniqueTermCount()).intValue();\n\n              tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n              if (tiq.size() > numTerms) { // if tiq full\n                tiq.pop(); // remove lowest in tiq\n                tiq.minFreq  = tiq.getTopTermInfo().docFreq;\n              }\n            }\n          }\n          tiq.histogram.add(buckets);\n        }\n      }\n      //Clumsy, but I'm tired.\n      indexInfo.add(\"numTerms\", (new Long(totalTerms)).intValue());\n\n    }\n    indexInfo.add(\"version\", reader.getVersion());  // TODO? Is this different then: IndexReader.getCurrentVersion( dir )?\n    indexInfo.add(\"segmentCount\", reader.getSequentialSubReaders().length);\n    indexInfo.add(\"current\", reader.isCurrent() );\n    indexInfo.add(\"hasDeletions\", reader.hasDeletions() );\n    indexInfo.add(\"directory\", dir );\n    String s = reader.getIndexCommit().getUserData().get(SolrIndexWriter.COMMIT_TIME_MSEC_KEY);\n    if (s != null) {\n      indexInfo.add(\"lastModified\", new Date(Long.parseLong(s)));\n    }\n    return indexInfo;\n  }\n  //////////////////////// SolrInfoMBeans methods //////////////////////\n\n","sourceOld":"  public static SimpleOrderedMap<Object> getIndexInfo( IndexReader reader, int numTerms,\n                                                       Map<String, TopTermQueue> topTerms,\n                                                       Set<String> fieldList) throws IOException {\n    Directory dir = reader.directory();\n    SimpleOrderedMap<Object> indexInfo = new SimpleOrderedMap<Object>();\n\n    indexInfo.add(\"numDocs\", reader.numDocs());\n    indexInfo.add(\"maxDoc\", reader.maxDoc());\n    final CharsRef spare = new CharsRef();\n    if( numTerms > 0 ) {\n      Fields fields = MultiFields.getFields(reader);\n      long totalTerms = 0;\n      if (fields != null) {\n        FieldsEnum fieldsEnum = fields.iterator();\n        String field;\n        while ((field = fieldsEnum.next()) != null) {\n          Terms terms = fieldsEnum.terms();\n          if (terms == null) {\n            continue;\n          }\n          totalTerms += terms.getUniqueTermCount();\n\n          if (fieldList != null && !fieldList.contains(field)) {\n            continue;\n          }\n\n          TermsEnum termsEnum = terms.iterator(null);\n          BytesRef text;\n          int[] buckets = new int[HIST_ARRAY_SIZE];\n          TopTermQueue tiq = topTerms.get(field);\n          if (tiq == null) {\n            tiq = new TopTermQueue(numTerms + 1);   // Allocating slots for the top N terms to collect freqs.\n            topTerms.put(field, tiq);\n          }\n          while ((text = termsEnum.next()) != null) {\n            int freq = termsEnum.docFreq();  // This calculation seems odd, but it gives the same results as it used to.\n            int slot = 32 - Integer.numberOfLeadingZeros(Math.max(0, freq - 1));\n            buckets[slot] = buckets[slot] + 1;\n            if (freq > tiq.minFreq) {\n              UnicodeUtil.UTF8toUTF16(text, spare);\n              String t = spare.toString();\n              tiq.distinctTerms = new Long(fieldsEnum.terms().getUniqueTermCount()).intValue();\n\n              tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n              if (tiq.size() > numTerms) { // if tiq full\n                tiq.pop(); // remove lowest in tiq\n                tiq.minFreq  = tiq.getTopTermInfo().docFreq;\n              }\n            }\n          }\n          tiq.histogram.add(buckets);\n        }\n      }\n      //Clumsy, but I'm tired.\n      indexInfo.add(\"numTerms\", (new Long(totalTerms)).intValue());\n\n    }\n    indexInfo.add(\"version\", reader.getVersion());  // TODO? Is this different then: IndexReader.getCurrentVersion( dir )?\n    indexInfo.add(\"segmentCount\", reader.getSequentialSubReaders().length);\n    indexInfo.add(\"current\", reader.isCurrent() );\n    indexInfo.add(\"hasDeletions\", reader.hasDeletions() );\n    indexInfo.add(\"directory\", dir );\n    String s = reader.getIndexCommit().getUserData().get(SolrIndexWriter.COMMIT_TIME_MSEC_KEY);\n    if (s != null) {\n      indexInfo.add(\"lastModified\", new Date(Long.parseLong(s)));\n    }\n    return indexInfo;\n  }\n  //////////////////////// SolrInfoMBeans methods //////////////////////\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"eb037ddbc4ef8b427189b9ca13486ea830d0c766":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c01638f4dd94981c1d3d52c4f7991246a5a24aba":["eb037ddbc4ef8b427189b9ca13486ea830d0c766"],"817882884229bace7dc5d1b75f6b0e4aa1e47122":["eb037ddbc4ef8b427189b9ca13486ea830d0c766","c01638f4dd94981c1d3d52c4f7991246a5a24aba"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"96d207426bd26fa5c1014e26d21d87603aea68b7":["817882884229bace7dc5d1b75f6b0e4aa1e47122"],"5b6fdfce35d0adb18836cf8711abe487a934df33":["eb037ddbc4ef8b427189b9ca13486ea830d0c766","c01638f4dd94981c1d3d52c4f7991246a5a24aba"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["c01638f4dd94981c1d3d52c4f7991246a5a24aba","96d207426bd26fa5c1014e26d21d87603aea68b7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5cab9a86bd67202d20b6adc463008c8e982b070a"]},"commit2Childs":{"eb037ddbc4ef8b427189b9ca13486ea830d0c766":["c01638f4dd94981c1d3d52c4f7991246a5a24aba","817882884229bace7dc5d1b75f6b0e4aa1e47122","5b6fdfce35d0adb18836cf8711abe487a934df33"],"c01638f4dd94981c1d3d52c4f7991246a5a24aba":["817882884229bace7dc5d1b75f6b0e4aa1e47122","5b6fdfce35d0adb18836cf8711abe487a934df33","5cab9a86bd67202d20b6adc463008c8e982b070a"],"817882884229bace7dc5d1b75f6b0e4aa1e47122":["96d207426bd26fa5c1014e26d21d87603aea68b7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["eb037ddbc4ef8b427189b9ca13486ea830d0c766"],"96d207426bd26fa5c1014e26d21d87603aea68b7":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"5b6fdfce35d0adb18836cf8711abe487a934df33":[],"5cab9a86bd67202d20b6adc463008c8e982b070a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5b6fdfce35d0adb18836cf8711abe487a934df33","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}