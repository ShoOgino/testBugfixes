{"path":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n\n      fieldInfos.addOrUpdate(fieldName, true);\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(BytesRef.deepCopyOf(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n\n      fieldInfos.addOrUpdate(fieldName, true);\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(BytesRef.deepCopyOf(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"07ab701c58e80e47103785a18f274bd1a0b0518d","date":1335099574,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n\n      fieldInfos.addOrUpdate(fieldName, true);\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(BytesRef.deepCopyOf(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n\n      fieldInfos.addOrUpdate(fieldName, true);\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(BytesRef.deepCopyOf(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"76923f6a33f2c4bec7f584e3f251261afe7ea276","date":1337149711,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n\n      fieldInfos.addOrUpdate(fieldName);\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(BytesRef.deepCopyOf(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n\n      fieldInfos.addOrUpdate(fieldName, true);\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(BytesRef.deepCopyOf(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a851824c09818632c94eba41e60ef5e72e323c8e","date":1337355760,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS, null, null));\n      }\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(BytesRef.deepCopyOf(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n\n      fieldInfos.addOrUpdate(fieldName);\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(BytesRef.deepCopyOf(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0f42e0639920b2e917c9ece35fb68ad83021e38","date":1337629438,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS, null, null, null));\n      }\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(BytesRef.deepCopyOf(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS, null, null));\n      }\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(BytesRef.deepCopyOf(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"615ddbd81799980d0fdd95e0238e1c498b6f47b0","date":1338233290,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS, null, null, null));\n      }\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(BytesRef.deepCopyOf(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n\n      fieldInfos.addOrUpdate(fieldName, true);\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(BytesRef.deepCopyOf(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30fd30bfbfa6b9e036bcd99c8339712e965d4a63","date":1351859294,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   *  \n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  \n  public void addField(String fieldName, TokenStream stream, float boost) {\n    addField(fieldName, stream, boost, 0);\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS, null, null, null));\n      }\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(BytesRef.deepCopyOf(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51ab863868b3d59bc3ee8e196a92ac2c4847328e","date":1351888296,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   *  \n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    addField(fieldName, stream, boost, 0);\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   *  \n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  \n  public void addField(String fieldName, TokenStream stream, float boost) {\n    addField(fieldName, stream, boost, 0);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"acd9883560fd89e6448b2b447302fe543040cd4f","date":1488478696,"type":4,"author":"Adrien Grand","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":null,"sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   *  \n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    addField(fieldName, stream, boost, 0);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"76923f6a33f2c4bec7f584e3f251261afe7ea276":["07ab701c58e80e47103785a18f274bd1a0b0518d"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["07ab701c58e80e47103785a18f274bd1a0b0518d","a0f42e0639920b2e917c9ece35fb68ad83021e38"],"51ab863868b3d59bc3ee8e196a92ac2c4847328e":["30fd30bfbfa6b9e036bcd99c8339712e965d4a63"],"30fd30bfbfa6b9e036bcd99c8339712e965d4a63":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"07ab701c58e80e47103785a18f274bd1a0b0518d":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"a0f42e0639920b2e917c9ece35fb68ad83021e38":["a851824c09818632c94eba41e60ef5e72e323c8e"],"a851824c09818632c94eba41e60ef5e72e323c8e":["76923f6a33f2c4bec7f584e3f251261afe7ea276"],"acd9883560fd89e6448b2b447302fe543040cd4f":["51ab863868b3d59bc3ee8e196a92ac2c4847328e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["acd9883560fd89e6448b2b447302fe543040cd4f"]},"commit2Childs":{"76923f6a33f2c4bec7f584e3f251261afe7ea276":["a851824c09818632c94eba41e60ef5e72e323c8e"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["07ab701c58e80e47103785a18f274bd1a0b0518d"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["30fd30bfbfa6b9e036bcd99c8339712e965d4a63"],"51ab863868b3d59bc3ee8e196a92ac2c4847328e":["acd9883560fd89e6448b2b447302fe543040cd4f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"30fd30bfbfa6b9e036bcd99c8339712e965d4a63":["51ab863868b3d59bc3ee8e196a92ac2c4847328e"],"07ab701c58e80e47103785a18f274bd1a0b0518d":["76923f6a33f2c4bec7f584e3f251261afe7ea276","615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"a0f42e0639920b2e917c9ece35fb68ad83021e38":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"a851824c09818632c94eba41e60ef5e72e323c8e":["a0f42e0639920b2e917c9ece35fb68ad83021e38"],"acd9883560fd89e6448b2b447302fe543040cd4f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}