{"path":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    SegmentWriteState state = new SegmentWriteState(null, directory, segment, null, mergedDocs, 0, termIndexInterval);\n\n    final FormatPostingsFieldsConsumer consumer = new FormatPostingsFieldsWriter(state, fieldInfos);\n\n    try {\n      queue = new SegmentMergeQueue(readers.size());\n\n      mergeTermInfos(consumer);\n\n    } finally {\n      consumer.finish();\n      if (queue != null) queue.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    SegmentWriteState state = new SegmentWriteState(null, directory, segment, null, mergedDocs, 0, termIndexInterval);\n\n    final FormatPostingsFieldsConsumer consumer = new FormatPostingsFieldsWriter(state, fieldInfos);\n\n    try {\n      queue = new SegmentMergeQueue(readers.size());\n\n      mergeTermInfos(consumer);\n\n    } finally {\n      consumer.finish();\n      if (queue != null) queue.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"955c32f886db6f6356c9fcdea6b1f1cb4effda24","date":1270581567,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n    codec = codecs.getWriter(segmentWriteState);\n    \n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<IndexReader> subReaders = new ArrayList<IndexReader>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    final int numReaders = readers.size();\n    for(int i=0;i<numReaders;i++) {\n      docBase = new ReaderUtil.Gather(readers.get(i)) {\n          @Override\n          protected void add(int base, IndexReader r) throws IOException {\n            subReaders.add(r);\n            fields.add(r.fields());\n            slices.add(new ReaderUtil.Slice(base, r.maxDoc(), fields.size()-1));\n            bits.add(r.getDeletedDocs());\n            bitsStarts.add(base);\n          }\n        }.run(docBase);\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = subReaders;\n    mergeState.readerCount = subReaders.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n    \n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    final int[] starts = new int[mergeState.readerCount+1];\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = subReaders.get(i);\n\n      starts[i] = inputDocBase;\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        Bits deletedDocs = reader.getDeletedDocs();\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (deletedDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n    }\n    starts[mergeState.readerCount] = inputDocBase;\n\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultidDcsEnum.... this only matters when we are\n    // interacting with a non-core IR subclass, because\n    // LegacyFieldsEnum.LegacyDocs[AndPositions]Enum checks\n    // that the skipDocs matches the delDocs for the reader\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n    \n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    SegmentWriteState state = new SegmentWriteState(null, directory, segment, null, mergedDocs, 0, termIndexInterval);\n\n    final FormatPostingsFieldsConsumer consumer = new FormatPostingsFieldsWriter(state, fieldInfos);\n\n    try {\n      queue = new SegmentMergeQueue(readers.size());\n\n      mergeTermInfos(consumer);\n\n    } finally {\n      consumer.finish();\n      if (queue != null) queue.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["b43b719dab44d1ccc5ee5b6e01c50f1ee86bb76c","406e7055a3e99d3fa6ce49a555a51dd18b321806"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b43b719dab44d1ccc5ee5b6e01c50f1ee86bb76c","date":1270671893,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n    codec = codecs.getWriter(segmentWriteState);\n    \n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<IndexReader> subReaders = new ArrayList<IndexReader>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    final int numReaders = readers.size();\n    for(int i=0;i<numReaders;i++) {\n      docBase = new ReaderUtil.Gather(readers.get(i)) {\n          @Override\n          protected void add(int base, IndexReader r) throws IOException {\n            final Fields f = r.fields();\n            if (f != null) {\n              subReaders.add(r);\n              fields.add(f);\n              slices.add(new ReaderUtil.Slice(base, r.maxDoc(), fields.size()-1));\n              bits.add(r.getDeletedDocs());\n              bitsStarts.add(base);\n            }\n          }\n        }.run(docBase);\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = subReaders;\n    mergeState.readerCount = subReaders.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n    \n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    final int[] starts = new int[mergeState.readerCount+1];\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = subReaders.get(i);\n\n      starts[i] = inputDocBase;\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        Bits deletedDocs = reader.getDeletedDocs();\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (deletedDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n    }\n    starts[mergeState.readerCount] = inputDocBase;\n\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultidDcsEnum.... this only matters when we are\n    // interacting with a non-core IR subclass, because\n    // LegacyFieldsEnum.LegacyDocs[AndPositions]Enum checks\n    // that the skipDocs matches the delDocs for the reader\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n    \n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n    codec = codecs.getWriter(segmentWriteState);\n    \n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<IndexReader> subReaders = new ArrayList<IndexReader>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    final int numReaders = readers.size();\n    for(int i=0;i<numReaders;i++) {\n      docBase = new ReaderUtil.Gather(readers.get(i)) {\n          @Override\n          protected void add(int base, IndexReader r) throws IOException {\n            subReaders.add(r);\n            fields.add(r.fields());\n            slices.add(new ReaderUtil.Slice(base, r.maxDoc(), fields.size()-1));\n            bits.add(r.getDeletedDocs());\n            bitsStarts.add(base);\n          }\n        }.run(docBase);\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = subReaders;\n    mergeState.readerCount = subReaders.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n    \n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    final int[] starts = new int[mergeState.readerCount+1];\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = subReaders.get(i);\n\n      starts[i] = inputDocBase;\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        Bits deletedDocs = reader.getDeletedDocs();\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (deletedDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n    }\n    starts[mergeState.readerCount] = inputDocBase;\n\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultidDcsEnum.... this only matters when we are\n    // interacting with a non-core IR subclass, because\n    // LegacyFieldsEnum.LegacyDocs[AndPositions]Enum checks\n    // that the skipDocs matches the delDocs for the reader\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n    \n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","bugFix":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"433777d1eaf9998136cd16515dc0e1eb26f5d535","date":1273839120,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n    codec = codecs.getWriter(segmentWriteState);\n    \n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<IndexReader> subReaders = new ArrayList<IndexReader>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    final int numReaders = readers.size();\n    for(int i=0;i<numReaders;i++) {\n      docBase = new ReaderUtil.Gather(readers.get(i)) {\n          @Override\n          protected void add(int base, IndexReader r) throws IOException {\n            final Fields f = r.fields();\n            if (f != null) {\n              subReaders.add(r);\n              fields.add(f);\n              slices.add(new ReaderUtil.Slice(base, r.maxDoc(), fields.size()-1));\n              bits.add(r.getDeletedDocs());\n              bitsStarts.add(base);\n            }\n          }\n        }.run(docBase);\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = subReaders;\n    mergeState.readerCount = subReaders.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n    \n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    final int[] starts = new int[mergeState.readerCount+1];\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = subReaders.get(i);\n\n      starts[i] = inputDocBase;\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        Bits deletedDocs = reader.getDeletedDocs();\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (deletedDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n      \n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    starts[mergeState.readerCount] = inputDocBase;\n\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultidDcsEnum.... this only matters when we are\n    // interacting with a non-core IR subclass, because\n    // LegacyFieldsEnum.LegacyDocs[AndPositions]Enum checks\n    // that the skipDocs matches the delDocs for the reader\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n    \n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n    codec = codecs.getWriter(segmentWriteState);\n    \n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<IndexReader> subReaders = new ArrayList<IndexReader>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    final int numReaders = readers.size();\n    for(int i=0;i<numReaders;i++) {\n      docBase = new ReaderUtil.Gather(readers.get(i)) {\n          @Override\n          protected void add(int base, IndexReader r) throws IOException {\n            final Fields f = r.fields();\n            if (f != null) {\n              subReaders.add(r);\n              fields.add(f);\n              slices.add(new ReaderUtil.Slice(base, r.maxDoc(), fields.size()-1));\n              bits.add(r.getDeletedDocs());\n              bitsStarts.add(base);\n            }\n          }\n        }.run(docBase);\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = subReaders;\n    mergeState.readerCount = subReaders.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n    \n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    final int[] starts = new int[mergeState.readerCount+1];\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = subReaders.get(i);\n\n      starts[i] = inputDocBase;\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        Bits deletedDocs = reader.getDeletedDocs();\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (deletedDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n    }\n    starts[mergeState.readerCount] = inputDocBase;\n\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultidDcsEnum.... this only matters when we are\n    // interacting with a non-core IR subclass, because\n    // LegacyFieldsEnum.LegacyDocs[AndPositions]Enum checks\n    // that the skipDocs matches the delDocs for the reader\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n    \n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"406e7055a3e99d3fa6ce49a555a51dd18b321806","date":1282520243,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n    codec = codecs.getWriter(segmentWriteState);\n    \n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<IndexReader> subReaders = new ArrayList<IndexReader>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    final int numReaders = readers.size();\n    for(int i=0;i<numReaders;i++) {\n      docBase = new ReaderUtil.Gather(readers.get(i)) {\n          @Override\n          protected void add(int base, IndexReader r) throws IOException {\n            final Fields f = r.fields();\n            if (f != null) {\n              subReaders.add(r);\n              fields.add(f);\n              slices.add(new ReaderUtil.Slice(base, r.maxDoc(), fields.size()-1));\n              bits.add(r.getDeletedDocs());\n              bitsStarts.add(base);\n            }\n          }\n        }.run(docBase);\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = subReaders;\n    mergeState.readerCount = subReaders.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n    \n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    final int[] starts = new int[mergeState.readerCount+1];\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = subReaders.get(i);\n\n      starts[i] = inputDocBase;\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = MultiFields.getDeletedDocs(reader);\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n      \n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    starts[mergeState.readerCount] = inputDocBase;\n\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n    \n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n    codec = codecs.getWriter(segmentWriteState);\n    \n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<IndexReader> subReaders = new ArrayList<IndexReader>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    final int numReaders = readers.size();\n    for(int i=0;i<numReaders;i++) {\n      docBase = new ReaderUtil.Gather(readers.get(i)) {\n          @Override\n          protected void add(int base, IndexReader r) throws IOException {\n            final Fields f = r.fields();\n            if (f != null) {\n              subReaders.add(r);\n              fields.add(f);\n              slices.add(new ReaderUtil.Slice(base, r.maxDoc(), fields.size()-1));\n              bits.add(r.getDeletedDocs());\n              bitsStarts.add(base);\n            }\n          }\n        }.run(docBase);\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = subReaders;\n    mergeState.readerCount = subReaders.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n    \n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    final int[] starts = new int[mergeState.readerCount+1];\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = subReaders.get(i);\n\n      starts[i] = inputDocBase;\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        Bits deletedDocs = reader.getDeletedDocs();\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (deletedDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n      \n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    starts[mergeState.readerCount] = inputDocBase;\n\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultidDcsEnum.... this only matters when we are\n    // interacting with a non-core IR subclass, because\n    // LegacyFieldsEnum.LegacyDocs[AndPositions]Enum checks\n    // that the skipDocs matches the delDocs for the reader\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n    \n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","bugFix":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a493e6d0c3ad86bd55c0a1360d110142e948f2bd","date":1289406991,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n    \n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<IndexReader> subReaders = new ArrayList<IndexReader>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    final int numReaders = readers.size();\n    for(int i=0;i<numReaders;i++) {\n      docBase = new ReaderUtil.Gather(readers.get(i)) {\n          @Override\n          protected void add(int base, IndexReader r) throws IOException {\n            final Fields f = r.fields();\n            if (f != null) {\n              subReaders.add(r);\n              fields.add(f);\n              slices.add(new ReaderUtil.Slice(base, r.maxDoc(), fields.size()-1));\n              bits.add(r.getDeletedDocs());\n              bitsStarts.add(base);\n            }\n          }\n        }.run(docBase);\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = subReaders;\n    mergeState.readerCount = subReaders.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n    \n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    final int[] starts = new int[mergeState.readerCount+1];\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = subReaders.get(i);\n\n      starts[i] = inputDocBase;\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = MultiFields.getDeletedDocs(reader);\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n      \n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    starts[mergeState.readerCount] = inputDocBase;\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n    \n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n    codec = codecs.getWriter(segmentWriteState);\n    \n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<IndexReader> subReaders = new ArrayList<IndexReader>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    final int numReaders = readers.size();\n    for(int i=0;i<numReaders;i++) {\n      docBase = new ReaderUtil.Gather(readers.get(i)) {\n          @Override\n          protected void add(int base, IndexReader r) throws IOException {\n            final Fields f = r.fields();\n            if (f != null) {\n              subReaders.add(r);\n              fields.add(f);\n              slices.add(new ReaderUtil.Slice(base, r.maxDoc(), fields.size()-1));\n              bits.add(r.getDeletedDocs());\n              bitsStarts.add(base);\n            }\n          }\n        }.run(docBase);\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = subReaders;\n    mergeState.readerCount = subReaders.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n    \n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    final int[] starts = new int[mergeState.readerCount+1];\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = subReaders.get(i);\n\n      starts[i] = inputDocBase;\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = MultiFields.getDeletedDocs(reader);\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n      \n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    starts[mergeState.readerCount] = inputDocBase;\n\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n    \n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"85a883878c0af761245ab048babc63d099f835f3","date":1289553330,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n    \n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<IndexReader> subReaders = new ArrayList<IndexReader>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    final int numReaders = readers.size();\n    for(int i=0;i<numReaders;i++) {\n      docBase = new ReaderUtil.Gather(readers.get(i)) {\n          @Override\n          protected void add(int base, IndexReader r) throws IOException {\n            final Fields f = r.fields();\n            if (f != null) {\n              subReaders.add(r);\n              fields.add(f);\n              slices.add(new ReaderUtil.Slice(base, r.maxDoc(), fields.size()-1));\n              bits.add(r.getDeletedDocs());\n              bitsStarts.add(base);\n            }\n          }\n        }.run(docBase);\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = subReaders;\n    mergeState.readerCount = subReaders.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n    \n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    final int[] starts = new int[mergeState.readerCount+1];\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = subReaders.get(i);\n\n      starts[i] = inputDocBase;\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = MultiFields.getDeletedDocs(reader);\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n      \n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    starts[mergeState.readerCount] = inputDocBase;\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n    \n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n    codec = codecs.getWriter(segmentWriteState);\n    \n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<IndexReader> subReaders = new ArrayList<IndexReader>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    final int numReaders = readers.size();\n    for(int i=0;i<numReaders;i++) {\n      docBase = new ReaderUtil.Gather(readers.get(i)) {\n          @Override\n          protected void add(int base, IndexReader r) throws IOException {\n            final Fields f = r.fields();\n            if (f != null) {\n              subReaders.add(r);\n              fields.add(f);\n              slices.add(new ReaderUtil.Slice(base, r.maxDoc(), fields.size()-1));\n              bits.add(r.getDeletedDocs());\n              bitsStarts.add(base);\n            }\n          }\n        }.run(docBase);\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = subReaders;\n    mergeState.readerCount = subReaders.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n    \n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    final int[] starts = new int[mergeState.readerCount+1];\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = subReaders.get(i);\n\n      starts[i] = inputDocBase;\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = MultiFields.getDeletedDocs(reader);\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n      \n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    starts[mergeState.readerCount] = inputDocBase;\n\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n    \n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7f367dfb9086b92a13c77e2d31112c715cd4502c","date":1290190924,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n    \n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n    \n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n      \n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n    \n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n    \n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<IndexReader> subReaders = new ArrayList<IndexReader>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    final int numReaders = readers.size();\n    for(int i=0;i<numReaders;i++) {\n      docBase = new ReaderUtil.Gather(readers.get(i)) {\n          @Override\n          protected void add(int base, IndexReader r) throws IOException {\n            final Fields f = r.fields();\n            if (f != null) {\n              subReaders.add(r);\n              fields.add(f);\n              slices.add(new ReaderUtil.Slice(base, r.maxDoc(), fields.size()-1));\n              bits.add(r.getDeletedDocs());\n              bitsStarts.add(base);\n            }\n          }\n        }.run(docBase);\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = subReaders;\n    mergeState.readerCount = subReaders.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n    \n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    final int[] starts = new int[mergeState.readerCount+1];\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = subReaders.get(i);\n\n      starts[i] = inputDocBase;\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = MultiFields.getDeletedDocs(reader);\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n      \n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    starts[mergeState.readerCount] = inputDocBase;\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n    \n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3bb13258feba31ab676502787ab2e1779f129b7a","date":1291596436,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n    \n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n    \n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n      \n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n    \n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n    \n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<IndexReader> subReaders = new ArrayList<IndexReader>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    final int numReaders = readers.size();\n    for(int i=0;i<numReaders;i++) {\n      docBase = new ReaderUtil.Gather(readers.get(i)) {\n          @Override\n          protected void add(int base, IndexReader r) throws IOException {\n            final Fields f = r.fields();\n            if (f != null) {\n              subReaders.add(r);\n              fields.add(f);\n              slices.add(new ReaderUtil.Slice(base, r.maxDoc(), fields.size()-1));\n              bits.add(r.getDeletedDocs());\n              bitsStarts.add(base);\n            }\n          }\n        }.run(docBase);\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = subReaders;\n    mergeState.readerCount = subReaders.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n    \n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    final int[] starts = new int[mergeState.readerCount+1];\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = subReaders.get(i);\n\n      starts[i] = inputDocBase;\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = MultiFields.getDeletedDocs(reader);\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n      \n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    starts[mergeState.readerCount] = inputDocBase;\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n    \n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n    codec = codecs.getWriter(segmentWriteState);\n    \n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<IndexReader> subReaders = new ArrayList<IndexReader>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    final int numReaders = readers.size();\n    for(int i=0;i<numReaders;i++) {\n      docBase = new ReaderUtil.Gather(readers.get(i)) {\n          @Override\n          protected void add(int base, IndexReader r) throws IOException {\n            final Fields f = r.fields();\n            if (f != null) {\n              subReaders.add(r);\n              fields.add(f);\n              slices.add(new ReaderUtil.Slice(base, r.maxDoc(), fields.size()-1));\n              bits.add(r.getDeletedDocs());\n              bitsStarts.add(base);\n            }\n          }\n        }.run(docBase);\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = subReaders;\n    mergeState.readerCount = subReaders.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n    \n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    final int[] starts = new int[mergeState.readerCount+1];\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = subReaders.get(i);\n\n      starts[i] = inputDocBase;\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        Bits deletedDocs = reader.getDeletedDocs();\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (deletedDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n      \n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    starts[mergeState.readerCount] = inputDocBase;\n\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultidDcsEnum.... this only matters when we are\n    // interacting with a non-core IR subclass, because\n    // LegacyFieldsEnum.LegacyDocs[AndPositions]Enum checks\n    // that the skipDocs matches the delDocs for the reader\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n    \n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e623f9a0e45508ab149c2fb3e0fd0c2503f98186","date":1295889977,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n    \n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n    \n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n      \n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n    \n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n    \n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n    \n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n      \n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n    \n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bb9b72f7c3d7827c64dd4ec580ded81778da361d","date":1295897920,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n    \n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n    \n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n      \n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n    \n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n    \n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n    \n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n      \n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n    \n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n    \n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n    \n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n      \n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n    \n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n    \n    // TODO: move this into its own method - this merges currently only docvalues\n    final List<PerDocValues> perDocProducers = new ArrayList<PerDocValues>();    \n    final List<ReaderUtil.Slice> perDocSlices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> perDocBits = new ArrayList<Bits>();\n    final List<Integer> perDocBitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      final PerDocValues producer = r.perDocValues();\n      if (producer != null) {\n        perDocSlices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        perDocProducers.add(producer);\n        perDocBits.add(r.getDeletedDocs());\n        perDocBitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n    perDocBitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n    if (!perDocSlices.isEmpty()) {\n      mergeState.multiDeletedDocs = new MultiBits(perDocBits, perDocBitsStarts);\n      final PerDocConsumer docsConsumer = codec\n          .docsConsumer(new PerDocWriteState(segmentWriteState));\n      try {\n        docsConsumer.merge(\n            mergeState,\n            new MultiPerDocValues(perDocProducers\n                .toArray(PerDocValues.EMPTY_ARRAY), perDocSlices\n                .toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n      } finally {\n        docsConsumer.close();\n      }\n    }\n    \n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n    \n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n    \n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n      \n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n    \n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"763d13ecba7c2e244aa7c7690a878daae26227f6","date":1305814974,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n    \n    // TODO: move this into its own method - this merges currently only docvalues\n    final List<PerDocValues> perDocProducers = new ArrayList<PerDocValues>();    \n    final List<ReaderUtil.Slice> perDocSlices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> perDocBits = new ArrayList<Bits>();\n    final List<Integer> perDocBitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      final PerDocValues producer = r.perDocValues();\n      if (producer != null) {\n        perDocSlices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        perDocProducers.add(producer);\n        perDocBits.add(r.getDeletedDocs());\n        perDocBitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n    perDocBitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n    if (!perDocSlices.isEmpty()) {\n      mergeState.multiDeletedDocs = new MultiBits(perDocBits, perDocBitsStarts);\n      final PerDocConsumer docsConsumer = codec\n          .docsConsumer(new PerDocWriteState(segmentWriteState));\n      MultiPerDocValues multiPerDocValues = null; \n      try {\n        multiPerDocValues = new MultiPerDocValues(perDocProducers\n            .toArray(PerDocValues.EMPTY_ARRAY), perDocSlices\n            .toArray(ReaderUtil.Slice.EMPTY_ARRAY));\n        docsConsumer.merge(mergeState, multiPerDocValues);\n      } finally {\n        if (multiPerDocValues != null)\n          multiPerDocValues.close();\n        docsConsumer.close();\n      }\n    }\n    \n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n    \n    // TODO: move this into its own method - this merges currently only docvalues\n    final List<PerDocValues> perDocProducers = new ArrayList<PerDocValues>();    \n    final List<ReaderUtil.Slice> perDocSlices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> perDocBits = new ArrayList<Bits>();\n    final List<Integer> perDocBitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      final PerDocValues producer = r.perDocValues();\n      if (producer != null) {\n        perDocSlices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        perDocProducers.add(producer);\n        perDocBits.add(r.getDeletedDocs());\n        perDocBitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n    perDocBitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n    if (!perDocSlices.isEmpty()) {\n      mergeState.multiDeletedDocs = new MultiBits(perDocBits, perDocBitsStarts);\n      final PerDocConsumer docsConsumer = codec\n          .docsConsumer(new PerDocWriteState(segmentWriteState));\n      try {\n        docsConsumer.merge(\n            mergeState,\n            new MultiPerDocValues(perDocProducers\n                .toArray(PerDocValues.EMPTY_ARRAY), perDocSlices\n                .toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n      } finally {\n        docsConsumer.close();\n      }\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":["333709f2f4e93855e69ab88ff9581c580c393891"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"333709f2f4e93855e69ab88ff9581c580c393891","date":1305839574,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n    \n    // TODO: move this into its own method - this merges currently only docvalues\n    final List<PerDocValues> perDocProducers = new ArrayList<PerDocValues>();    \n    final List<ReaderUtil.Slice> perDocSlices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> perDocBits = new ArrayList<Bits>();\n    final List<Integer> perDocBitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      final PerDocValues producer = r.perDocValues();\n      if (producer != null) {\n        perDocSlices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        perDocProducers.add(producer);\n        perDocBits.add(r.getDeletedDocs());\n        perDocBitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n    perDocBitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n    if (!perDocSlices.isEmpty()) {\n      mergeState.multiDeletedDocs = new MultiBits(perDocBits, perDocBitsStarts);\n      final PerDocConsumer docsConsumer = codec\n          .docsConsumer(new PerDocWriteState(segmentWriteState));\n      try {\n        final MultiPerDocValues multiPerDocValues = new MultiPerDocValues(perDocProducers\n            .toArray(PerDocValues.EMPTY_ARRAY), perDocSlices\n            .toArray(ReaderUtil.Slice.EMPTY_ARRAY));\n        docsConsumer.merge(mergeState, multiPerDocValues);\n      } finally {\n        docsConsumer.close();\n      }\n    }\n    \n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n    \n    // TODO: move this into its own method - this merges currently only docvalues\n    final List<PerDocValues> perDocProducers = new ArrayList<PerDocValues>();    \n    final List<ReaderUtil.Slice> perDocSlices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> perDocBits = new ArrayList<Bits>();\n    final List<Integer> perDocBitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      final PerDocValues producer = r.perDocValues();\n      if (producer != null) {\n        perDocSlices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        perDocProducers.add(producer);\n        perDocBits.add(r.getDeletedDocs());\n        perDocBitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n    perDocBitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n    if (!perDocSlices.isEmpty()) {\n      mergeState.multiDeletedDocs = new MultiBits(perDocBits, perDocBitsStarts);\n      final PerDocConsumer docsConsumer = codec\n          .docsConsumer(new PerDocWriteState(segmentWriteState));\n      MultiPerDocValues multiPerDocValues = null; \n      try {\n        multiPerDocValues = new MultiPerDocValues(perDocProducers\n            .toArray(PerDocValues.EMPTY_ARRAY), perDocSlices\n            .toArray(ReaderUtil.Slice.EMPTY_ARRAY));\n        docsConsumer.merge(mergeState, multiPerDocValues);\n      } finally {\n        if (multiPerDocValues != null)\n          multiPerDocValues.close();\n        docsConsumer.close();\n      }\n    }\n    \n  }\n\n","bugFix":["763d13ecba7c2e244aa7c7690a878daae26227f6"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n    \n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n    \n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n      \n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n    \n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"efb7a19703a037c29e30440260d393500febc1f4","date":1306648116,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      // NOTE: this is silly, yet, necessary -- we create a\n      // MultiBits as our skip docs only to have it broken\n      // apart when we step through the docs enums in\n      // MultiDocsEnum.\n      mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n      \n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","date":1306767085,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      // NOTE: this is silly, yet, necessary -- we create a\n      // MultiBits as our skip docs only to have it broken\n      // apart when we step through the docs enums in\n      // MultiDocsEnum.\n      mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n      \n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e10cb22a8bdb44339e282925a29182bb2f3174d","date":1306841137,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n    \n    // TODO: move this into its own method - this merges currently only docvalues\n    final List<PerDocValues> perDocProducers = new ArrayList<PerDocValues>();    \n    final List<ReaderUtil.Slice> perDocSlices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> perDocBits = new ArrayList<Bits>();\n    final List<Integer> perDocBitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      final PerDocValues producer = r.perDocValues();\n      if (producer != null) {\n        perDocSlices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        perDocProducers.add(producer);\n        perDocBits.add(r.getDeletedDocs());\n        perDocBitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n    perDocBitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      // NOTE: this is silly, yet, necessary -- we create a\n      // MultiBits as our skip docs only to have it broken\n      // apart when we step through the docs enums in\n      // MultiDocsEnum.\n      mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n      \n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n    if (!perDocSlices.isEmpty()) {\n      mergeState.multiDeletedDocs = new MultiBits(perDocBits, perDocBitsStarts);\n      final PerDocConsumer docsConsumer = codec\n          .docsConsumer(new PerDocWriteState(segmentWriteState));\n      try {\n        final MultiPerDocValues multiPerDocValues = new MultiPerDocValues(perDocProducers\n            .toArray(PerDocValues.EMPTY_ARRAY), perDocSlices\n            .toArray(ReaderUtil.Slice.EMPTY_ARRAY));\n        docsConsumer.merge(mergeState, multiPerDocValues);\n      } finally {\n        docsConsumer.close();\n      }\n    }\n    \n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n    \n    // TODO: move this into its own method - this merges currently only docvalues\n    final List<PerDocValues> perDocProducers = new ArrayList<PerDocValues>();    \n    final List<ReaderUtil.Slice> perDocSlices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> perDocBits = new ArrayList<Bits>();\n    final List<Integer> perDocBitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      final PerDocValues producer = r.perDocValues();\n      if (producer != null) {\n        perDocSlices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        perDocProducers.add(producer);\n        perDocBits.add(r.getDeletedDocs());\n        perDocBitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n    perDocBitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n\n    // NOTE: this is silly, yet, necessary -- we create a\n    // MultiBits as our skip docs only to have it broken\n    // apart when we step through the docs enums in\n    // MultiDocsEnum.\n    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n    if (!perDocSlices.isEmpty()) {\n      mergeState.multiDeletedDocs = new MultiBits(perDocBits, perDocBitsStarts);\n      final PerDocConsumer docsConsumer = codec\n          .docsConsumer(new PerDocWriteState(segmentWriteState));\n      try {\n        final MultiPerDocValues multiPerDocValues = new MultiPerDocValues(perDocProducers\n            .toArray(PerDocValues.EMPTY_ARRAY), perDocSlices\n            .toArray(ReaderUtil.Slice.EMPTY_ARRAY));\n        docsConsumer.merge(mergeState, multiPerDocValues);\n      } finally {\n        docsConsumer.close();\n      }\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e8d7ba2175f47e280231533f7d3016249cea88b","date":1307711934,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n    \n    // TODO: move this into its own method - this merges currently only docvalues\n    final List<PerDocValues> perDocProducers = new ArrayList<PerDocValues>();    \n    final List<ReaderUtil.Slice> perDocSlices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> perDocBits = new ArrayList<Bits>();\n    final List<Integer> perDocBitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      final PerDocValues producer = r.perDocValues();\n      if (producer != null) {\n        perDocSlices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        perDocProducers.add(producer);\n        perDocBits.add(r.getDeletedDocs());\n        perDocBitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n    perDocBitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      // NOTE: this is silly, yet, necessary -- we create a\n      // MultiBits as our skip docs only to have it broken\n      // apart when we step through the docs enums in\n      // MultiDocsEnum.\n      mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n      \n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n    if (!perDocSlices.isEmpty()) {\n      mergeState.multiDeletedDocs = new MultiBits(perDocBits, perDocBitsStarts);\n      final PerDocConsumer docsConsumer = codec\n          .docsConsumer(new PerDocWriteState(segmentWriteState));\n      try {\n        final MultiPerDocValues multiPerDocValues = new MultiPerDocValues(perDocProducers\n            .toArray(PerDocValues.EMPTY_ARRAY), perDocSlices\n            .toArray(ReaderUtil.Slice.EMPTY_ARRAY));\n        docsConsumer.merge(mergeState, multiPerDocValues);\n      } finally {\n        docsConsumer.close();\n      }\n    }\n    \n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      // NOTE: this is silly, yet, necessary -- we create a\n      // MultiBits as our skip docs only to have it broken\n      // apart when we step through the docs enums in\n      // MultiDocsEnum.\n      mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n      \n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","date":1307729864,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n    \n    // TODO: move this into its own method - this merges currently only docvalues\n    final List<PerDocValues> perDocProducers = new ArrayList<PerDocValues>();    \n    final List<ReaderUtil.Slice> perDocSlices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> perDocBits = new ArrayList<Bits>();\n    final List<Integer> perDocBitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      final PerDocValues producer = r.perDocValues();\n      if (producer != null) {\n        perDocSlices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        perDocProducers.add(producer);\n        perDocBits.add(r.getDeletedDocs());\n        perDocBitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n    perDocBitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      // NOTE: this is silly, yet, necessary -- we create a\n      // MultiBits as our skip docs only to have it broken\n      // apart when we step through the docs enums in\n      // MultiDocsEnum.\n      mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n      \n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n    if (!perDocSlices.isEmpty()) {\n      mergeState.multiDeletedDocs = new MultiBits(perDocBits, perDocBitsStarts);\n      final PerDocConsumer docsConsumer = codec\n          .docsConsumer(new PerDocWriteState(segmentWriteState));\n      try {\n        final MultiPerDocValues multiPerDocValues = new MultiPerDocValues(perDocProducers\n            .toArray(PerDocValues.EMPTY_ARRAY), perDocSlices\n            .toArray(ReaderUtil.Slice.EMPTY_ARRAY));\n        docsConsumer.merge(mergeState, multiPerDocValues);\n      } finally {\n        docsConsumer.close();\n      }\n    }\n    \n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      // NOTE: this is silly, yet, necessary -- we create a\n      // MultiBits as our skip docs only to have it broken\n      // apart when we step through the docs enums in\n      // MultiDocsEnum.\n      mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n      \n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","date":1309960478,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n    \n    // TODO: move this into its own method - this merges currently only docvalues\n    final List<PerDocValues> perDocProducers = new ArrayList<PerDocValues>();    \n    final List<ReaderUtil.Slice> perDocSlices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> perDocBits = new ArrayList<Bits>();\n    final List<Integer> perDocBitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getLiveDocs());\n        bitsStarts.add(docBase);\n      }\n      final PerDocValues producer = r.perDocValues();\n      if (producer != null) {\n        perDocSlices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        perDocProducers.add(producer);\n        perDocBits.add(r.getLiveDocs());\n        perDocBitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n    perDocBitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits liveDocs = reader.getLiveDocs();\n        assert liveDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      // NOTE: this is silly, yet, necessary -- we create a\n      // MultiBits as our skip docs only to have it broken\n      // apart when we step through the docs enums in\n      // MultiDocsEnum.\n      mergeState.multiLiveDocs = new MultiBits(bits, bitsStarts, true);\n      \n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n    if (!perDocSlices.isEmpty()) {\n      mergeState.multiLiveDocs = new MultiBits(perDocBits, perDocBitsStarts, true);\n      final PerDocConsumer docsConsumer = codec\n          .docsConsumer(new PerDocWriteState(segmentWriteState));\n      try {\n        final MultiPerDocValues multiPerDocValues = new MultiPerDocValues(perDocProducers\n            .toArray(PerDocValues.EMPTY_ARRAY), perDocSlices\n            .toArray(ReaderUtil.Slice.EMPTY_ARRAY));\n        docsConsumer.merge(mergeState, multiPerDocValues);\n      } finally {\n        docsConsumer.close();\n      }\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n    \n    // TODO: move this into its own method - this merges currently only docvalues\n    final List<PerDocValues> perDocProducers = new ArrayList<PerDocValues>();    \n    final List<ReaderUtil.Slice> perDocSlices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> perDocBits = new ArrayList<Bits>();\n    final List<Integer> perDocBitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      final PerDocValues producer = r.perDocValues();\n      if (producer != null) {\n        perDocSlices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        perDocProducers.add(producer);\n        perDocBits.add(r.getDeletedDocs());\n        perDocBitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n    perDocBitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      // NOTE: this is silly, yet, necessary -- we create a\n      // MultiBits as our skip docs only to have it broken\n      // apart when we step through the docs enums in\n      // MultiDocsEnum.\n      mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n      \n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n    if (!perDocSlices.isEmpty()) {\n      mergeState.multiDeletedDocs = new MultiBits(perDocBits, perDocBitsStarts);\n      final PerDocConsumer docsConsumer = codec\n          .docsConsumer(new PerDocWriteState(segmentWriteState));\n      try {\n        final MultiPerDocValues multiPerDocValues = new MultiPerDocValues(perDocProducers\n            .toArray(PerDocValues.EMPTY_ARRAY), perDocSlices\n            .toArray(ReaderUtil.Slice.EMPTY_ARRAY));\n        docsConsumer.merge(mergeState, multiPerDocValues);\n      } finally {\n        docsConsumer.close();\n      }\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n    \n    // TODO: move this into its own method - this merges currently only docvalues\n    final List<PerDocValues> perDocProducers = new ArrayList<PerDocValues>();    \n    final List<ReaderUtil.Slice> perDocSlices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> perDocBits = new ArrayList<Bits>();\n    final List<Integer> perDocBitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getLiveDocs());\n        bitsStarts.add(docBase);\n      }\n      final PerDocValues producer = r.perDocValues();\n      if (producer != null) {\n        perDocSlices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        perDocProducers.add(producer);\n        perDocBits.add(r.getLiveDocs());\n        perDocBitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n    perDocBitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits liveDocs = reader.getLiveDocs();\n        assert liveDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      // NOTE: this is silly, yet, necessary -- we create a\n      // MultiBits as our skip docs only to have it broken\n      // apart when we step through the docs enums in\n      // MultiDocsEnum.\n      mergeState.multiLiveDocs = new MultiBits(bits, bitsStarts, true);\n      \n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n    if (!perDocSlices.isEmpty()) {\n      mergeState.multiLiveDocs = new MultiBits(perDocBits, perDocBitsStarts, true);\n      final PerDocConsumer docsConsumer = codec\n          .docsConsumer(new PerDocWriteState(segmentWriteState));\n      try {\n        final MultiPerDocValues multiPerDocValues = new MultiPerDocValues(perDocProducers\n            .toArray(PerDocValues.EMPTY_ARRAY), perDocSlices\n            .toArray(ReaderUtil.Slice.EMPTY_ARRAY));\n        docsConsumer.merge(mergeState, multiPerDocValues);\n      } finally {\n        docsConsumer.close();\n      }\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n    \n    // TODO: move this into its own method - this merges currently only docvalues\n    final List<PerDocValues> perDocProducers = new ArrayList<PerDocValues>();    \n    final List<ReaderUtil.Slice> perDocSlices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> perDocBits = new ArrayList<Bits>();\n    final List<Integer> perDocBitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      final PerDocValues producer = r.perDocValues();\n      if (producer != null) {\n        perDocSlices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        perDocProducers.add(producer);\n        perDocBits.add(r.getDeletedDocs());\n        perDocBitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n    perDocBitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      // NOTE: this is silly, yet, necessary -- we create a\n      // MultiBits as our skip docs only to have it broken\n      // apart when we step through the docs enums in\n      // MultiDocsEnum.\n      mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n      \n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n    if (!perDocSlices.isEmpty()) {\n      mergeState.multiDeletedDocs = new MultiBits(perDocBits, perDocBitsStarts);\n      final PerDocConsumer docsConsumer = codec\n          .docsConsumer(new PerDocWriteState(segmentWriteState));\n      try {\n        final MultiPerDocValues multiPerDocValues = new MultiPerDocValues(perDocProducers\n            .toArray(PerDocValues.EMPTY_ARRAY), perDocSlices\n            .toArray(ReaderUtil.Slice.EMPTY_ARRAY));\n        docsConsumer.merge(mergeState, multiPerDocValues);\n      } finally {\n        docsConsumer.close();\n      }\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n    \n    // TODO: move this into its own method - this merges currently only docvalues\n    final List<PerDocValues> perDocProducers = new ArrayList<PerDocValues>();    \n    final List<ReaderUtil.Slice> perDocSlices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> perDocBits = new ArrayList<Bits>();\n    final List<Integer> perDocBitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getLiveDocs());\n        bitsStarts.add(docBase);\n      }\n      final PerDocValues producer = r.perDocValues();\n      if (producer != null) {\n        perDocSlices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        perDocProducers.add(producer);\n        perDocBits.add(r.getLiveDocs());\n        perDocBitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n    perDocBitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits liveDocs = reader.getLiveDocs();\n        assert liveDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      // NOTE: this is silly, yet, necessary -- we create a\n      // MultiBits as our skip docs only to have it broken\n      // apart when we step through the docs enums in\n      // MultiDocsEnum.\n      mergeState.multiLiveDocs = new MultiBits(bits, bitsStarts, true);\n      \n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n    if (!perDocSlices.isEmpty()) {\n      mergeState.multiLiveDocs = new MultiBits(perDocBits, perDocBitsStarts, true);\n      final PerDocConsumer docsConsumer = codec\n          .docsConsumer(new PerDocWriteState(segmentWriteState));\n      try {\n        final MultiPerDocValues multiPerDocValues = new MultiPerDocValues(perDocProducers\n            .toArray(PerDocValues.EMPTY_ARRAY), perDocSlices\n            .toArray(ReaderUtil.Slice.EMPTY_ARRAY));\n        docsConsumer.merge(mergeState, multiPerDocValues);\n      } finally {\n        docsConsumer.close();\n      }\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n    \n    // TODO: move this into its own method - this merges currently only docvalues\n    final List<PerDocValues> perDocProducers = new ArrayList<PerDocValues>();    \n    final List<ReaderUtil.Slice> perDocSlices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> perDocBits = new ArrayList<Bits>();\n    final List<Integer> perDocBitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getDeletedDocs());\n        bitsStarts.add(docBase);\n      }\n      final PerDocValues producer = r.perDocValues();\n      if (producer != null) {\n        perDocSlices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        perDocProducers.add(producer);\n        perDocBits.add(r.getDeletedDocs());\n        perDocBitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n    perDocBitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits delDocs = reader.getDeletedDocs();\n        assert delDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (delDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      // NOTE: this is silly, yet, necessary -- we create a\n      // MultiBits as our skip docs only to have it broken\n      // apart when we step through the docs enums in\n      // MultiDocsEnum.\n      mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);\n      \n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n    if (!perDocSlices.isEmpty()) {\n      mergeState.multiDeletedDocs = new MultiBits(perDocBits, perDocBitsStarts);\n      final PerDocConsumer docsConsumer = codec\n          .docsConsumer(new PerDocWriteState(segmentWriteState));\n      try {\n        final MultiPerDocValues multiPerDocValues = new MultiPerDocValues(perDocProducers\n            .toArray(PerDocValues.EMPTY_ARRAY), perDocSlices\n            .toArray(ReaderUtil.Slice.EMPTY_ARRAY));\n        docsConsumer.merge(mergeState, multiPerDocValues);\n      } finally {\n        docsConsumer.close();\n      }\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a07ca455ec3f405a6078602f3f3dcf2d4fa8679","date":1310042027,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n    \n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getLiveDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits liveDocs = reader.getLiveDocs();\n        assert liveDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      // NOTE: this is silly, yet, necessary -- we create a\n      // MultiBits as our skip docs only to have it broken\n      // apart when we step through the docs enums in\n      // MultiDocsEnum.\n      mergeState.multiLiveDocs = new MultiBits(bits, bitsStarts, true);\n      \n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n    \n    // TODO: move this into its own method - this merges currently only docvalues\n    final List<PerDocValues> perDocProducers = new ArrayList<PerDocValues>();    \n    final List<ReaderUtil.Slice> perDocSlices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> perDocBits = new ArrayList<Bits>();\n    final List<Integer> perDocBitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getLiveDocs());\n        bitsStarts.add(docBase);\n      }\n      final PerDocValues producer = r.perDocValues();\n      if (producer != null) {\n        perDocSlices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        perDocProducers.add(producer);\n        perDocBits.add(r.getLiveDocs());\n        perDocBitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n    perDocBitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits liveDocs = reader.getLiveDocs();\n        assert liveDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      // NOTE: this is silly, yet, necessary -- we create a\n      // MultiBits as our skip docs only to have it broken\n      // apart when we step through the docs enums in\n      // MultiDocsEnum.\n      mergeState.multiLiveDocs = new MultiBits(bits, bitsStarts, true);\n      \n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n    if (!perDocSlices.isEmpty()) {\n      mergeState.multiLiveDocs = new MultiBits(perDocBits, perDocBitsStarts, true);\n      final PerDocConsumer docsConsumer = codec\n          .docsConsumer(new PerDocWriteState(segmentWriteState));\n      try {\n        final MultiPerDocValues multiPerDocValues = new MultiPerDocValues(perDocProducers\n            .toArray(PerDocValues.EMPTY_ARRAY), perDocSlices\n            .toArray(ReaderUtil.Slice.EMPTY_ARRAY));\n        docsConsumer.merge(mergeState, multiPerDocValues);\n      } finally {\n        docsConsumer.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f0b9507caf22f292ac0e5e59f62db4275adf4511","date":1310107283,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n    \n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getLiveDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits liveDocs = reader.getLiveDocs();\n        assert liveDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      // NOTE: this is silly, yet, necessary -- we create a\n      // MultiBits as our skip docs only to have it broken\n      // apart when we step through the docs enums in\n      // MultiDocsEnum.\n      mergeState.multiLiveDocs = new MultiBits(bits, bitsStarts, true);\n      \n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n    \n    // TODO: move this into its own method - this merges currently only docvalues\n    final List<PerDocValues> perDocProducers = new ArrayList<PerDocValues>();    \n    final List<ReaderUtil.Slice> perDocSlices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> perDocBits = new ArrayList<Bits>();\n    final List<Integer> perDocBitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getLiveDocs());\n        bitsStarts.add(docBase);\n      }\n      final PerDocValues producer = r.perDocValues();\n      if (producer != null) {\n        perDocSlices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        perDocProducers.add(producer);\n        perDocBits.add(r.getLiveDocs());\n        perDocBitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n    perDocBitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits liveDocs = reader.getLiveDocs();\n        assert liveDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      // NOTE: this is silly, yet, necessary -- we create a\n      // MultiBits as our skip docs only to have it broken\n      // apart when we step through the docs enums in\n      // MultiDocsEnum.\n      mergeState.multiLiveDocs = new MultiBits(bits, bitsStarts, true);\n      \n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n    if (!perDocSlices.isEmpty()) {\n      mergeState.multiLiveDocs = new MultiBits(perDocBits, perDocBitsStarts, true);\n      final PerDocConsumer docsConsumer = codec\n          .docsConsumer(new PerDocWriteState(segmentWriteState));\n      try {\n        final MultiPerDocValues multiPerDocValues = new MultiPerDocValues(perDocProducers\n            .toArray(PerDocValues.EMPTY_ARRAY), perDocSlices\n            .toArray(ReaderUtil.Slice.EMPTY_ARRAY));\n        docsConsumer.merge(mergeState, multiPerDocValues);\n      } finally {\n        docsConsumer.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1291e4568eb7d9463d751627596ef14baf4c1603","date":1310112572,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n    \n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getLiveDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits liveDocs = reader.getLiveDocs();\n        assert liveDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      // NOTE: this is silly, yet, necessary -- we create a\n      // MultiBits as our skip docs only to have it broken\n      // apart when we step through the docs enums in\n      // MultiDocsEnum.\n      mergeState.multiLiveDocs = new MultiBits(bits, bitsStarts, true);\n      \n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n\n    final List<Fields> fields = new ArrayList<Fields>();\n\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n    \n    // TODO: move this into its own method - this merges currently only docvalues\n    final List<PerDocValues> perDocProducers = new ArrayList<PerDocValues>();    \n    final List<ReaderUtil.Slice> perDocSlices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> perDocBits = new ArrayList<Bits>();\n    final List<Integer> perDocBitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getLiveDocs());\n        bitsStarts.add(docBase);\n      }\n      final PerDocValues producer = r.perDocValues();\n      if (producer != null) {\n        perDocSlices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        perDocProducers.add(producer);\n        perDocBits.add(r.getLiveDocs());\n        perDocBitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n    perDocBitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits liveDocs = reader.getLiveDocs();\n        assert liveDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      // NOTE: this is silly, yet, necessary -- we create a\n      // MultiBits as our skip docs only to have it broken\n      // apart when we step through the docs enums in\n      // MultiDocsEnum.\n      mergeState.multiLiveDocs = new MultiBits(bits, bitsStarts, true);\n      \n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n    if (!perDocSlices.isEmpty()) {\n      mergeState.multiLiveDocs = new MultiBits(perDocBits, perDocBitsStarts, true);\n      final PerDocConsumer docsConsumer = codec\n          .docsConsumer(new PerDocWriteState(segmentWriteState));\n      try {\n        final MultiPerDocValues multiPerDocValues = new MultiPerDocValues(perDocProducers\n            .toArray(PerDocValues.EMPTY_ARRAY), perDocSlices\n            .toArray(ReaderUtil.Slice.EMPTY_ARRAY));\n        docsConsumer.merge(mergeState, multiPerDocValues);\n      } finally {\n        docsConsumer.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d53e0e0de54796610619b32f911e89d9fb752c4c","date":1310918942,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n    \n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getLiveDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (reader.hasDeletions()) {\n        int delCount = 0;\n        final Bits liveDocs = reader.getLiveDocs();\n        assert liveDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == reader.numDeletedDocs(): \"reader delCount=\" + reader.numDeletedDocs() + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n    \n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getLiveDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.delCounts = new int[mergeState.readerCount];\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.delCounts[i] = reader.numDeletedDocs();\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (mergeState.delCounts[i] != 0) {\n        int delCount = 0;\n        final Bits liveDocs = reader.getLiveDocs();\n        assert liveDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == mergeState.delCounts[i]: \"reader delCount=\" + mergeState.delCounts[i] + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      // NOTE: this is silly, yet, necessary -- we create a\n      // MultiBits as our skip docs only to have it broken\n      // apart when we step through the docs enums in\n      // MultiDocsEnum.\n      mergeState.multiLiveDocs = new MultiBits(bits, bitsStarts, true);\n      \n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0061262413ecc163d6eebba1b5c43ab91a0c2dc5","date":1311195279,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n    \n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n\n    for(MergeState.IndexReaderAndLiveDocs r : readers) {\n      final Fields f = r.reader.fields();\n      final int maxDoc = r.reader.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n      }\n      docBase += maxDoc;\n    }\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      inputDocBase += reader.reader.maxDoc();\n      final int maxDoc = reader.reader.maxDoc();\n      if (reader.liveDocs != null) {\n        int delCount = 0;\n        final Bits liveDocs = reader.liveDocs;\n        assert liveDocs != null;\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docBase += maxDoc - delCount;\n      } else {\n        docBase += maxDoc;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n    \n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n    final List<Bits> bits = new ArrayList<Bits>();\n    final List<Integer> bitsStarts = new ArrayList<Integer>();\n\n    for(IndexReader r : readers) {\n      final Fields f = r.fields();\n      final int maxDoc = r.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n        bits.add(r.getLiveDocs());\n        bitsStarts.add(docBase);\n      }\n      docBase += maxDoc;\n    }\n\n    bitsStarts.add(docBase);\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final IndexReader reader = readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      docBase += reader.numDocs();\n      inputDocBase += reader.maxDoc();\n      if (reader.hasDeletions()) {\n        int delCount = 0;\n        final Bits liveDocs = reader.getLiveDocs();\n        assert liveDocs != null;\n        final int maxDoc = reader.maxDoc();\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;  // only for assert\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        assert delCount == reader.numDeletedDocs(): \"reader delCount=\" + reader.numDeletedDocs() + \" vs recomputed delCount=\" + delCount;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"60ba444201d2570214b6fcf1d15600dc1a01f548","date":1313868045,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n    \n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n\n    for(MergeState.IndexReaderAndLiveDocs r : readers) {\n      final Fields f = r.reader.fields();\n      final int maxDoc = r.reader.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n      }\n      docBase += maxDoc;\n    }\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      inputDocBase += reader.reader.maxDoc();\n      final int maxDoc = reader.reader.maxDoc();\n      if (reader.liveDocs != null) {\n        int delCount = 0;\n        final Bits liveDocs = reader.liveDocs;\n        assert liveDocs != null;\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docBase += maxDoc - delCount;\n      } else {\n        docBase += maxDoc;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    boolean success = false;\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, consumer);\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n    \n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n\n    for(MergeState.IndexReaderAndLiveDocs r : readers) {\n      final Fields f = r.reader.fields();\n      final int maxDoc = r.reader.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n      }\n      docBase += maxDoc;\n    }\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      inputDocBase += reader.reader.maxDoc();\n      final int maxDoc = reader.reader.maxDoc();\n      if (reader.liveDocs != null) {\n        int delCount = 0;\n        final Bits liveDocs = reader.liveDocs;\n        assert liveDocs != null;\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docBase += maxDoc - delCount;\n      } else {\n        docBase += maxDoc;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n    } finally {\n      consumer.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"24230fe54121f9be9d85f2c2067536296785e421","date":1314462346,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n    \n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n\n    for(MergeState.IndexReaderAndLiveDocs r : readers) {\n      final Fields f = r.reader.fields();\n      final int maxDoc = r.reader.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n      }\n      docBase += maxDoc;\n    }\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      inputDocBase += reader.reader.maxDoc();\n      final int maxDoc = reader.reader.maxDoc();\n      if (reader.liveDocs != null) {\n        int delCount = 0;\n        final Bits liveDocs = reader.liveDocs;\n        assert liveDocs != null;\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docBase += maxDoc - delCount;\n      } else {\n        docBase += maxDoc;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    boolean success = false;\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n    \n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n\n    for(MergeState.IndexReaderAndLiveDocs r : readers) {\n      final Fields f = r.reader.fields();\n      final int maxDoc = r.reader.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n      }\n      docBase += maxDoc;\n    }\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      inputDocBase += reader.reader.maxDoc();\n      final int maxDoc = reader.reader.maxDoc();\n      if (reader.liveDocs != null) {\n        int delCount = 0;\n        final Bits liveDocs = reader.liveDocs;\n        assert liveDocs != null;\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docBase += maxDoc - delCount;\n      } else {\n        docBase += maxDoc;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    boolean success = false;\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, consumer);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n    \n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n\n    for(MergeState.IndexReaderAndLiveDocs r : readers) {\n      final Fields f = r.reader.fields();\n      final int maxDoc = r.reader.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n      }\n      docBase += maxDoc;\n    }\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      inputDocBase += reader.reader.maxDoc();\n      final int maxDoc = reader.reader.maxDoc();\n      if (reader.liveDocs != null) {\n        int delCount = 0;\n        final Bits liveDocs = reader.liveDocs;\n        assert liveDocs != null;\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docBase += maxDoc - delCount;\n      } else {\n        docBase += maxDoc;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.reader.directory());\n      }\n    }\n\n    final FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(segmentWriteState);\n    boolean success = false;\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n    \n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n\n    for(MergeState.IndexReaderAndLiveDocs r : readers) {\n      final Fields f = r.reader.fields();\n      final int maxDoc = r.reader.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n      }\n      docBase += maxDoc;\n    }\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      inputDocBase += reader.reader.maxDoc();\n      final int maxDoc = reader.reader.maxDoc();\n      if (reader.liveDocs != null) {\n        int delCount = 0;\n        final Bits liveDocs = reader.liveDocs;\n        assert liveDocs != null;\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docBase += maxDoc - delCount;\n      } else {\n        docBase += maxDoc;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.reader.directory());\n      }\n    }\n    codec = segmentWriteState.segmentCodecs.codec();\n    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);\n    boolean success = false;\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms(SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeTerms().mjava","sourceNew":"  private final void mergeTerms(SegmentWriteState segmentWriteState) throws CorruptIndexException, IOException {\n    int docBase = 0;\n    \n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n\n    for(MergeState.IndexReaderAndLiveDocs r : mergeState.readers) {\n      final Fields f = r.reader.fields();\n      final int maxDoc = r.reader.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n      }\n      docBase += maxDoc;\n    }\n\n    final int numReaders = mergeState.readers.size();\n\n    docBase = 0;\n\n    for(int i=0;i<numReaders;i++) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      final int maxDoc = reader.reader.maxDoc();\n      if (reader.liveDocs != null) {\n        int delCount = 0;\n        final Bits liveDocs = reader.liveDocs;\n        assert liveDocs != null;\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docBase += maxDoc - delCount;\n      } else {\n        docBase += maxDoc;\n      }\n\n      if (mergeState.payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = mergeState.payloadProcessorProvider.getDirProcessor(reader.reader.directory());\n      }\n    }\n\n    final FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(segmentWriteState);\n    boolean success = false;\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","sourceOld":"  private final void mergeTerms() throws CorruptIndexException, IOException {\n\n    // Let CodecProvider decide which codec will be used to write\n    // the new segment:\n\n    int docBase = 0;\n    \n    final List<Fields> fields = new ArrayList<Fields>();\n    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();\n\n    for(MergeState.IndexReaderAndLiveDocs r : readers) {\n      final Fields f = r.reader.fields();\n      final int maxDoc = r.reader.maxDoc();\n      if (f != null) {\n        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));\n        fields.add(f);\n      }\n      docBase += maxDoc;\n    }\n\n    // we may gather more readers than mergeState.readerCount\n    mergeState = new MergeState();\n    mergeState.readers = readers;\n    mergeState.readerCount = readers.size();\n    mergeState.fieldInfos = fieldInfos;\n    mergeState.mergedDocCount = mergedDocs;\n\n    // Remap docIDs\n    mergeState.docMaps = new int[mergeState.readerCount][];\n    mergeState.docBase = new int[mergeState.readerCount];\n    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];\n    mergeState.checkAbort = checkAbort;\n\n    docBase = 0;\n    int inputDocBase = 0;\n\n    for(int i=0;i<mergeState.readerCount;i++) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      inputDocBase += reader.reader.maxDoc();\n      final int maxDoc = reader.reader.maxDoc();\n      if (reader.liveDocs != null) {\n        int delCount = 0;\n        final Bits liveDocs = reader.liveDocs;\n        assert liveDocs != null;\n        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docBase += maxDoc - delCount;\n      } else {\n        docBase += maxDoc;\n      }\n\n      if (payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.reader.directory());\n      }\n    }\n\n    final FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(segmentWriteState);\n    boolean success = false;\n    try {\n      consumer.merge(mergeState,\n                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),\n                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["c95a819869502635864dac0a788f874787e3395b","4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"24230fe54121f9be9d85f2c2067536296785e421":["60ba444201d2570214b6fcf1d15600dc1a01f548"],"b43b719dab44d1ccc5ee5b6e01c50f1ee86bb76c":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"0061262413ecc163d6eebba1b5c43ab91a0c2dc5":["d53e0e0de54796610619b32f911e89d9fb752c4c"],"efb7a19703a037c29e30440260d393500febc1f4":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["2e8d7ba2175f47e280231533f7d3016249cea88b"],"a493e6d0c3ad86bd55c0a1360d110142e948f2bd":["406e7055a3e99d3fa6ce49a555a51dd18b321806"],"e623f9a0e45508ab149c2fb3e0fd0c2503f98186":["7f367dfb9086b92a13c77e2d31112c715cd4502c"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["433777d1eaf9998136cd16515dc0e1eb26f5d535","7f367dfb9086b92a13c77e2d31112c715cd4502c"],"bb9b72f7c3d7827c64dd4ec580ded81778da361d":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","e623f9a0e45508ab149c2fb3e0fd0c2503f98186"],"2e8d7ba2175f47e280231533f7d3016249cea88b":["efb7a19703a037c29e30440260d393500febc1f4","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"f0b9507caf22f292ac0e5e59f62db4275adf4511":["817d8435e9135b756f08ce6710ab0baac51bdf88","5a07ca455ec3f405a6078602f3f3dcf2d4fa8679"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["a3776dccca01c11e7046323cfad46a3b4a471233","efb7a19703a037c29e30440260d393500febc1f4"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["3bb13258feba31ab676502787ab2e1779f129b7a","e623f9a0e45508ab149c2fb3e0fd0c2503f98186"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"433777d1eaf9998136cd16515dc0e1eb26f5d535":["b43b719dab44d1ccc5ee5b6e01c50f1ee86bb76c"],"406e7055a3e99d3fa6ce49a555a51dd18b321806":["433777d1eaf9998136cd16515dc0e1eb26f5d535"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"2e10cb22a8bdb44339e282925a29182bb2f3174d":["333709f2f4e93855e69ab88ff9581c580c393891","efb7a19703a037c29e30440260d393500febc1f4"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","2e8d7ba2175f47e280231533f7d3016249cea88b"],"d53e0e0de54796610619b32f911e89d9fb752c4c":["5a07ca455ec3f405a6078602f3f3dcf2d4fa8679"],"06584e6e98d592b34e1329b384182f368d2025e8":["7b91922b55d15444d554721b352861d028eb8278"],"85a883878c0af761245ab048babc63d099f835f3":["406e7055a3e99d3fa6ce49a555a51dd18b321806","a493e6d0c3ad86bd55c0a1360d110142e948f2bd"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["29ef99d61cda9641b6250bf9567329a6e65f901d","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["e623f9a0e45508ab149c2fb3e0fd0c2503f98186","bb9b72f7c3d7827c64dd4ec580ded81778da361d"],"7f367dfb9086b92a13c77e2d31112c715cd4502c":["a493e6d0c3ad86bd55c0a1360d110142e948f2bd"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["2e8d7ba2175f47e280231533f7d3016249cea88b","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"60ba444201d2570214b6fcf1d15600dc1a01f548":["0061262413ecc163d6eebba1b5c43ab91a0c2dc5"],"333709f2f4e93855e69ab88ff9581c580c393891":["763d13ecba7c2e244aa7c7690a878daae26227f6"],"7b91922b55d15444d554721b352861d028eb8278":["24230fe54121f9be9d85f2c2067536296785e421"],"763d13ecba7c2e244aa7c7690a878daae26227f6":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"1291e4568eb7d9463d751627596ef14baf4c1603":["d083e83f225b11e5fdd900e83d26ddb385b6955c","5a07ca455ec3f405a6078602f3f3dcf2d4fa8679"],"a3776dccca01c11e7046323cfad46a3b4a471233":["e623f9a0e45508ab149c2fb3e0fd0c2503f98186","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"5a07ca455ec3f405a6078602f3f3dcf2d4fa8679":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"3bb13258feba31ab676502787ab2e1779f129b7a":["85a883878c0af761245ab048babc63d099f835f3","7f367dfb9086b92a13c77e2d31112c715cd4502c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["06584e6e98d592b34e1329b384182f368d2025e8"]},"commit2Childs":{"24230fe54121f9be9d85f2c2067536296785e421":["7b91922b55d15444d554721b352861d028eb8278"],"b43b719dab44d1ccc5ee5b6e01c50f1ee86bb76c":["433777d1eaf9998136cd16515dc0e1eb26f5d535"],"0061262413ecc163d6eebba1b5c43ab91a0c2dc5":["60ba444201d2570214b6fcf1d15600dc1a01f548"],"efb7a19703a037c29e30440260d393500febc1f4":["2e8d7ba2175f47e280231533f7d3016249cea88b","5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","5a07ca455ec3f405a6078602f3f3dcf2d4fa8679"],"a493e6d0c3ad86bd55c0a1360d110142e948f2bd":["85a883878c0af761245ab048babc63d099f835f3","7f367dfb9086b92a13c77e2d31112c715cd4502c"],"e623f9a0e45508ab149c2fb3e0fd0c2503f98186":["bb9b72f7c3d7827c64dd4ec580ded81778da361d","29ef99d61cda9641b6250bf9567329a6e65f901d","b3e06be49006ecac364d39d12b9c9f74882f9b9f","a3776dccca01c11e7046323cfad46a3b4a471233"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["b43b719dab44d1ccc5ee5b6e01c50f1ee86bb76c"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["bb9b72f7c3d7827c64dd4ec580ded81778da361d"],"bb9b72f7c3d7827c64dd4ec580ded81778da361d":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"2e8d7ba2175f47e280231533f7d3016249cea88b":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","d083e83f225b11e5fdd900e83d26ddb385b6955c"],"f0b9507caf22f292ac0e5e59f62db4275adf4511":[],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"433777d1eaf9998136cd16515dc0e1eb26f5d535":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","406e7055a3e99d3fa6ce49a555a51dd18b321806"],"406e7055a3e99d3fa6ce49a555a51dd18b321806":["a493e6d0c3ad86bd55c0a1360d110142e948f2bd","85a883878c0af761245ab048babc63d099f835f3"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"2e10cb22a8bdb44339e282925a29182bb2f3174d":["2e8d7ba2175f47e280231533f7d3016249cea88b"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["817d8435e9135b756f08ce6710ab0baac51bdf88"],"d53e0e0de54796610619b32f911e89d9fb752c4c":["0061262413ecc163d6eebba1b5c43ab91a0c2dc5"],"06584e6e98d592b34e1329b384182f368d2025e8":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"85a883878c0af761245ab048babc63d099f835f3":["3bb13258feba31ab676502787ab2e1779f129b7a"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["763d13ecba7c2e244aa7c7690a878daae26227f6"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["efb7a19703a037c29e30440260d393500febc1f4","135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233"],"7f367dfb9086b92a13c77e2d31112c715cd4502c":["e623f9a0e45508ab149c2fb3e0fd0c2503f98186","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","3bb13258feba31ab676502787ab2e1779f129b7a"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["1291e4568eb7d9463d751627596ef14baf4c1603"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["f0b9507caf22f292ac0e5e59f62db4275adf4511"],"60ba444201d2570214b6fcf1d15600dc1a01f548":["24230fe54121f9be9d85f2c2067536296785e421"],"333709f2f4e93855e69ab88ff9581c580c393891":["2e10cb22a8bdb44339e282925a29182bb2f3174d"],"7b91922b55d15444d554721b352861d028eb8278":["06584e6e98d592b34e1329b384182f368d2025e8"],"763d13ecba7c2e244aa7c7690a878daae26227f6":["333709f2f4e93855e69ab88ff9581c580c393891"],"1291e4568eb7d9463d751627596ef14baf4c1603":[],"a3776dccca01c11e7046323cfad46a3b4a471233":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91"],"5a07ca455ec3f405a6078602f3f3dcf2d4fa8679":["f0b9507caf22f292ac0e5e59f62db4275adf4511","d53e0e0de54796610619b32f911e89d9fb752c4c","1291e4568eb7d9463d751627596ef14baf4c1603"],"3bb13258feba31ab676502787ab2e1779f129b7a":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["f0b9507caf22f292ac0e5e59f62db4275adf4511","1291e4568eb7d9463d751627596ef14baf4c1603","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}