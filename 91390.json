{"path":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,StatsField,String[]).mjava","commits":[{"id":"283ff02f401ec3e7a2fad73643970f052383fb0c","date":1411407953,"type":1,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,StatsField,String[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,boolean,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param statsField the {@link StatsField} param corrisponding to a real {@link SchemaField} to compute stats over\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, StatsField statsField, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    assert null != statsField.getSchemaField()\n      : \"DocValuesStats requires a StatsField using a SchemaField\";\n\n    SchemaField sf = statsField.getSchemaField();\n    // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(statsField);\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    SortedDocValues si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      finfo[i] = new FieldFacetStats(searcher, facet_sf, statsField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param calcDistinct whether distinct values should be collected and counted\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, boolean calcDistinct, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    SchemaField sf = searcher.getSchema().getField(field);\n   // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(sf, calcDistinct);\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    SortedDocValues si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      finfo[i] = new FieldFacetStats(searcher, f, sf, facet_sf, calcDistinct);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,StatsField,String[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,StatsField,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param statsField the {@link StatsField} param corrisponding to a real {@link SchemaField} to compute stats over\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, StatsField statsField, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    assert null != statsField.getSchemaField()\n      : \"DocValuesStats requires a StatsField using a SchemaField\";\n\n    SchemaField sf = statsField.getSchemaField();\n    // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(statsField);\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    SortedDocValues si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      finfo[i] = new FieldFacetStats(searcher, facet_sf, statsField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getLeafReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param statsField the {@link StatsField} param corrisponding to a real {@link SchemaField} to compute stats over\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, StatsField statsField, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    assert null != statsField.getSchemaField()\n      : \"DocValuesStats requires a StatsField using a SchemaField\";\n\n    SchemaField sf = statsField.getSchemaField();\n    // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(statsField);\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    SortedDocValues si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      finfo[i] = new FieldFacetStats(searcher, facet_sf, statsField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1e210ae1e604402eb4eeff2a52e56d189cd4f2f1","date":1423508552,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,StatsField,String[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,StatsField,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param statsField the {@link StatsField} param corrisponding to a real {@link SchemaField} to compute stats over\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, StatsField statsField, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    assert null != statsField.getSchemaField()\n      : \"DocValuesStats requires a StatsField using a SchemaField\";\n\n    SchemaField sf = statsField.getSchemaField();\n    // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(statsField);\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    SortedDocValues si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      finfo[i] = new FieldFacetStats(searcher, facet_sf, statsField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getLeafReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param statsField the {@link StatsField} param corrisponding to a real {@link SchemaField} to compute stats over\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, StatsField statsField, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    assert null != statsField.getSchemaField()\n      : \"DocValuesStats requires a StatsField using a SchemaField\";\n\n    SchemaField sf = statsField.getSchemaField();\n    // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(statsField);\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    SortedDocValues si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      finfo[i] = new FieldFacetStats(searcher, facet_sf, statsField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getLeafReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":["861fa37cce2d9d3f8978bbb767e87a91d41ed4a8"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d12bbc45d641864ffe03291bc30f178eb34e434c","date":1426001646,"type":4,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,StatsField,String[]).mjava","sourceNew":null,"sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param statsField the {@link StatsField} param corrisponding to a real {@link SchemaField} to compute stats over\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, StatsField statsField, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    assert null != statsField.getSchemaField()\n      : \"DocValuesStats requires a StatsField using a SchemaField\";\n\n    SchemaField sf = statsField.getSchemaField();\n    // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(statsField);\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    SortedDocValues si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      finfo[i] = new FieldFacetStats(searcher, facet_sf, statsField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getLeafReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":4,"author":"Ryan Ernst","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,StatsField,String[]).mjava","sourceNew":null,"sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param statsField the {@link StatsField} param corrisponding to a real {@link SchemaField} to compute stats over\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, StatsField statsField, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    assert null != statsField.getSchemaField()\n      : \"DocValuesStats requires a StatsField using a SchemaField\";\n\n    SchemaField sf = statsField.getSchemaField();\n    // FieldType ft = sf.getType();\n\n    StatsValues allstats = StatsValuesFactory.createStatsValues(statsField);\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    SortedDocValues si;\n    for (String f : facet) {\n      SchemaField facet_sf = searcher.getSchema().getField(f);\n      finfo[i] = new FieldFacetStats(searcher, facet_sf, statsField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getLeafReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    \n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      BytesRef value = getTermValue(te, i);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.addFacet(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"283ff02f401ec3e7a2fad73643970f052383fb0c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["1e210ae1e604402eb4eeff2a52e56d189cd4f2f1","d12bbc45d641864ffe03291bc30f178eb34e434c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d12bbc45d641864ffe03291bc30f178eb34e434c":["1e210ae1e604402eb4eeff2a52e56d189cd4f2f1"],"1e210ae1e604402eb4eeff2a52e56d189cd4f2f1":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["283ff02f401ec3e7a2fad73643970f052383fb0c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d12bbc45d641864ffe03291bc30f178eb34e434c"]},"commit2Childs":{"283ff02f401ec3e7a2fad73643970f052383fb0c":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["283ff02f401ec3e7a2fad73643970f052383fb0c"],"d12bbc45d641864ffe03291bc30f178eb34e434c":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"1e210ae1e604402eb4eeff2a52e56d189cd4f2f1":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","d12bbc45d641864ffe03291bc30f178eb34e434c"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["1e210ae1e604402eb4eeff2a52e56d189cd4f2f1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}