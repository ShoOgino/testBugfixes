{"path":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,Map[String,NumericFieldUpdates]).mjava","commits":[{"id":"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0","date":1383367127,"type":1,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,Map[String,NumericFieldUpdates]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndLiveDocs#writeFieldUpdates(Directory,Map[String,NumericFieldUpdates]).mjava","sourceNew":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, Map<String,NumericFieldUpdates> numericFieldUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert numericFieldUpdates != null && !numericFieldUpdates.isEmpty();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : numericFieldUpdates.keySet()) {\n          builder.addOrUpdate(f, NumericDocValuesField.TYPE);\n        }\n        \n        fieldInfos = builder.finish();\n        final long nextFieldInfosGen = info.getNextFieldInfosGen();\n        final String segmentSuffix = Long.toString(nextFieldInfosGen, Character.MAX_RADIX);\n        final SegmentWriteState state = new SegmentWriteState(null, trackingDir, info.info, fieldInfos, null, IOContext.DEFAULT, segmentSuffix);\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        final DocValuesConsumer fieldsConsumer = docValuesFormat.fieldsConsumer(state);\n        boolean fieldsConsumerSuccess = false;\n        try {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: applying updates; seg=\" + info + \" updates=\" + numericUpdates);\n          for (Entry<String,NumericFieldUpdates> e : numericFieldUpdates.entrySet()) {\n            final String field = e.getKey();\n            final NumericFieldUpdates fieldUpdates = e.getValue();\n            final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n            assert fieldInfo != null;\n\n            fieldInfo.setDocValuesGen(nextFieldInfosGen);\n            // write the numeric updates to a new gen'd docvalues file\n            fieldsConsumer.addNumericField(fieldInfo, new Iterable<Number>() {\n              final NumericDocValues currentValues = reader.getNumericDocValues(field);\n              final Bits docsWithField = reader.getDocsWithField(field);\n              final int maxDoc = reader.maxDoc();\n              final UpdatesIterator updatesIter = fieldUpdates.getUpdates();\n              @Override\n              public Iterator<Number> iterator() {\n                updatesIter.reset();\n                return new Iterator<Number>() {\n\n                  int curDoc = -1;\n                  int updateDoc = updatesIter.nextDoc();\n                  \n                  @Override\n                  public boolean hasNext() {\n                    return curDoc < maxDoc - 1;\n                  }\n\n                  @Override\n                  public Number next() {\n                    if (++curDoc >= maxDoc) {\n                      throw new NoSuchElementException(\"no more documents to return values for\");\n                    }\n                    if (curDoc == updateDoc) { // this document has an updated value\n                      Long value = updatesIter.value(); // either null (unset value) or updated value\n                      updateDoc = updatesIter.nextDoc(); // prepare for next round\n                      return value;\n                    } else {\n                      // no update for this document\n                      assert curDoc < updateDoc;\n                      if (currentValues != null && docsWithField.get(curDoc)) {\n                        // only read the current value if the document had a value before\n                        return currentValues.get(curDoc);\n                      } else {\n                        return null;\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void remove() {\n                    throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                  }\n                };\n              }\n            });\n          }\n          \n          codec.fieldInfosFormat().getFieldInfosWriter().write(trackingDir, info.info.name, segmentSuffix, fieldInfos, IOContext.DEFAULT);\n          fieldsConsumerSuccess = true;\n        } finally {\n          if (fieldsConsumerSuccess) {\n            fieldsConsumer.close();\n          } else {\n            IOUtils.closeWhileHandlingException(fieldsConsumer);\n          }\n        }\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteDocValuesGen so that a 2nd\n        // attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          try {\n            dir.deleteFile(fileName);\n          } catch (Throwable t) {\n            // Ignore so we throw only the first exc\n          }\n        }\n      }\n    }\n    \n    info.advanceFieldInfosGen();\n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericFieldUpdates> e : numericFieldUpdates.entrySet()) {\n        NumericFieldUpdates fieldUpdates = mergingNumericUpdates.get(e.getKey());\n        if (fieldUpdates == null) {\n          mergingNumericUpdates.put(e.getKey(), e.getValue());\n        } else {\n          fieldUpdates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // create a new map, keeping only the gens that are in use\n    Map<Long,Set<String>> genUpdatesFiles = info.getUpdatesFiles();\n    Map<Long,Set<String>> newGenUpdatesFiles = new HashMap<Long,Set<String>>();\n    final long fieldInfosGen = info.getFieldInfosGen();\n    for (FieldInfo fi : fieldInfos) {\n      long dvGen = fi.getDocValuesGen();\n      if (dvGen != -1 && !newGenUpdatesFiles.containsKey(dvGen)) {\n        if (dvGen == fieldInfosGen) {\n          newGenUpdatesFiles.put(fieldInfosGen, trackingDir.getCreatedFiles());\n        } else {\n          newGenUpdatesFiles.put(dvGen, genUpdatesFiles.get(dvGen));\n        }\n      }\n    }\n    \n    info.setGenUpdatesFiles(newGenUpdatesFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, Map<String,NumericFieldUpdates> numericFieldUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert numericFieldUpdates != null && !numericFieldUpdates.isEmpty();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : numericFieldUpdates.keySet()) {\n          builder.addOrUpdate(f, NumericDocValuesField.TYPE);\n        }\n        \n        fieldInfos = builder.finish();\n        final long nextFieldInfosGen = info.getNextFieldInfosGen();\n        final String segmentSuffix = Long.toString(nextFieldInfosGen, Character.MAX_RADIX);\n        final SegmentWriteState state = new SegmentWriteState(null, trackingDir, info.info, fieldInfos, null, IOContext.DEFAULT, segmentSuffix);\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        final DocValuesConsumer fieldsConsumer = docValuesFormat.fieldsConsumer(state);\n        boolean fieldsConsumerSuccess = false;\n        try {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: applying updates; seg=\" + info + \" updates=\" + numericUpdates);\n          for (Entry<String,NumericFieldUpdates> e : numericFieldUpdates.entrySet()) {\n            final String field = e.getKey();\n            final NumericFieldUpdates fieldUpdates = e.getValue();\n            final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n            assert fieldInfo != null;\n\n            fieldInfo.setDocValuesGen(nextFieldInfosGen);\n            // write the numeric updates to a new gen'd docvalues file\n            fieldsConsumer.addNumericField(fieldInfo, new Iterable<Number>() {\n              final NumericDocValues currentValues = reader.getNumericDocValues(field);\n              final Bits docsWithField = reader.getDocsWithField(field);\n              final int maxDoc = reader.maxDoc();\n              final UpdatesIterator updatesIter = fieldUpdates.getUpdates();\n              @Override\n              public Iterator<Number> iterator() {\n                updatesIter.reset();\n                return new Iterator<Number>() {\n\n                  int curDoc = -1;\n                  int updateDoc = updatesIter.nextDoc();\n                  \n                  @Override\n                  public boolean hasNext() {\n                    return curDoc < maxDoc - 1;\n                  }\n\n                  @Override\n                  public Number next() {\n                    if (++curDoc >= maxDoc) {\n                      throw new NoSuchElementException(\"no more documents to return values for\");\n                    }\n                    if (curDoc == updateDoc) { // this document has an updated value\n                      Long value = updatesIter.value(); // either null (unset value) or updated value\n                      updateDoc = updatesIter.nextDoc(); // prepare for next round\n                      return value;\n                    } else {\n                      // no update for this document\n                      assert curDoc < updateDoc;\n                      if (currentValues != null && docsWithField.get(curDoc)) {\n                        // only read the current value if the document had a value before\n                        return currentValues.get(curDoc);\n                      } else {\n                        return null;\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void remove() {\n                    throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                  }\n                };\n              }\n            });\n          }\n          \n          codec.fieldInfosFormat().getFieldInfosWriter().write(trackingDir, info.info.name, segmentSuffix, fieldInfos, IOContext.DEFAULT);\n          fieldsConsumerSuccess = true;\n        } finally {\n          if (fieldsConsumerSuccess) {\n            fieldsConsumer.close();\n          } else {\n            IOUtils.closeWhileHandlingException(fieldsConsumer);\n          }\n        }\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteDocValuesGen so that a 2nd\n        // attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          try {\n            dir.deleteFile(fileName);\n          } catch (Throwable t) {\n            // Ignore so we throw only the first exc\n          }\n        }\n      }\n    }\n    \n    info.advanceFieldInfosGen();\n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericFieldUpdates> e : numericFieldUpdates.entrySet()) {\n        NumericFieldUpdates fieldUpdates = mergingNumericUpdates.get(e.getKey());\n        if (fieldUpdates == null) {\n          mergingNumericUpdates.put(e.getKey(), e.getValue());\n        } else {\n          fieldUpdates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // create a new map, keeping only the gens that are in use\n    Map<Long,Set<String>> genUpdatesFiles = info.getUpdatesFiles();\n    Map<Long,Set<String>> newGenUpdatesFiles = new HashMap<Long,Set<String>>();\n    final long fieldInfosGen = info.getFieldInfosGen();\n    for (FieldInfo fi : fieldInfos) {\n      long dvGen = fi.getDocValuesGen();\n      if (dvGen != -1 && !newGenUpdatesFiles.containsKey(dvGen)) {\n        if (dvGen == fieldInfosGen) {\n          newGenUpdatesFiles.put(fieldInfosGen, trackingDir.getCreatedFiles());\n        } else {\n          newGenUpdatesFiles.put(dvGen, genUpdatesFiles.get(dvGen));\n        }\n      }\n    }\n    \n    info.setGenUpdatesFiles(newGenUpdatesFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,Map[String,NumericFieldUpdates]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,Map[String,NumericFieldUpdates]).mjava","sourceNew":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, Map<String,NumericFieldUpdates> numericFieldUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert numericFieldUpdates != null && !numericFieldUpdates.isEmpty();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : numericFieldUpdates.keySet()) {\n          builder.addOrUpdate(f, NumericDocValuesField.TYPE);\n        }\n        \n        fieldInfos = builder.finish();\n        final long nextFieldInfosGen = info.getNextFieldInfosGen();\n        final String segmentSuffix = Long.toString(nextFieldInfosGen, Character.MAX_RADIX);\n        final SegmentWriteState state = new SegmentWriteState(null, trackingDir, info.info, fieldInfos, null, IOContext.DEFAULT, segmentSuffix);\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        final DocValuesConsumer fieldsConsumer = docValuesFormat.fieldsConsumer(state);\n        boolean fieldsConsumerSuccess = false;\n        try {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: applying updates; seg=\" + info + \" updates=\" + numericUpdates);\n          for (Entry<String,NumericFieldUpdates> e : numericFieldUpdates.entrySet()) {\n            final String field = e.getKey();\n            final NumericFieldUpdates fieldUpdates = e.getValue();\n            final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n            assert fieldInfo != null;\n\n            fieldInfo.setDocValuesGen(nextFieldInfosGen);\n            // write the numeric updates to a new gen'd docvalues file\n            fieldsConsumer.addNumericField(fieldInfo, new Iterable<Number>() {\n              final NumericDocValues currentValues = reader.getNumericDocValues(field);\n              final Bits docsWithField = reader.getDocsWithField(field);\n              final int maxDoc = reader.maxDoc();\n              final UpdatesIterator updatesIter = fieldUpdates.getUpdates();\n              @Override\n              public Iterator<Number> iterator() {\n                updatesIter.reset();\n                return new Iterator<Number>() {\n\n                  int curDoc = -1;\n                  int updateDoc = updatesIter.nextDoc();\n                  \n                  @Override\n                  public boolean hasNext() {\n                    return curDoc < maxDoc - 1;\n                  }\n\n                  @Override\n                  public Number next() {\n                    if (++curDoc >= maxDoc) {\n                      throw new NoSuchElementException(\"no more documents to return values for\");\n                    }\n                    if (curDoc == updateDoc) { // this document has an updated value\n                      Long value = updatesIter.value(); // either null (unset value) or updated value\n                      updateDoc = updatesIter.nextDoc(); // prepare for next round\n                      return value;\n                    } else {\n                      // no update for this document\n                      assert curDoc < updateDoc;\n                      if (currentValues != null && docsWithField.get(curDoc)) {\n                        // only read the current value if the document had a value before\n                        return currentValues.get(curDoc);\n                      } else {\n                        return null;\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void remove() {\n                    throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                  }\n                };\n              }\n            });\n          }\n          \n          codec.fieldInfosFormat().getFieldInfosWriter().write(trackingDir, info.info.name, segmentSuffix, fieldInfos, IOContext.DEFAULT);\n          fieldsConsumerSuccess = true;\n        } finally {\n          if (fieldsConsumerSuccess) {\n            fieldsConsumer.close();\n          } else {\n            IOUtils.closeWhileHandlingException(fieldsConsumer);\n          }\n        }\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteDocValuesGen so that a 2nd\n        // attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          try {\n            dir.deleteFile(fileName);\n          } catch (Throwable t) {\n            // Ignore so we throw only the first exc\n          }\n        }\n      }\n    }\n    \n    info.advanceFieldInfosGen();\n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericFieldUpdates> e : numericFieldUpdates.entrySet()) {\n        NumericFieldUpdates fieldUpdates = mergingNumericUpdates.get(e.getKey());\n        if (fieldUpdates == null) {\n          mergingNumericUpdates.put(e.getKey(), e.getValue());\n        } else {\n          fieldUpdates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // create a new map, keeping only the gens that are in use\n    Map<Long,Set<String>> genUpdatesFiles = info.getUpdatesFiles();\n    Map<Long,Set<String>> newGenUpdatesFiles = new HashMap<>();\n    final long fieldInfosGen = info.getFieldInfosGen();\n    for (FieldInfo fi : fieldInfos) {\n      long dvGen = fi.getDocValuesGen();\n      if (dvGen != -1 && !newGenUpdatesFiles.containsKey(dvGen)) {\n        if (dvGen == fieldInfosGen) {\n          newGenUpdatesFiles.put(fieldInfosGen, trackingDir.getCreatedFiles());\n        } else {\n          newGenUpdatesFiles.put(dvGen, genUpdatesFiles.get(dvGen));\n        }\n      }\n    }\n    \n    info.setGenUpdatesFiles(newGenUpdatesFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, Map<String,NumericFieldUpdates> numericFieldUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert numericFieldUpdates != null && !numericFieldUpdates.isEmpty();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : numericFieldUpdates.keySet()) {\n          builder.addOrUpdate(f, NumericDocValuesField.TYPE);\n        }\n        \n        fieldInfos = builder.finish();\n        final long nextFieldInfosGen = info.getNextFieldInfosGen();\n        final String segmentSuffix = Long.toString(nextFieldInfosGen, Character.MAX_RADIX);\n        final SegmentWriteState state = new SegmentWriteState(null, trackingDir, info.info, fieldInfos, null, IOContext.DEFAULT, segmentSuffix);\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        final DocValuesConsumer fieldsConsumer = docValuesFormat.fieldsConsumer(state);\n        boolean fieldsConsumerSuccess = false;\n        try {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: applying updates; seg=\" + info + \" updates=\" + numericUpdates);\n          for (Entry<String,NumericFieldUpdates> e : numericFieldUpdates.entrySet()) {\n            final String field = e.getKey();\n            final NumericFieldUpdates fieldUpdates = e.getValue();\n            final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n            assert fieldInfo != null;\n\n            fieldInfo.setDocValuesGen(nextFieldInfosGen);\n            // write the numeric updates to a new gen'd docvalues file\n            fieldsConsumer.addNumericField(fieldInfo, new Iterable<Number>() {\n              final NumericDocValues currentValues = reader.getNumericDocValues(field);\n              final Bits docsWithField = reader.getDocsWithField(field);\n              final int maxDoc = reader.maxDoc();\n              final UpdatesIterator updatesIter = fieldUpdates.getUpdates();\n              @Override\n              public Iterator<Number> iterator() {\n                updatesIter.reset();\n                return new Iterator<Number>() {\n\n                  int curDoc = -1;\n                  int updateDoc = updatesIter.nextDoc();\n                  \n                  @Override\n                  public boolean hasNext() {\n                    return curDoc < maxDoc - 1;\n                  }\n\n                  @Override\n                  public Number next() {\n                    if (++curDoc >= maxDoc) {\n                      throw new NoSuchElementException(\"no more documents to return values for\");\n                    }\n                    if (curDoc == updateDoc) { // this document has an updated value\n                      Long value = updatesIter.value(); // either null (unset value) or updated value\n                      updateDoc = updatesIter.nextDoc(); // prepare for next round\n                      return value;\n                    } else {\n                      // no update for this document\n                      assert curDoc < updateDoc;\n                      if (currentValues != null && docsWithField.get(curDoc)) {\n                        // only read the current value if the document had a value before\n                        return currentValues.get(curDoc);\n                      } else {\n                        return null;\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void remove() {\n                    throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                  }\n                };\n              }\n            });\n          }\n          \n          codec.fieldInfosFormat().getFieldInfosWriter().write(trackingDir, info.info.name, segmentSuffix, fieldInfos, IOContext.DEFAULT);\n          fieldsConsumerSuccess = true;\n        } finally {\n          if (fieldsConsumerSuccess) {\n            fieldsConsumer.close();\n          } else {\n            IOUtils.closeWhileHandlingException(fieldsConsumer);\n          }\n        }\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteDocValuesGen so that a 2nd\n        // attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          try {\n            dir.deleteFile(fileName);\n          } catch (Throwable t) {\n            // Ignore so we throw only the first exc\n          }\n        }\n      }\n    }\n    \n    info.advanceFieldInfosGen();\n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericFieldUpdates> e : numericFieldUpdates.entrySet()) {\n        NumericFieldUpdates fieldUpdates = mergingNumericUpdates.get(e.getKey());\n        if (fieldUpdates == null) {\n          mergingNumericUpdates.put(e.getKey(), e.getValue());\n        } else {\n          fieldUpdates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // create a new map, keeping only the gens that are in use\n    Map<Long,Set<String>> genUpdatesFiles = info.getUpdatesFiles();\n    Map<Long,Set<String>> newGenUpdatesFiles = new HashMap<Long,Set<String>>();\n    final long fieldInfosGen = info.getFieldInfosGen();\n    for (FieldInfo fi : fieldInfos) {\n      long dvGen = fi.getDocValuesGen();\n      if (dvGen != -1 && !newGenUpdatesFiles.containsKey(dvGen)) {\n        if (dvGen == fieldInfosGen) {\n          newGenUpdatesFiles.put(fieldInfosGen, trackingDir.getCreatedFiles());\n        } else {\n          newGenUpdatesFiles.put(dvGen, genUpdatesFiles.get(dvGen));\n        }\n      }\n    }\n    \n    info.setGenUpdatesFiles(newGenUpdatesFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["0567bdc5c86c94ced64201187cfcef2417d76dda"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"06805da26538ed636bd89b10c2699cc3834032ae","date":1395132972,"type":5,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,Map[String,NumericFieldUpdates]).mjava","sourceNew":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          builder.addOrUpdate(f, NumericDocValuesField.TYPE);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          builder.addOrUpdate(f, BinaryDocValuesField.TYPE);\n        }\n        \n        fieldInfos = builder.finish();\n        final long nextFieldInfosGen = info.getNextFieldInfosGen();\n        final String segmentSuffix = Long.toString(nextFieldInfosGen, Character.MAX_RADIX);\n        final SegmentWriteState state = new SegmentWriteState(null, trackingDir, info.info, fieldInfos, null, IOContext.DEFAULT, segmentSuffix);\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        final DocValuesConsumer fieldsConsumer = docValuesFormat.fieldsConsumer(state);\n        boolean fieldsConsumerSuccess = false;\n        try {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n          for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n            final String field = e.getKey();\n            final NumericDocValuesFieldUpdates fieldUpdates = e.getValue();\n            final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n            assert fieldInfo != null;\n\n            fieldInfo.setDocValuesGen(nextFieldInfosGen);\n            // write the numeric updates to a new gen'd docvalues file\n            fieldsConsumer.addNumericField(fieldInfo, new Iterable<Number>() {\n              final NumericDocValues currentValues = reader.getNumericDocValues(field);\n              final Bits docsWithField = reader.getDocsWithField(field);\n              final int maxDoc = reader.maxDoc();\n              final NumericDocValuesFieldUpdates.Iterator updatesIter = fieldUpdates.iterator();\n              @Override\n              public Iterator<Number> iterator() {\n                updatesIter.reset();\n                return new Iterator<Number>() {\n\n                  int curDoc = -1;\n                  int updateDoc = updatesIter.nextDoc();\n                  \n                  @Override\n                  public boolean hasNext() {\n                    return curDoc < maxDoc - 1;\n                  }\n\n                  @Override\n                  public Number next() {\n                    if (++curDoc >= maxDoc) {\n                      throw new NoSuchElementException(\"no more documents to return values for\");\n                    }\n                    if (curDoc == updateDoc) { // this document has an updated value\n                      Long value = updatesIter.value(); // either null (unset value) or updated value\n                      updateDoc = updatesIter.nextDoc(); // prepare for next round\n                      return value;\n                    } else {\n                      // no update for this document\n                      assert curDoc < updateDoc;\n                      if (currentValues != null && docsWithField.get(curDoc)) {\n                        // only read the current value if the document had a value before\n                        return currentValues.get(curDoc);\n                      } else {\n                        return null;\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void remove() {\n                    throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                  }\n                };\n              }\n            });\n          }\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n          final String field = e.getKey();\n          final BinaryDocValuesFieldUpdates dvFieldUpdates = e.getValue();\n          final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n          assert fieldInfo != null;\n\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" f=\" + dvFieldUpdates + \", updates=\" + dvFieldUpdates);\n\n          fieldInfo.setDocValuesGen(nextFieldInfosGen);\n          // write the numeric updates to a new gen'd docvalues file\n          fieldsConsumer.addBinaryField(fieldInfo, new Iterable<BytesRef>() {\n            final BinaryDocValues currentValues = reader.getBinaryDocValues(field);\n            final Bits docsWithField = reader.getDocsWithField(field);\n            final int maxDoc = reader.maxDoc();\n            final BinaryDocValuesFieldUpdates.Iterator updatesIter = dvFieldUpdates.iterator();\n            @Override\n            public Iterator<BytesRef> iterator() {\n              updatesIter.reset();\n              return new Iterator<BytesRef>() {\n\n                int curDoc = -1;\n                int updateDoc = updatesIter.nextDoc();\n                BytesRef scratch = new BytesRef();\n                \n                @Override\n                public boolean hasNext() {\n                  return curDoc < maxDoc - 1;\n                }\n\n                @Override\n                public BytesRef next() {\n                  if (++curDoc >= maxDoc) {\n                    throw new NoSuchElementException(\"no more documents to return values for\");\n                  }\n                  if (curDoc == updateDoc) { // this document has an updated value\n                    BytesRef value = updatesIter.value(); // either null (unset value) or updated value\n                    updateDoc = updatesIter.nextDoc(); // prepare for next round\n                    return value;\n                  } else {\n                    // no update for this document\n                    assert curDoc < updateDoc;\n                    if (currentValues != null && docsWithField.get(curDoc)) {\n                      // only read the current value if the document had a value before\n                      currentValues.get(curDoc, scratch);\n                      return scratch;\n                    } else {\n                      return null;\n                    }\n                  }\n                }\n\n                @Override\n                public void remove() {\n                  throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                }\n              };\n            }\n          });\n        }\n\n          codec.fieldInfosFormat().getFieldInfosWriter().write(trackingDir, info.info.name, segmentSuffix, fieldInfos, IOContext.DEFAULT);\n          fieldsConsumerSuccess = true;\n        } finally {\n          if (fieldsConsumerSuccess) {\n            fieldsConsumer.close();\n          } else {\n            IOUtils.closeWhileHandlingException(fieldsConsumer);\n          }\n        }\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteDocValuesGen so that a 2nd\n        // attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          try {\n            dir.deleteFile(fileName);\n          } catch (Throwable t) {\n            // Ignore so we throw only the first exc\n          }\n        }\n      }\n    }\n    \n    info.advanceFieldInfosGen();\n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // create a new map, keeping only the gens that are in use\n    Map<Long,Set<String>> genUpdatesFiles = info.getUpdatesFiles();\n    Map<Long,Set<String>> newGenUpdatesFiles = new HashMap<>();\n    final long fieldInfosGen = info.getFieldInfosGen();\n    for (FieldInfo fi : fieldInfos) {\n      long dvGen = fi.getDocValuesGen();\n      if (dvGen != -1 && !newGenUpdatesFiles.containsKey(dvGen)) {\n        if (dvGen == fieldInfosGen) {\n          newGenUpdatesFiles.put(fieldInfosGen, trackingDir.getCreatedFiles());\n        } else {\n          newGenUpdatesFiles.put(dvGen, genUpdatesFiles.get(dvGen));\n        }\n      }\n    }\n    \n    info.setGenUpdatesFiles(newGenUpdatesFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, Map<String,NumericFieldUpdates> numericFieldUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert numericFieldUpdates != null && !numericFieldUpdates.isEmpty();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : numericFieldUpdates.keySet()) {\n          builder.addOrUpdate(f, NumericDocValuesField.TYPE);\n        }\n        \n        fieldInfos = builder.finish();\n        final long nextFieldInfosGen = info.getNextFieldInfosGen();\n        final String segmentSuffix = Long.toString(nextFieldInfosGen, Character.MAX_RADIX);\n        final SegmentWriteState state = new SegmentWriteState(null, trackingDir, info.info, fieldInfos, null, IOContext.DEFAULT, segmentSuffix);\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        final DocValuesConsumer fieldsConsumer = docValuesFormat.fieldsConsumer(state);\n        boolean fieldsConsumerSuccess = false;\n        try {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: applying updates; seg=\" + info + \" updates=\" + numericUpdates);\n          for (Entry<String,NumericFieldUpdates> e : numericFieldUpdates.entrySet()) {\n            final String field = e.getKey();\n            final NumericFieldUpdates fieldUpdates = e.getValue();\n            final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n            assert fieldInfo != null;\n\n            fieldInfo.setDocValuesGen(nextFieldInfosGen);\n            // write the numeric updates to a new gen'd docvalues file\n            fieldsConsumer.addNumericField(fieldInfo, new Iterable<Number>() {\n              final NumericDocValues currentValues = reader.getNumericDocValues(field);\n              final Bits docsWithField = reader.getDocsWithField(field);\n              final int maxDoc = reader.maxDoc();\n              final UpdatesIterator updatesIter = fieldUpdates.getUpdates();\n              @Override\n              public Iterator<Number> iterator() {\n                updatesIter.reset();\n                return new Iterator<Number>() {\n\n                  int curDoc = -1;\n                  int updateDoc = updatesIter.nextDoc();\n                  \n                  @Override\n                  public boolean hasNext() {\n                    return curDoc < maxDoc - 1;\n                  }\n\n                  @Override\n                  public Number next() {\n                    if (++curDoc >= maxDoc) {\n                      throw new NoSuchElementException(\"no more documents to return values for\");\n                    }\n                    if (curDoc == updateDoc) { // this document has an updated value\n                      Long value = updatesIter.value(); // either null (unset value) or updated value\n                      updateDoc = updatesIter.nextDoc(); // prepare for next round\n                      return value;\n                    } else {\n                      // no update for this document\n                      assert curDoc < updateDoc;\n                      if (currentValues != null && docsWithField.get(curDoc)) {\n                        // only read the current value if the document had a value before\n                        return currentValues.get(curDoc);\n                      } else {\n                        return null;\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void remove() {\n                    throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                  }\n                };\n              }\n            });\n          }\n          \n          codec.fieldInfosFormat().getFieldInfosWriter().write(trackingDir, info.info.name, segmentSuffix, fieldInfos, IOContext.DEFAULT);\n          fieldsConsumerSuccess = true;\n        } finally {\n          if (fieldsConsumerSuccess) {\n            fieldsConsumer.close();\n          } else {\n            IOUtils.closeWhileHandlingException(fieldsConsumer);\n          }\n        }\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteDocValuesGen so that a 2nd\n        // attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          try {\n            dir.deleteFile(fileName);\n          } catch (Throwable t) {\n            // Ignore so we throw only the first exc\n          }\n        }\n      }\n    }\n    \n    info.advanceFieldInfosGen();\n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericFieldUpdates> e : numericFieldUpdates.entrySet()) {\n        NumericFieldUpdates fieldUpdates = mergingNumericUpdates.get(e.getKey());\n        if (fieldUpdates == null) {\n          mergingNumericUpdates.put(e.getKey(), e.getValue());\n        } else {\n          fieldUpdates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // create a new map, keeping only the gens that are in use\n    Map<Long,Set<String>> genUpdatesFiles = info.getUpdatesFiles();\n    Map<Long,Set<String>> newGenUpdatesFiles = new HashMap<>();\n    final long fieldInfosGen = info.getFieldInfosGen();\n    for (FieldInfo fi : fieldInfos) {\n      long dvGen = fi.getDocValuesGen();\n      if (dvGen != -1 && !newGenUpdatesFiles.containsKey(dvGen)) {\n        if (dvGen == fieldInfosGen) {\n          newGenUpdatesFiles.put(fieldInfosGen, trackingDir.getCreatedFiles());\n        } else {\n          newGenUpdatesFiles.put(dvGen, genUpdatesFiles.get(dvGen));\n        }\n      }\n    }\n    \n    info.setGenUpdatesFiles(newGenUpdatesFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["0567bdc5c86c94ced64201187cfcef2417d76dda"],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"06805da26538ed636bd89b10c2699cc3834032ae":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["06805da26538ed636bd89b10c2699cc3834032ae"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["06805da26538ed636bd89b10c2699cc3834032ae"],"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"06805da26538ed636bd89b10c2699cc3834032ae":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}