{"path":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName, new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=new Term(f.fieldName, term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()), atts, startTerm, f.minSimilarity, f.prefixLength, false);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName, new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=new Term(f.fieldName, term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()), atts, startTerm, f.minSimilarity, f.prefixLength, false);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d374631bb6f5665bd5907a6186a9bb283a315f7d","date":1336345692,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName, new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=new Term(f.fieldName, term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName, new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=new Term(f.fieldName, term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()), atts, startTerm, f.minSimilarity, f.prefixLength, false);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4093b270ba337f9c25a4c0e6cb2ae2c07f697376","date":1347897716,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"  private void addTerms(IndexReader reader, FieldVals f) throws IOException {\n    if (f.queryString == null) return;\n    TokenStream ts = analyzer.tokenStream(f.fieldName, new StringReader(f.queryString));\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n    int corpusNumDocs = reader.numDocs();\n    HashSet<String> processedTerms = new HashSet<String>();\n    ts.reset();\n    while (ts.incrementToken()) {\n      String term = termAtt.toString();\n      if (!processedTerms.contains(term)) {\n        processedTerms.add(term);\n        ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n        float minScore = 0;\n        Term startTerm = new Term(f.fieldName, term);\n        AttributeSource atts = new AttributeSource();\n        MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n        SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()), atts, startTerm, f.minSimilarity, f.prefixLength);\n        //store the df so all variants use same idf\n        int df = reader.docFreq(startTerm);\n        int numVariants = 0;\n        int totalVariantDocFreqs = 0;\n        BytesRef possibleMatch;\n        BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n        while ((possibleMatch = fe.next()) != null) {\n          numVariants++;\n          totalVariantDocFreqs += fe.docFreq();\n          float score = boostAtt.getBoost();\n          if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n            ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n            variantsQ.insertWithOverflow(st);\n            minScore = variantsQ.top().score; // maintain minScore\n          }\n          maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n        }\n\n        if (numVariants > 0) {\n          int avgDf = totalVariantDocFreqs / numVariants;\n          if (df == 0)//no direct match we can use as df for all variants\n          {\n            df = avgDf; //use avg df of all variants\n          }\n\n          // take the top variants (scored by edit distance) and reset the score\n          // to include an IDF factor then add to the global queue for ranking\n          // overall top query terms\n          int size = variantsQ.size();\n          for (int i = 0; i < size; i++) {\n            ScoreTerm st = variantsQ.pop();\n            st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n            q.insertWithOverflow(st);\n          }\n        }\n      }\n    }\n    ts.end();\n    ts.close();\n  }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName, new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=new Term(f.fieldName, term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","bugFix":null,"bugIntro":["4768021ba34ebadac4585e912bccd53d943cfb40","c83d6c4335f31cae14f625a222bc842f20073dcd","782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4768021ba34ebadac4585e912bccd53d943cfb40","date":1362142240,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"  private void addTerms(IndexReader reader, FieldVals f) throws IOException {\n    if (f.queryString == null) return;\n    TokenStream ts = analyzer.tokenStream(f.fieldName, new StringReader(f.queryString));\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n    int corpusNumDocs = reader.numDocs();\n    HashSet<String> processedTerms = new HashSet<String>();\n    ts.reset();\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    while (ts.incrementToken()) {\n      String term = termAtt.toString();\n      if (!processedTerms.contains(term)) {\n        processedTerms.add(term);\n        ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n        float minScore = 0;\n        Term startTerm = new Term(f.fieldName, term);\n        AttributeSource atts = new AttributeSource();\n        MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n        SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(terms, atts, startTerm, f.minSimilarity, f.prefixLength);\n        //store the df so all variants use same idf\n        int df = reader.docFreq(startTerm);\n        int numVariants = 0;\n        int totalVariantDocFreqs = 0;\n        BytesRef possibleMatch;\n        BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n        while ((possibleMatch = fe.next()) != null) {\n          numVariants++;\n          totalVariantDocFreqs += fe.docFreq();\n          float score = boostAtt.getBoost();\n          if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n            ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n            variantsQ.insertWithOverflow(st);\n            minScore = variantsQ.top().score; // maintain minScore\n          }\n          maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n        }\n\n        if (numVariants > 0) {\n          int avgDf = totalVariantDocFreqs / numVariants;\n          if (df == 0)//no direct match we can use as df for all variants\n          {\n            df = avgDf; //use avg df of all variants\n          }\n\n          // take the top variants (scored by edit distance) and reset the score\n          // to include an IDF factor then add to the global queue for ranking\n          // overall top query terms\n          int size = variantsQ.size();\n          for (int i = 0; i < size; i++) {\n            ScoreTerm st = variantsQ.pop();\n            st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n            q.insertWithOverflow(st);\n          }\n        }\n      }\n    }\n    ts.end();\n    ts.close();\n  }\n\n","sourceOld":"  private void addTerms(IndexReader reader, FieldVals f) throws IOException {\n    if (f.queryString == null) return;\n    TokenStream ts = analyzer.tokenStream(f.fieldName, new StringReader(f.queryString));\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n    int corpusNumDocs = reader.numDocs();\n    HashSet<String> processedTerms = new HashSet<String>();\n    ts.reset();\n    while (ts.incrementToken()) {\n      String term = termAtt.toString();\n      if (!processedTerms.contains(term)) {\n        processedTerms.add(term);\n        ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n        float minScore = 0;\n        Term startTerm = new Term(f.fieldName, term);\n        AttributeSource atts = new AttributeSource();\n        MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n        SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()), atts, startTerm, f.minSimilarity, f.prefixLength);\n        //store the df so all variants use same idf\n        int df = reader.docFreq(startTerm);\n        int numVariants = 0;\n        int totalVariantDocFreqs = 0;\n        BytesRef possibleMatch;\n        BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n        while ((possibleMatch = fe.next()) != null) {\n          numVariants++;\n          totalVariantDocFreqs += fe.docFreq();\n          float score = boostAtt.getBoost();\n          if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n            ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n            variantsQ.insertWithOverflow(st);\n            minScore = variantsQ.top().score; // maintain minScore\n          }\n          maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n        }\n\n        if (numVariants > 0) {\n          int avgDf = totalVariantDocFreqs / numVariants;\n          if (df == 0)//no direct match we can use as df for all variants\n          {\n            df = avgDf; //use avg df of all variants\n          }\n\n          // take the top variants (scored by edit distance) and reset the score\n          // to include an IDF factor then add to the global queue for ranking\n          // overall top query terms\n          int size = variantsQ.size();\n          for (int i = 0; i < size; i++) {\n            ScoreTerm st = variantsQ.pop();\n            st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n            q.insertWithOverflow(st);\n          }\n        }\n      }\n    }\n    ts.end();\n    ts.close();\n  }\n\n","bugFix":["4093b270ba337f9c25a4c0e6cb2ae2c07f697376"],"bugIntro":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c83d6c4335f31cae14f625a222bc842f20073dcd","date":1373306148,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"  private void addTerms(IndexReader reader, FieldVals f) throws IOException {\n    if (f.queryString == null) return;\n    TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString);\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n    int corpusNumDocs = reader.numDocs();\n    HashSet<String> processedTerms = new HashSet<String>();\n    ts.reset();\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    while (ts.incrementToken()) {\n      String term = termAtt.toString();\n      if (!processedTerms.contains(term)) {\n        processedTerms.add(term);\n        ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n        float minScore = 0;\n        Term startTerm = new Term(f.fieldName, term);\n        AttributeSource atts = new AttributeSource();\n        MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n        SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(terms, atts, startTerm, f.minSimilarity, f.prefixLength);\n        //store the df so all variants use same idf\n        int df = reader.docFreq(startTerm);\n        int numVariants = 0;\n        int totalVariantDocFreqs = 0;\n        BytesRef possibleMatch;\n        BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n        while ((possibleMatch = fe.next()) != null) {\n          numVariants++;\n          totalVariantDocFreqs += fe.docFreq();\n          float score = boostAtt.getBoost();\n          if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n            ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n            variantsQ.insertWithOverflow(st);\n            minScore = variantsQ.top().score; // maintain minScore\n          }\n          maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n        }\n\n        if (numVariants > 0) {\n          int avgDf = totalVariantDocFreqs / numVariants;\n          if (df == 0)//no direct match we can use as df for all variants\n          {\n            df = avgDf; //use avg df of all variants\n          }\n\n          // take the top variants (scored by edit distance) and reset the score\n          // to include an IDF factor then add to the global queue for ranking\n          // overall top query terms\n          int size = variantsQ.size();\n          for (int i = 0; i < size; i++) {\n            ScoreTerm st = variantsQ.pop();\n            st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n            q.insertWithOverflow(st);\n          }\n        }\n      }\n    }\n    ts.end();\n    ts.close();\n  }\n\n","sourceOld":"  private void addTerms(IndexReader reader, FieldVals f) throws IOException {\n    if (f.queryString == null) return;\n    TokenStream ts = analyzer.tokenStream(f.fieldName, new StringReader(f.queryString));\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n    int corpusNumDocs = reader.numDocs();\n    HashSet<String> processedTerms = new HashSet<String>();\n    ts.reset();\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    while (ts.incrementToken()) {\n      String term = termAtt.toString();\n      if (!processedTerms.contains(term)) {\n        processedTerms.add(term);\n        ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n        float minScore = 0;\n        Term startTerm = new Term(f.fieldName, term);\n        AttributeSource atts = new AttributeSource();\n        MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n        SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(terms, atts, startTerm, f.minSimilarity, f.prefixLength);\n        //store the df so all variants use same idf\n        int df = reader.docFreq(startTerm);\n        int numVariants = 0;\n        int totalVariantDocFreqs = 0;\n        BytesRef possibleMatch;\n        BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n        while ((possibleMatch = fe.next()) != null) {\n          numVariants++;\n          totalVariantDocFreqs += fe.docFreq();\n          float score = boostAtt.getBoost();\n          if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n            ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n            variantsQ.insertWithOverflow(st);\n            minScore = variantsQ.top().score; // maintain minScore\n          }\n          maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n        }\n\n        if (numVariants > 0) {\n          int avgDf = totalVariantDocFreqs / numVariants;\n          if (df == 0)//no direct match we can use as df for all variants\n          {\n            df = avgDf; //use avg df of all variants\n          }\n\n          // take the top variants (scored by edit distance) and reset the score\n          // to include an IDF factor then add to the global queue for ranking\n          // overall top query terms\n          int size = variantsQ.size();\n          for (int i = 0; i < size; i++) {\n            ScoreTerm st = variantsQ.pop();\n            st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n            q.insertWithOverflow(st);\n          }\n        }\n      }\n    }\n    ts.end();\n    ts.close();\n  }\n\n","bugFix":["4093b270ba337f9c25a4c0e6cb2ae2c07f697376"],"bugIntro":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"  private void addTerms(IndexReader reader, FieldVals f) throws IOException {\n    if (f.queryString == null) return;\n    TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString);\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n    int corpusNumDocs = reader.numDocs();\n    HashSet<String> processedTerms = new HashSet<String>();\n    ts.reset();\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    while (ts.incrementToken()) {\n      String term = termAtt.toString();\n      if (!processedTerms.contains(term)) {\n        processedTerms.add(term);\n        ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n        float minScore = 0;\n        Term startTerm = new Term(f.fieldName, term);\n        AttributeSource atts = new AttributeSource();\n        MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n        SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(terms, atts, startTerm, f.minSimilarity, f.prefixLength);\n        //store the df so all variants use same idf\n        int df = reader.docFreq(startTerm);\n        int numVariants = 0;\n        int totalVariantDocFreqs = 0;\n        BytesRef possibleMatch;\n        BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n        while ((possibleMatch = fe.next()) != null) {\n          numVariants++;\n          totalVariantDocFreqs += fe.docFreq();\n          float score = boostAtt.getBoost();\n          if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n            ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n            variantsQ.insertWithOverflow(st);\n            minScore = variantsQ.top().score; // maintain minScore\n          }\n          maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n        }\n\n        if (numVariants > 0) {\n          int avgDf = totalVariantDocFreqs / numVariants;\n          if (df == 0)//no direct match we can use as df for all variants\n          {\n            df = avgDf; //use avg df of all variants\n          }\n\n          // take the top variants (scored by edit distance) and reset the score\n          // to include an IDF factor then add to the global queue for ranking\n          // overall top query terms\n          int size = variantsQ.size();\n          for (int i = 0; i < size; i++) {\n            ScoreTerm st = variantsQ.pop();\n            st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n            q.insertWithOverflow(st);\n          }\n        }\n      }\n    }\n    ts.end();\n    ts.close();\n  }\n\n","sourceOld":"  private void addTerms(IndexReader reader, FieldVals f) throws IOException {\n    if (f.queryString == null) return;\n    TokenStream ts = analyzer.tokenStream(f.fieldName, new StringReader(f.queryString));\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n    int corpusNumDocs = reader.numDocs();\n    HashSet<String> processedTerms = new HashSet<String>();\n    ts.reset();\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    while (ts.incrementToken()) {\n      String term = termAtt.toString();\n      if (!processedTerms.contains(term)) {\n        processedTerms.add(term);\n        ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n        float minScore = 0;\n        Term startTerm = new Term(f.fieldName, term);\n        AttributeSource atts = new AttributeSource();\n        MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n        SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(terms, atts, startTerm, f.minSimilarity, f.prefixLength);\n        //store the df so all variants use same idf\n        int df = reader.docFreq(startTerm);\n        int numVariants = 0;\n        int totalVariantDocFreqs = 0;\n        BytesRef possibleMatch;\n        BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n        while ((possibleMatch = fe.next()) != null) {\n          numVariants++;\n          totalVariantDocFreqs += fe.docFreq();\n          float score = boostAtt.getBoost();\n          if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n            ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n            variantsQ.insertWithOverflow(st);\n            minScore = variantsQ.top().score; // maintain minScore\n          }\n          maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n        }\n\n        if (numVariants > 0) {\n          int avgDf = totalVariantDocFreqs / numVariants;\n          if (df == 0)//no direct match we can use as df for all variants\n          {\n            df = avgDf; //use avg df of all variants\n          }\n\n          // take the top variants (scored by edit distance) and reset the score\n          // to include an IDF factor then add to the global queue for ranking\n          // overall top query terms\n          int size = variantsQ.size();\n          for (int i = 0; i < size; i++) {\n            ScoreTerm st = variantsQ.pop();\n            st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n            q.insertWithOverflow(st);\n          }\n        }\n      }\n    }\n    ts.end();\n    ts.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"782ed6a4b4ba50ec19734fc8db4e570ee193d627","date":1381127065,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"  private void addTerms(IndexReader reader, FieldVals f) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<String>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(terms, atts, startTerm, f.minSimilarity, f.prefixLength);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n            {\n              df = avgDf; //use avg df of all variants\n            }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n              q.insertWithOverflow(st);\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","sourceOld":"  private void addTerms(IndexReader reader, FieldVals f) throws IOException {\n    if (f.queryString == null) return;\n    TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString);\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n    int corpusNumDocs = reader.numDocs();\n    HashSet<String> processedTerms = new HashSet<String>();\n    ts.reset();\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    while (ts.incrementToken()) {\n      String term = termAtt.toString();\n      if (!processedTerms.contains(term)) {\n        processedTerms.add(term);\n        ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n        float minScore = 0;\n        Term startTerm = new Term(f.fieldName, term);\n        AttributeSource atts = new AttributeSource();\n        MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n        SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(terms, atts, startTerm, f.minSimilarity, f.prefixLength);\n        //store the df so all variants use same idf\n        int df = reader.docFreq(startTerm);\n        int numVariants = 0;\n        int totalVariantDocFreqs = 0;\n        BytesRef possibleMatch;\n        BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n        while ((possibleMatch = fe.next()) != null) {\n          numVariants++;\n          totalVariantDocFreqs += fe.docFreq();\n          float score = boostAtt.getBoost();\n          if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n            ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n            variantsQ.insertWithOverflow(st);\n            minScore = variantsQ.top().score; // maintain minScore\n          }\n          maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n        }\n\n        if (numVariants > 0) {\n          int avgDf = totalVariantDocFreqs / numVariants;\n          if (df == 0)//no direct match we can use as df for all variants\n          {\n            df = avgDf; //use avg df of all variants\n          }\n\n          // take the top variants (scored by edit distance) and reset the score\n          // to include an IDF factor then add to the global queue for ranking\n          // overall top query terms\n          int size = variantsQ.size();\n          for (int i = 0; i < size; i++) {\n            ScoreTerm st = variantsQ.pop();\n            st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n            q.insertWithOverflow(st);\n          }\n        }\n      }\n    }\n    ts.end();\n    ts.close();\n  }\n\n","bugFix":["4768021ba34ebadac4585e912bccd53d943cfb40","955c32f886db6f6356c9fcdea6b1f1cb4effda24","c83d6c4335f31cae14f625a222bc842f20073dcd","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4","4093b270ba337f9c25a4c0e6cb2ae2c07f697376"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"  private void addTerms(IndexReader reader, FieldVals f) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(terms, atts, startTerm, f.minSimilarity, f.prefixLength);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n            {\n              df = avgDf; //use avg df of all variants\n            }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n              q.insertWithOverflow(st);\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","sourceOld":"  private void addTerms(IndexReader reader, FieldVals f) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<String>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(terms, atts, startTerm, f.minSimilarity, f.prefixLength);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n            {\n              df = avgDf; //use avg df of all variants\n            }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n              q.insertWithOverflow(st);\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"683697c94be5a13ed67b070f48f5c5499ee8f6ea","date":1441704970,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals,ScoreTermQueue).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"  private void addTerms(IndexReader reader, FieldVals f, ScoreTermQueue q) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(terms, atts, startTerm, f.minSimilarity, f.prefixLength);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n            {\n              df = avgDf; //use avg df of all variants\n            }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n              q.insertWithOverflow(st);\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","sourceOld":"  private void addTerms(IndexReader reader, FieldVals f) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(terms, atts, startTerm, f.minSimilarity, f.prefixLength);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n            {\n              df = avgDf; //use avg df of all variants\n            }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n              q.insertWithOverflow(st);\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"4768021ba34ebadac4585e912bccd53d943cfb40":["4093b270ba337f9c25a4c0e6cb2ae2c07f697376"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["4768021ba34ebadac4585e912bccd53d943cfb40","c83d6c4335f31cae14f625a222bc842f20073dcd"],"c83d6c4335f31cae14f625a222bc842f20073dcd":["4768021ba34ebadac4585e912bccd53d943cfb40"],"683697c94be5a13ed67b070f48f5c5499ee8f6ea":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"782ed6a4b4ba50ec19734fc8db4e570ee193d627":["c83d6c4335f31cae14f625a222bc842f20073dcd"],"d374631bb6f5665bd5907a6186a9bb283a315f7d":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["683697c94be5a13ed67b070f48f5c5499ee8f6ea"],"4093b270ba337f9c25a4c0e6cb2ae2c07f697376":["d374631bb6f5665bd5907a6186a9bb283a315f7d"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["683697c94be5a13ed67b070f48f5c5499ee8f6ea"],"4768021ba34ebadac4585e912bccd53d943cfb40":["37a0f60745e53927c4c876cfe5b5a58170f0646c","c83d6c4335f31cae14f625a222bc842f20073dcd"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["d374631bb6f5665bd5907a6186a9bb283a315f7d"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"c83d6c4335f31cae14f625a222bc842f20073dcd":["37a0f60745e53927c4c876cfe5b5a58170f0646c","782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"683697c94be5a13ed67b070f48f5c5499ee8f6ea":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"782ed6a4b4ba50ec19734fc8db4e570ee193d627":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"d374631bb6f5665bd5907a6186a9bb283a315f7d":["4093b270ba337f9c25a4c0e6cb2ae2c07f697376"],"4093b270ba337f9c25a4c0e6cb2ae2c07f697376":["4768021ba34ebadac4585e912bccd53d943cfb40"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["37a0f60745e53927c4c876cfe5b5a58170f0646c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}