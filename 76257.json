{"path":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","commits":[{"id":"f00f1c5fad501b66705121feb623f8cfbb6712f9","date":1431347838,"type":1,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/ParallelStream#constructStreams().mjava","sourceNew":"  protected void constructStreams() throws IOException {\n\n    try {\n      Object pushStream = null;\n\n      if (objectSerialize) {\n        ByteArrayOutputStream bout = new ByteArrayOutputStream();\n        ObjectOutputStream out = new ObjectOutputStream(bout);\n        out.writeObject(tupleStream);\n        byte[] bytes = bout.toByteArray();\n        String encoded = Base64.byteArrayToBase64(bytes, 0, bytes.length);\n        pushStream = URLEncoder.encode(encoded, \"UTF-8\");\n      } else {\n        pushStream = ((ExpressibleStream) tupleStream).toExpression(streamFactory);\n      }\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      long time = System.currentTimeMillis();\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random(time));\n\n      for(int w=0; w<workers; w++) {\n        HashMap params = new HashMap();\n        params.put(\"distrib\",\"false\"); // We are the aggregator.\n        params.put(\"numWorkers\", workers);\n        params.put(\"workerID\", w);\n        params.put(\"stream\", pushStream);\n        params.put(\"qt\",\"/stream\");\n        params.put(\"objectSerialize\", objectSerialize);\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, params);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","sourceOld":"  protected void constructStreams() throws IOException {\n\n    try {\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      long time = System.currentTimeMillis();\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random(time));\n\n      for(int w=0; w<workers; w++) {\n        HashMap params = new HashMap();\n        params.put(\"distrib\",\"false\"); // We are the aggregator.\n        params.put(\"numWorkers\", workers);\n        params.put(\"workerID\", w);\n        params.put(\"stream\", this.encoded);\n        params.put(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, params);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e431b0806b8db369da896b66a74e304a49953c7b","date":1434740037,"type":3,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","sourceNew":"  protected void constructStreams() throws IOException {\n\n    try {\n      Object pushStream = null;\n\n      if (objectSerialize) {\n        ByteArrayOutputStream bout = new ByteArrayOutputStream();\n        ObjectOutputStream out = new ObjectOutputStream(bout);\n        out.writeObject(tupleStream);\n        byte[] bytes = bout.toByteArray();\n        String encoded = Base64.byteArrayToBase64(bytes, 0, bytes.length);\n        pushStream = URLEncoder.encode(encoded, \"UTF-8\");\n      } else {\n        pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n      }\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      long time = System.currentTimeMillis();\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random(time));\n\n      for(int w=0; w<workers; w++) {\n        HashMap params = new HashMap();\n        params.put(\"distrib\",\"false\"); // We are the aggregator.\n        params.put(\"numWorkers\", workers);\n        params.put(\"workerID\", w);\n        params.put(\"stream\", pushStream);\n        params.put(\"qt\",\"/stream\");\n        params.put(\"objectSerialize\", objectSerialize);\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, params);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","sourceOld":"  protected void constructStreams() throws IOException {\n\n    try {\n      Object pushStream = null;\n\n      if (objectSerialize) {\n        ByteArrayOutputStream bout = new ByteArrayOutputStream();\n        ObjectOutputStream out = new ObjectOutputStream(bout);\n        out.writeObject(tupleStream);\n        byte[] bytes = bout.toByteArray();\n        String encoded = Base64.byteArrayToBase64(bytes, 0, bytes.length);\n        pushStream = URLEncoder.encode(encoded, \"UTF-8\");\n      } else {\n        pushStream = ((ExpressibleStream) tupleStream).toExpression(streamFactory);\n      }\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      long time = System.currentTimeMillis();\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random(time));\n\n      for(int w=0; w<workers; w++) {\n        HashMap params = new HashMap();\n        params.put(\"distrib\",\"false\"); // We are the aggregator.\n        params.put(\"numWorkers\", workers);\n        params.put(\"workerID\", w);\n        params.put(\"stream\", pushStream);\n        params.put(\"qt\",\"/stream\");\n        params.put(\"objectSerialize\", objectSerialize);\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, params);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bcf9886c8ff537aafde14de48ebf744f5673f08b","date":1439041198,"type":3,"author":"Ramkumar Aiyengar","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","sourceNew":"  protected void constructStreams() throws IOException {\n\n    try {\n      Object pushStream = null;\n\n      if (objectSerialize) {\n        ByteArrayOutputStream bout = new ByteArrayOutputStream();\n        ObjectOutputStream out = new ObjectOutputStream(bout);\n        out.writeObject(tupleStream);\n        byte[] bytes = bout.toByteArray();\n        String encoded = Base64.byteArrayToBase64(bytes, 0, bytes.length);\n        pushStream = URLEncoder.encode(encoded, \"UTF-8\");\n      } else {\n        pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n      }\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        HashMap params = new HashMap();\n        params.put(\"distrib\",\"false\"); // We are the aggregator.\n        params.put(\"numWorkers\", workers);\n        params.put(\"workerID\", w);\n        params.put(\"stream\", pushStream);\n        params.put(\"qt\",\"/stream\");\n        params.put(\"objectSerialize\", objectSerialize);\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, params);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","sourceOld":"  protected void constructStreams() throws IOException {\n\n    try {\n      Object pushStream = null;\n\n      if (objectSerialize) {\n        ByteArrayOutputStream bout = new ByteArrayOutputStream();\n        ObjectOutputStream out = new ObjectOutputStream(bout);\n        out.writeObject(tupleStream);\n        byte[] bytes = bout.toByteArray();\n        String encoded = Base64.byteArrayToBase64(bytes, 0, bytes.length);\n        pushStream = URLEncoder.encode(encoded, \"UTF-8\");\n      } else {\n        pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n      }\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      long time = System.currentTimeMillis();\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random(time));\n\n      for(int w=0; w<workers; w++) {\n        HashMap params = new HashMap();\n        params.put(\"distrib\",\"false\"); // We are the aggregator.\n        params.put(\"numWorkers\", workers);\n        params.put(\"workerID\", w);\n        params.put(\"stream\", pushStream);\n        params.put(\"qt\",\"/stream\");\n        params.put(\"objectSerialize\", objectSerialize);\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, params);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c3cf1396610fbacfdf69deb27bc5d3f36b5fbd43","date":1449690748,"type":3,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","sourceNew":"  protected void constructStreams() throws IOException {\n\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        HashMap params = new HashMap();\n        params.put(\"distrib\",\"false\"); // We are the aggregator.\n        params.put(\"numWorkers\", workers);\n        params.put(\"workerID\", w);\n        params.put(\"stream\", pushStream);\n        params.put(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, params);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","sourceOld":"  protected void constructStreams() throws IOException {\n\n    try {\n      Object pushStream = null;\n\n      if (objectSerialize) {\n        ByteArrayOutputStream bout = new ByteArrayOutputStream();\n        ObjectOutputStream out = new ObjectOutputStream(bout);\n        out.writeObject(tupleStream);\n        byte[] bytes = bout.toByteArray();\n        String encoded = Base64.byteArrayToBase64(bytes, 0, bytes.length);\n        pushStream = URLEncoder.encode(encoded, \"UTF-8\");\n      } else {\n        pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n      }\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        HashMap params = new HashMap();\n        params.put(\"distrib\",\"false\"); // We are the aggregator.\n        params.put(\"numWorkers\", workers);\n        params.put(\"workerID\", w);\n        params.put(\"stream\", pushStream);\n        params.put(\"qt\",\"/stream\");\n        params.put(\"objectSerialize\", objectSerialize);\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, params);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"58b0ac98a4f0daf02f6400e863ed311c169630c6","date":1450474697,"type":3,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","sourceNew":"  protected void constructStreams() throws IOException {\n\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        HashMap params = new HashMap();\n        params.put(\"distrib\",\"false\"); // We are the aggregator.\n        params.put(\"numWorkers\", workers);\n        params.put(\"workerID\", w);\n        params.put(\"expr\", pushStream);\n        params.put(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, params);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","sourceOld":"  protected void constructStreams() throws IOException {\n\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        HashMap params = new HashMap();\n        params.put(\"distrib\",\"false\"); // We are the aggregator.\n        params.put(\"numWorkers\", workers);\n        params.put(\"workerID\", w);\n        params.put(\"stream\", pushStream);\n        params.put(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, params);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","bugFix":["f00f1c5fad501b66705121feb623f8cfbb6712f9"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad0d3af10297301b05f967ec10f31cbd376bdea3","date":1455206043,"type":3,"author":"jbernste","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","sourceNew":"  protected void constructStreams() throws IOException {\n\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Set<String> liveNodes = clusterState.getLiveNodes();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          if(replica.getState() == Replica.State.ACTIVE && liveNodes.contains(replica.getNodeName()))\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        HashMap params = new HashMap();\n        params.put(\"distrib\",\"false\"); // We are the aggregator.\n        params.put(\"numWorkers\", workers);\n        params.put(\"workerID\", w);\n        params.put(\"expr\", pushStream);\n        params.put(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, params);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","sourceOld":"  protected void constructStreams() throws IOException {\n\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        HashMap params = new HashMap();\n        params.put(\"distrib\",\"false\"); // We are the aggregator.\n        params.put(\"numWorkers\", workers);\n        params.put(\"workerID\", w);\n        params.put(\"expr\", pushStream);\n        params.put(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, params);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"826d15444ddf61716dc768c229cd54b2c2ccce1c","date":1462822652,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","sourceNew":"  protected void constructStreams() throws IOException {\n\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Set<String> liveNodes = clusterState.getLiveNodes();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          if(replica.getState() == Replica.State.ACTIVE && liveNodes.contains(replica.getNodeName()))\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        ModifiableSolrParams paramsLoc = new ModifiableSolrParams();\n        paramsLoc.set(\"distrib\",\"false\"); // We are the aggregator.\n        paramsLoc.set(\"numWorkers\", workers);\n        paramsLoc.set(\"workerID\", w);\n\n        paramsLoc.set(\"expr\", pushStream.toString());\n        paramsLoc.set(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, paramsLoc);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","sourceOld":"  protected void constructStreams() throws IOException {\n\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Set<String> liveNodes = clusterState.getLiveNodes();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          if(replica.getState() == Replica.State.ACTIVE && liveNodes.contains(replica.getNodeName()))\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        HashMap params = new HashMap();\n        params.put(\"distrib\",\"false\"); // We are the aggregator.\n        params.put(\"numWorkers\", workers);\n        params.put(\"workerID\", w);\n        params.put(\"expr\", pushStream);\n        params.put(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, params);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e66a459d38c1c4a2f97128433dab546f683a9fed","date":1462873476,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","sourceNew":"  protected void constructStreams() throws IOException {\n\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Set<String> liveNodes = clusterState.getLiveNodes();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          if(replica.getState() == Replica.State.ACTIVE && liveNodes.contains(replica.getNodeName()))\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        ModifiableSolrParams paramsLoc = new ModifiableSolrParams();\n        paramsLoc.set(\"distrib\",\"false\"); // We are the aggregator.\n        paramsLoc.set(\"numWorkers\", workers);\n        paramsLoc.set(\"workerID\", w);\n\n        paramsLoc.set(\"expr\", pushStream.toString());\n        paramsLoc.set(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, paramsLoc);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","sourceOld":"  protected void constructStreams() throws IOException {\n\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Set<String> liveNodes = clusterState.getLiveNodes();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          if(replica.getState() == Replica.State.ACTIVE && liveNodes.contains(replica.getNodeName()))\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        HashMap params = new HashMap();\n        params.put(\"distrib\",\"false\"); // We are the aggregator.\n        params.put(\"numWorkers\", workers);\n        params.put(\"workerID\", w);\n        params.put(\"expr\", pushStream);\n        params.put(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, params);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","sourceNew":"  protected void constructStreams() throws IOException {\n\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Set<String> liveNodes = clusterState.getLiveNodes();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          if(replica.getState() == Replica.State.ACTIVE && liveNodes.contains(replica.getNodeName()))\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        ModifiableSolrParams paramsLoc = new ModifiableSolrParams();\n        paramsLoc.set(\"distrib\",\"false\"); // We are the aggregator.\n        paramsLoc.set(\"numWorkers\", workers);\n        paramsLoc.set(\"workerID\", w);\n\n        paramsLoc.set(\"expr\", pushStream.toString());\n        paramsLoc.set(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, paramsLoc);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","sourceOld":"  protected void constructStreams() throws IOException {\n\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Set<String> liveNodes = clusterState.getLiveNodes();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          if(replica.getState() == Replica.State.ACTIVE && liveNodes.contains(replica.getNodeName()))\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        HashMap params = new HashMap();\n        params.put(\"distrib\",\"false\"); // We are the aggregator.\n        params.put(\"numWorkers\", workers);\n        params.put(\"workerID\", w);\n        params.put(\"expr\", pushStream);\n        params.put(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, params);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","sourceNew":"  protected void constructStreams() throws IOException {\n\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Set<String> liveNodes = clusterState.getLiveNodes();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          if(replica.getState() == Replica.State.ACTIVE && liveNodes.contains(replica.getNodeName()))\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        ModifiableSolrParams paramsLoc = new ModifiableSolrParams();\n        paramsLoc.set(\"distrib\",\"false\"); // We are the aggregator.\n        paramsLoc.set(\"numWorkers\", workers);\n        paramsLoc.set(\"workerID\", w);\n\n        paramsLoc.set(\"expr\", pushStream.toString());\n        paramsLoc.set(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, paramsLoc);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","sourceOld":"  protected void constructStreams() throws IOException {\n\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Set<String> liveNodes = clusterState.getLiveNodes();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          if(replica.getState() == Replica.State.ACTIVE && liveNodes.contains(replica.getNodeName()))\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        HashMap params = new HashMap();\n        params.put(\"distrib\",\"false\"); // We are the aggregator.\n        params.put(\"numWorkers\", workers);\n        params.put(\"workerID\", w);\n        params.put(\"expr\", pushStream);\n        params.put(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, params);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8c969f15cd04d31e520319c619a445ae21f02d72","date":1479263638,"type":3,"author":"Kevin Risden","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","sourceNew":"  protected void constructStreams() throws IOException {\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n\n      Collection<Slice> slices = CloudSolrStream.getSlices(this.collection, zkStateReader, true);\n\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Set<String> liveNodes = clusterState.getLiveNodes();\n\n      List<Replica> shuffler = new ArrayList<>();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          if(replica.getState() == Replica.State.ACTIVE && liveNodes.contains(replica.getNodeName()))\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        ModifiableSolrParams paramsLoc = new ModifiableSolrParams();\n        paramsLoc.set(\"distrib\",\"false\"); // We are the aggregator.\n        paramsLoc.set(\"numWorkers\", workers);\n        paramsLoc.set(\"workerID\", w);\n\n        paramsLoc.set(\"expr\", pushStream.toString());\n        paramsLoc.set(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, paramsLoc);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","sourceOld":"  protected void constructStreams() throws IOException {\n\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Set<String> liveNodes = clusterState.getLiveNodes();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          if(replica.getState() == Replica.State.ACTIVE && liveNodes.contains(replica.getNodeName()))\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        ModifiableSolrParams paramsLoc = new ModifiableSolrParams();\n        paramsLoc.set(\"distrib\",\"false\"); // We are the aggregator.\n        paramsLoc.set(\"numWorkers\", workers);\n        paramsLoc.set(\"workerID\", w);\n\n        paramsLoc.set(\"expr\", pushStream.toString());\n        paramsLoc.set(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, paramsLoc);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a1ef55e1fff7ff44354432770ad8bc19be1fcc75","date":1479266056,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","sourceNew":"  protected void constructStreams() throws IOException {\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n\n      Collection<Slice> slices = CloudSolrStream.getSlices(this.collection, zkStateReader, true);\n\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Set<String> liveNodes = clusterState.getLiveNodes();\n\n      List<Replica> shuffler = new ArrayList<>();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          if(replica.getState() == Replica.State.ACTIVE && liveNodes.contains(replica.getNodeName()))\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        ModifiableSolrParams paramsLoc = new ModifiableSolrParams();\n        paramsLoc.set(\"distrib\",\"false\"); // We are the aggregator.\n        paramsLoc.set(\"numWorkers\", workers);\n        paramsLoc.set(\"workerID\", w);\n\n        paramsLoc.set(\"expr\", pushStream.toString());\n        paramsLoc.set(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, paramsLoc);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","sourceOld":"  protected void constructStreams() throws IOException {\n\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Set<String> liveNodes = clusterState.getLiveNodes();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          if(replica.getState() == Replica.State.ACTIVE && liveNodes.contains(replica.getNodeName()))\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        ModifiableSolrParams paramsLoc = new ModifiableSolrParams();\n        paramsLoc.set(\"distrib\",\"false\"); // We are the aggregator.\n        paramsLoc.set(\"numWorkers\", workers);\n        paramsLoc.set(\"workerID\", w);\n\n        paramsLoc.set(\"expr\", pushStream.toString());\n        paramsLoc.set(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, paramsLoc);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"01624b85de12fb02335810bdf325124e59040772","date":1490254940,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","sourceNew":"  protected void constructStreams() throws IOException {\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n\n      Collection<Slice> slices = CloudSolrStream.getSlices(this.collection, zkStateReader, true);\n\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Set<String> liveNodes = clusterState.getLiveNodes();\n\n      List<Replica> shuffler = new ArrayList<>();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          if(replica.getState() == Replica.State.ACTIVE && liveNodes.contains(replica.getNodeName()))\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        ModifiableSolrParams paramsLoc = new ModifiableSolrParams();\n        paramsLoc.set(DISTRIB,\"false\"); // We are the aggregator.\n        paramsLoc.set(\"numWorkers\", workers);\n        paramsLoc.set(\"workerID\", w);\n\n        paramsLoc.set(\"expr\", pushStream.toString());\n        paramsLoc.set(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, paramsLoc);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","sourceOld":"  protected void constructStreams() throws IOException {\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n\n      Collection<Slice> slices = CloudSolrStream.getSlices(this.collection, zkStateReader, true);\n\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Set<String> liveNodes = clusterState.getLiveNodes();\n\n      List<Replica> shuffler = new ArrayList<>();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          if(replica.getState() == Replica.State.ACTIVE && liveNodes.contains(replica.getNodeName()))\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        ModifiableSolrParams paramsLoc = new ModifiableSolrParams();\n        paramsLoc.set(\"distrib\",\"false\"); // We are the aggregator.\n        paramsLoc.set(\"numWorkers\", workers);\n        paramsLoc.set(\"workerID\", w);\n\n        paramsLoc.set(\"expr\", pushStream.toString());\n        paramsLoc.set(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, paramsLoc);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6f4c5d3859373c3a74734e85efa122b17514e3e8","date":1490280013,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","sourceNew":"  protected void constructStreams() throws IOException {\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n\n      Collection<Slice> slices = CloudSolrStream.getSlices(this.collection, zkStateReader, true);\n\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Set<String> liveNodes = clusterState.getLiveNodes();\n\n      List<Replica> shuffler = new ArrayList<>();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          if(replica.getState() == Replica.State.ACTIVE && liveNodes.contains(replica.getNodeName()))\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        ModifiableSolrParams paramsLoc = new ModifiableSolrParams();\n        paramsLoc.set(DISTRIB,\"false\"); // We are the aggregator.\n        paramsLoc.set(\"numWorkers\", workers);\n        paramsLoc.set(\"workerID\", w);\n\n        paramsLoc.set(\"expr\", pushStream.toString());\n        paramsLoc.set(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, paramsLoc);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","sourceOld":"  protected void constructStreams() throws IOException {\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n\n      Collection<Slice> slices = CloudSolrStream.getSlices(this.collection, zkStateReader, true);\n\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Set<String> liveNodes = clusterState.getLiveNodes();\n\n      List<Replica> shuffler = new ArrayList<>();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          if(replica.getState() == Replica.State.ACTIVE && liveNodes.contains(replica.getNodeName()))\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        ModifiableSolrParams paramsLoc = new ModifiableSolrParams();\n        paramsLoc.set(\"distrib\",\"false\"); // We are the aggregator.\n        paramsLoc.set(\"numWorkers\", workers);\n        paramsLoc.set(\"workerID\", w);\n\n        paramsLoc.set(\"expr\", pushStream.toString());\n        paramsLoc.set(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, paramsLoc);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c405288c4553ffb50ab8ca5adbdde9881bcec4e4","date":1491938682,"type":3,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","sourceNew":"  protected void constructStreams() throws IOException {\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      List<String> shardUrls = getShards(this.zkHost, this.collection, this.streamContext);\n\n      for(int w=0; w<workers; w++) {\n        ModifiableSolrParams paramsLoc = new ModifiableSolrParams();\n        paramsLoc.set(DISTRIB,\"false\"); // We are the aggregator.\n        paramsLoc.set(\"numWorkers\", workers);\n        paramsLoc.set(\"workerID\", w);\n\n        paramsLoc.set(\"expr\", pushStream.toString());\n        paramsLoc.set(\"qt\",\"/stream\");\n\n        String url = shardUrls.get(w);\n        SolrStream solrStream = new SolrStream(url, paramsLoc);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","sourceOld":"  protected void constructStreams() throws IOException {\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n\n      Collection<Slice> slices = CloudSolrStream.getSlices(this.collection, zkStateReader, true);\n\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Set<String> liveNodes = clusterState.getLiveNodes();\n\n      List<Replica> shuffler = new ArrayList<>();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          if(replica.getState() == Replica.State.ACTIVE && liveNodes.contains(replica.getNodeName()))\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        ModifiableSolrParams paramsLoc = new ModifiableSolrParams();\n        paramsLoc.set(DISTRIB,\"false\"); // We are the aggregator.\n        paramsLoc.set(\"numWorkers\", workers);\n        paramsLoc.set(\"workerID\", w);\n\n        paramsLoc.set(\"expr\", pushStream.toString());\n        paramsLoc.set(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, paramsLoc);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54ca69905c5d9d1529286f06ab1d12c68f6c13cb","date":1492683554,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","sourceNew":"  protected void constructStreams() throws IOException {\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      List<String> shardUrls = getShards(this.zkHost, this.collection, this.streamContext);\n\n      for(int w=0; w<workers; w++) {\n        ModifiableSolrParams paramsLoc = new ModifiableSolrParams();\n        paramsLoc.set(DISTRIB,\"false\"); // We are the aggregator.\n        paramsLoc.set(\"numWorkers\", workers);\n        paramsLoc.set(\"workerID\", w);\n\n        paramsLoc.set(\"expr\", pushStream.toString());\n        paramsLoc.set(\"qt\",\"/stream\");\n\n        String url = shardUrls.get(w);\n        SolrStream solrStream = new SolrStream(url, paramsLoc);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","sourceOld":"  protected void constructStreams() throws IOException {\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n\n      Collection<Slice> slices = CloudSolrStream.getSlices(this.collection, zkStateReader, true);\n\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Set<String> liveNodes = clusterState.getLiveNodes();\n\n      List<Replica> shuffler = new ArrayList<>();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          if(replica.getState() == Replica.State.ACTIVE && liveNodes.contains(replica.getNodeName()))\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random());\n\n      for(int w=0; w<workers; w++) {\n        ModifiableSolrParams paramsLoc = new ModifiableSolrParams();\n        paramsLoc.set(DISTRIB,\"false\"); // We are the aggregator.\n        paramsLoc.set(\"numWorkers\", workers);\n        paramsLoc.set(\"workerID\", w);\n\n        paramsLoc.set(\"expr\", pushStream.toString());\n        paramsLoc.set(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, paramsLoc);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8ec104cc4094eef1734dd925e460640b4dfee879","date":1499388859,"type":3,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","sourceNew":"  protected void constructStreams() throws IOException {\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      List<String> shardUrls = getShards(this.zkHost, this.collection, this.streamContext);\n\n      for(int w=0; w<workers; w++) {\n        ModifiableSolrParams paramsLoc = new ModifiableSolrParams();\n        paramsLoc.set(DISTRIB,\"false\"); // We are the aggregator.\n        paramsLoc.set(\"numWorkers\", workers);\n        paramsLoc.set(\"workerID\", w);\n\n        paramsLoc.set(\"expr\", pushStream.toString());\n        paramsLoc.set(\"qt\",\"/stream\");\n\n        String url = shardUrls.get(w);\n        SolrStream solrStream = new SolrStream(url, paramsLoc);\n        solrStream.setStreamContext(streamContext);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","sourceOld":"  protected void constructStreams() throws IOException {\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      List<String> shardUrls = getShards(this.zkHost, this.collection, this.streamContext);\n\n      for(int w=0; w<workers; w++) {\n        ModifiableSolrParams paramsLoc = new ModifiableSolrParams();\n        paramsLoc.set(DISTRIB,\"false\"); // We are the aggregator.\n        paramsLoc.set(\"numWorkers\", workers);\n        paramsLoc.set(\"workerID\", w);\n\n        paramsLoc.set(\"expr\", pushStream.toString());\n        paramsLoc.set(\"qt\",\"/stream\");\n\n        String url = shardUrls.get(w);\n        SolrStream solrStream = new SolrStream(url, paramsLoc);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e73d8d559120669b47658108d818b637df5456ea","date":1499401413,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","sourceNew":"  protected void constructStreams() throws IOException {\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      List<String> shardUrls = getShards(this.zkHost, this.collection, this.streamContext);\n\n      for(int w=0; w<workers; w++) {\n        ModifiableSolrParams paramsLoc = new ModifiableSolrParams();\n        paramsLoc.set(DISTRIB,\"false\"); // We are the aggregator.\n        paramsLoc.set(\"numWorkers\", workers);\n        paramsLoc.set(\"workerID\", w);\n\n        paramsLoc.set(\"expr\", pushStream.toString());\n        paramsLoc.set(\"qt\",\"/stream\");\n\n        String url = shardUrls.get(w);\n        SolrStream solrStream = new SolrStream(url, paramsLoc);\n        solrStream.setStreamContext(streamContext);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","sourceOld":"  protected void constructStreams() throws IOException {\n    try {\n      Object pushStream = ((Expressible) tupleStream).toExpression(streamFactory);\n\n      List<String> shardUrls = getShards(this.zkHost, this.collection, this.streamContext);\n\n      for(int w=0; w<workers; w++) {\n        ModifiableSolrParams paramsLoc = new ModifiableSolrParams();\n        paramsLoc.set(DISTRIB,\"false\"); // We are the aggregator.\n        paramsLoc.set(\"numWorkers\", workers);\n        paramsLoc.set(\"workerID\", w);\n\n        paramsLoc.set(\"expr\", pushStream.toString());\n        paramsLoc.set(\"qt\",\"/stream\");\n\n        String url = shardUrls.get(w);\n        SolrStream solrStream = new SolrStream(url, paramsLoc);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"58b0ac98a4f0daf02f6400e863ed311c169630c6":["c3cf1396610fbacfdf69deb27bc5d3f36b5fbd43"],"f00f1c5fad501b66705121feb623f8cfbb6712f9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"54ca69905c5d9d1529286f06ab1d12c68f6c13cb":["6f4c5d3859373c3a74734e85efa122b17514e3e8"],"8ec104cc4094eef1734dd925e460640b4dfee879":["c405288c4553ffb50ab8ca5adbdde9881bcec4e4"],"c3cf1396610fbacfdf69deb27bc5d3f36b5fbd43":["bcf9886c8ff537aafde14de48ebf744f5673f08b"],"c405288c4553ffb50ab8ca5adbdde9881bcec4e4":["01624b85de12fb02335810bdf325124e59040772"],"6f4c5d3859373c3a74734e85efa122b17514e3e8":["8c969f15cd04d31e520319c619a445ae21f02d72"],"01624b85de12fb02335810bdf325124e59040772":["8c969f15cd04d31e520319c619a445ae21f02d72"],"e431b0806b8db369da896b66a74e304a49953c7b":["f00f1c5fad501b66705121feb623f8cfbb6712f9"],"826d15444ddf61716dc768c229cd54b2c2ccce1c":["ad0d3af10297301b05f967ec10f31cbd376bdea3"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["ad0d3af10297301b05f967ec10f31cbd376bdea3","d470c8182e92b264680e34081b75e70a9f2b3c89"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["ad0d3af10297301b05f967ec10f31cbd376bdea3","826d15444ddf61716dc768c229cd54b2c2ccce1c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a1ef55e1fff7ff44354432770ad8bc19be1fcc75":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","8c969f15cd04d31e520319c619a445ae21f02d72"],"ad0d3af10297301b05f967ec10f31cbd376bdea3":["58b0ac98a4f0daf02f6400e863ed311c169630c6"],"e73d8d559120669b47658108d818b637df5456ea":["54ca69905c5d9d1529286f06ab1d12c68f6c13cb","8ec104cc4094eef1734dd925e460640b4dfee879"],"bcf9886c8ff537aafde14de48ebf744f5673f08b":["e431b0806b8db369da896b66a74e304a49953c7b"],"e66a459d38c1c4a2f97128433dab546f683a9fed":["ad0d3af10297301b05f967ec10f31cbd376bdea3","826d15444ddf61716dc768c229cd54b2c2ccce1c"],"8c969f15cd04d31e520319c619a445ae21f02d72":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["8ec104cc4094eef1734dd925e460640b4dfee879"]},"commit2Childs":{"58b0ac98a4f0daf02f6400e863ed311c169630c6":["ad0d3af10297301b05f967ec10f31cbd376bdea3"],"f00f1c5fad501b66705121feb623f8cfbb6712f9":["e431b0806b8db369da896b66a74e304a49953c7b"],"54ca69905c5d9d1529286f06ab1d12c68f6c13cb":["e73d8d559120669b47658108d818b637df5456ea"],"8ec104cc4094eef1734dd925e460640b4dfee879":["e73d8d559120669b47658108d818b637df5456ea","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c3cf1396610fbacfdf69deb27bc5d3f36b5fbd43":["58b0ac98a4f0daf02f6400e863ed311c169630c6"],"c405288c4553ffb50ab8ca5adbdde9881bcec4e4":["8ec104cc4094eef1734dd925e460640b4dfee879"],"6f4c5d3859373c3a74734e85efa122b17514e3e8":["54ca69905c5d9d1529286f06ab1d12c68f6c13cb"],"01624b85de12fb02335810bdf325124e59040772":["c405288c4553ffb50ab8ca5adbdde9881bcec4e4"],"e431b0806b8db369da896b66a74e304a49953c7b":["bcf9886c8ff537aafde14de48ebf744f5673f08b"],"826d15444ddf61716dc768c229cd54b2c2ccce1c":["d470c8182e92b264680e34081b75e70a9f2b3c89","e66a459d38c1c4a2f97128433dab546f683a9fed"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a1ef55e1fff7ff44354432770ad8bc19be1fcc75"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","8c969f15cd04d31e520319c619a445ae21f02d72"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f00f1c5fad501b66705121feb623f8cfbb6712f9"],"a1ef55e1fff7ff44354432770ad8bc19be1fcc75":[],"ad0d3af10297301b05f967ec10f31cbd376bdea3":["826d15444ddf61716dc768c229cd54b2c2ccce1c","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d470c8182e92b264680e34081b75e70a9f2b3c89","e66a459d38c1c4a2f97128433dab546f683a9fed"],"e73d8d559120669b47658108d818b637df5456ea":[],"bcf9886c8ff537aafde14de48ebf744f5673f08b":["c3cf1396610fbacfdf69deb27bc5d3f36b5fbd43"],"e66a459d38c1c4a2f97128433dab546f683a9fed":[],"8c969f15cd04d31e520319c619a445ae21f02d72":["6f4c5d3859373c3a74734e85efa122b17514e3e8","01624b85de12fb02335810bdf325124e59040772","a1ef55e1fff7ff44354432770ad8bc19be1fcc75"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a1ef55e1fff7ff44354432770ad8bc19be1fcc75","e73d8d559120669b47658108d818b637df5456ea","e66a459d38c1c4a2f97128433dab546f683a9fed","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}