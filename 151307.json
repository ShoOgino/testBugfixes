{"path":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter#merge(MergeState).mjava","commits":[{"id":"cab7a79353f33d1a94cd307bf33aa5148601ebe6","date":1453391888,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60DimensionalWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointReader reader : mergeState.pointReaders) {\n      if (reader instanceof Lucene60PointReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointReaders.length;i++) {\n              PointReader reader = mergeState.pointReaders[i];\n\n              Lucene60PointReader reader60 = (Lucene60PointReader) reader;\n              if (reader60 != null) {\n                // TODO: I could just use the merged fieldInfo.number instead of resolving to this\n                // reader's FieldInfo, right?  Field numbers are always consistent across segments,\n                // since when?\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    } \n  }  \n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(DimensionalReader reader : mergeState.dimensionalReaders) {\n      if (reader instanceof Lucene60DimensionalReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getDimensionCount() != 0) {\n        if (fieldInfo.getDimensionCount() == 1) {\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getDimensionCount(),\n                                                fieldInfo.getDimensionNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.dimensionalReaders.length;i++) {\n              DimensionalReader reader = mergeState.dimensionalReaders[i];\n\n              Lucene60DimensionalReader reader60 = (Lucene60DimensionalReader) reader;\n              if (reader60 != null) {\n                // TODO: I could just use the merged fieldInfo.number instead of resolving to this\n                // reader's FieldInfo, right?  Field numbers are always consistent across segments,\n                // since when?\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    } \n  }  \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"85ca0e073c286ebb2c89364ada6dd2740fc18880","date":1453996944,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointReader reader : mergeState.pointReaders) {\n      if (reader instanceof Lucene60PointReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointReaders.length;i++) {\n              PointReader reader = mergeState.pointReaders[i];\n\n              Lucene60PointReader reader60 = (Lucene60PointReader) reader;\n              if (reader60 != null) {\n                // TODO: I could just use the merged fieldInfo.number instead of resolving to this\n                // reader's FieldInfo, right?  Field numbers are always consistent across segments,\n                // since when?\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointReader reader : mergeState.pointReaders) {\n      if (reader instanceof Lucene60PointReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointReaders.length;i++) {\n              PointReader reader = mergeState.pointReaders[i];\n\n              Lucene60PointReader reader60 = (Lucene60PointReader) reader;\n              if (reader60 != null) {\n                // TODO: I could just use the merged fieldInfo.number instead of resolving to this\n                // reader's FieldInfo, right?  Field numbers are always consistent across segments,\n                // since when?\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    } \n  }  \n\n","bugFix":["1786be6a11f9cf5e48ce84869d1bb71e9c02f966"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"07ef6935eaaefebe92999fbdd19780d4af12bedf","date":1454066190,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointReader reader : mergeState.pointReaders) {\n      if (reader instanceof Lucene60PointReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointReaders.length;i++) {\n              PointReader reader = mergeState.pointReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointReader;\n                Lucene60PointReader reader60 = (Lucene60PointReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointReader reader : mergeState.pointReaders) {\n      if (reader instanceof Lucene60PointReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointReaders.length;i++) {\n              PointReader reader = mergeState.pointReaders[i];\n\n              Lucene60PointReader reader60 = (Lucene60PointReader) reader;\n              if (reader60 != null) {\n                // TODO: I could just use the merged fieldInfo.number instead of resolving to this\n                // reader's FieldInfo, right?  Field numbers are always consistent across segments,\n                // since when?\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8d15e34266d75e4e8b95da046cd0afc812367b38","date":1454246129,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointReader reader : mergeState.pointReaders) {\n      if (reader instanceof Lucene60PointReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointReaders.length;i++) {\n              PointReader reader = mergeState.pointReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointReader;\n                Lucene60PointReader reader60 = (Lucene60PointReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointReader reader : mergeState.pointReaders) {\n      if (reader instanceof Lucene60PointReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointReaders.length;i++) {\n              PointReader reader = mergeState.pointReaders[i];\n\n              Lucene60PointReader reader60 = (Lucene60PointReader) reader;\n              if (reader60 != null) {\n                // TODO: I could just use the merged fieldInfo.number instead of resolving to this\n                // reader's FieldInfo, right?  Field numbers are always consistent across segments,\n                // since when?\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    } \n  }  \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1e6acbaae7af722f17204ceccf0f7db5753eccf3","date":1454775255,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointReader reader : mergeState.pointReaders) {\n      if (reader instanceof Lucene60PointReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointReaders.length;i++) {\n              PointReader reader = mergeState.pointReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointReader;\n                Lucene60PointReader reader60 = (Lucene60PointReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointReader reader : mergeState.pointReaders) {\n      if (reader instanceof Lucene60PointReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointReaders.length;i++) {\n              PointReader reader = mergeState.pointReaders[i];\n\n              Lucene60PointReader reader60 = (Lucene60PointReader) reader;\n              if (reader60 != null) {\n                // TODO: I could just use the merged fieldInfo.number instead of resolving to this\n                // reader's FieldInfo, right?  Field numbers are always consistent across segments,\n                // since when?\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    } \n  }  \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"251c5b33f0a2c8988550b63c78ed22b0e84524e5","date":1456961997,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointReader reader : mergeState.pointReaders) {\n      if (reader instanceof Lucene60PointReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointReaders.length;i++) {\n              PointReader reader = mergeState.pointReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointReader;\n                Lucene60PointReader reader60 = (Lucene60PointReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointReader reader : mergeState.pointReaders) {\n      if (reader instanceof Lucene60PointReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointReaders.length;i++) {\n              PointReader reader = mergeState.pointReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointReader;\n                Lucene60PointReader reader60 = (Lucene60PointReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cf1a614098b46c9c22afebd7b898ae4d1d2fc273","date":1457088850,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointReader reader : mergeState.pointReaders) {\n      if (reader instanceof Lucene60PointReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointReaders.length;i++) {\n              PointReader reader = mergeState.pointReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointReader;\n                Lucene60PointReader reader60 = (Lucene60PointReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointReader reader : mergeState.pointReaders) {\n      if (reader instanceof Lucene60PointReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointReaders.length;i++) {\n              PointReader reader = mergeState.pointReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointReader;\n                Lucene60PointReader reader60 = (Lucene60PointReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"11a4140b15efc64fe1f3dc86b79679d3474add50","date":1457130322,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointReader reader : mergeState.pointReaders) {\n      if (reader instanceof Lucene60PointReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointReader reader : mergeState.pointReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointReaders.length;i++) {\n              PointReader reader = mergeState.pointReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointReader;\n                Lucene60PointReader reader60 = (Lucene60PointReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointReader reader : mergeState.pointReaders) {\n      if (reader instanceof Lucene60PointReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointReaders.length;i++) {\n              PointReader reader = mergeState.pointReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointReader;\n                Lucene60PointReader reader60 = (Lucene60PointReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"922ddd897402a6df25c766ea8300443be5e82b3d","date":1457157606,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointReader reader : mergeState.pointReaders) {\n      if (reader instanceof Lucene60PointReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointReader reader : mergeState.pointReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointReaders.length;i++) {\n              PointReader reader = mergeState.pointReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointReader;\n                Lucene60PointReader reader60 = (Lucene60PointReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointReader reader : mergeState.pointReaders) {\n      if (reader instanceof Lucene60PointReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointReaders.length;i++) {\n              PointReader reader = mergeState.pointReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointReader;\n                Lucene60PointReader reader60 = (Lucene60PointReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4522ffca5a1f420c6a02198c9332d7c596a30ca5","date":1457270822,"type":5,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointReader reader : mergeState.pointReaders) {\n      if (reader instanceof Lucene60PointReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointReader reader : mergeState.pointReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointReaders.length;i++) {\n              PointReader reader = mergeState.pointReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointReader;\n                Lucene60PointReader reader60 = (Lucene60PointReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"251c5b33f0a2c8988550b63c78ed22b0e84524e5":["8d15e34266d75e4e8b95da046cd0afc812367b38"],"11a4140b15efc64fe1f3dc86b79679d3474add50":["cf1a614098b46c9c22afebd7b898ae4d1d2fc273"],"4522ffca5a1f420c6a02198c9332d7c596a30ca5":["922ddd897402a6df25c766ea8300443be5e82b3d"],"cab7a79353f33d1a94cd307bf33aa5148601ebe6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"922ddd897402a6df25c766ea8300443be5e82b3d":["cf1a614098b46c9c22afebd7b898ae4d1d2fc273","11a4140b15efc64fe1f3dc86b79679d3474add50"],"07ef6935eaaefebe92999fbdd19780d4af12bedf":["85ca0e073c286ebb2c89364ada6dd2740fc18880"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":["cab7a79353f33d1a94cd307bf33aa5148601ebe6","8d15e34266d75e4e8b95da046cd0afc812367b38"],"85ca0e073c286ebb2c89364ada6dd2740fc18880":["cab7a79353f33d1a94cd307bf33aa5148601ebe6"],"cf1a614098b46c9c22afebd7b898ae4d1d2fc273":["8d15e34266d75e4e8b95da046cd0afc812367b38","251c5b33f0a2c8988550b63c78ed22b0e84524e5"],"8d15e34266d75e4e8b95da046cd0afc812367b38":["cab7a79353f33d1a94cd307bf33aa5148601ebe6","07ef6935eaaefebe92999fbdd19780d4af12bedf"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["4522ffca5a1f420c6a02198c9332d7c596a30ca5"]},"commit2Childs":{"251c5b33f0a2c8988550b63c78ed22b0e84524e5":["cf1a614098b46c9c22afebd7b898ae4d1d2fc273"],"11a4140b15efc64fe1f3dc86b79679d3474add50":["922ddd897402a6df25c766ea8300443be5e82b3d"],"4522ffca5a1f420c6a02198c9332d7c596a30ca5":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cab7a79353f33d1a94cd307bf33aa5148601ebe6":["1e6acbaae7af722f17204ceccf0f7db5753eccf3","85ca0e073c286ebb2c89364ada6dd2740fc18880","8d15e34266d75e4e8b95da046cd0afc812367b38"],"922ddd897402a6df25c766ea8300443be5e82b3d":["4522ffca5a1f420c6a02198c9332d7c596a30ca5"],"07ef6935eaaefebe92999fbdd19780d4af12bedf":["8d15e34266d75e4e8b95da046cd0afc812367b38"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cab7a79353f33d1a94cd307bf33aa5148601ebe6"],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":[],"85ca0e073c286ebb2c89364ada6dd2740fc18880":["07ef6935eaaefebe92999fbdd19780d4af12bedf"],"cf1a614098b46c9c22afebd7b898ae4d1d2fc273":["11a4140b15efc64fe1f3dc86b79679d3474add50","922ddd897402a6df25c766ea8300443be5e82b3d"],"8d15e34266d75e4e8b95da046cd0afc812367b38":["251c5b33f0a2c8988550b63c78ed22b0e84524e5","1e6acbaae7af722f17204ceccf0f7db5753eccf3","cf1a614098b46c9c22afebd7b898ae4d1d2fc273"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["1e6acbaae7af722f17204ceccf0f7db5753eccf3","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}