{"path":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#applyDeletes(IndexReader,int).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#applyDeletes(IndexReader,int).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter#applyDeletes(IndexReader,int).mjava","sourceNew":"  // Apply buffered delete terms, queries and docIDs to the\n  // provided reader\n  private final synchronized boolean applyDeletes(IndexReader reader, int docIDStart)\n    throws CorruptIndexException, IOException {\n\n    final int docEnd = docIDStart + reader.maxDoc();\n    boolean any = false;\n\n    assert checkDeleteTerm(null);\n\n    // Delete by term\n    if (deletesFlushed.terms.size() > 0) {\n      TermDocs docs = reader.termDocs();\n      try {\n        for (Entry<Term, BufferedDeletes.Num> entry: deletesFlushed.terms.entrySet()) {\n          Term term = entry.getKey();\n          // LUCENE-2086: we should be iterating a TreeMap,\n          // here, so terms better be in order:\n          assert checkDeleteTerm(term);\n          docs.seek(term);\n          int limit = entry.getValue().getNum();\n          while (docs.next()) {\n            int docID = docs.doc();\n            if (docIDStart+docID >= limit)\n              break;\n            reader.deleteDocument(docID);\n            any = true;\n          }\n        }\n      } finally {\n        docs.close();\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletesFlushed.docIDs) {\n      int docID = docIdInt.intValue();\n      if (docID >= docIDStart && docID < docEnd) {\n        reader.deleteDocument(docID-docIDStart);\n        any = true;\n      }\n    }\n\n    // Delete by query\n    if (deletesFlushed.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletesFlushed.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (((long) docIDStart) + doc >= limit)\n                break;\n              reader.deleteDocument(doc);\n              any = true;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n    return any;\n  }\n\n","sourceOld":"  // Apply buffered delete terms, queries and docIDs to the\n  // provided reader\n  private final synchronized boolean applyDeletes(IndexReader reader, int docIDStart)\n    throws CorruptIndexException, IOException {\n\n    final int docEnd = docIDStart + reader.maxDoc();\n    boolean any = false;\n\n    assert checkDeleteTerm(null);\n\n    // Delete by term\n    if (deletesFlushed.terms.size() > 0) {\n      TermDocs docs = reader.termDocs();\n      try {\n        for (Entry<Term, BufferedDeletes.Num> entry: deletesFlushed.terms.entrySet()) {\n          Term term = entry.getKey();\n          // LUCENE-2086: we should be iterating a TreeMap,\n          // here, so terms better be in order:\n          assert checkDeleteTerm(term);\n          docs.seek(term);\n          int limit = entry.getValue().getNum();\n          while (docs.next()) {\n            int docID = docs.doc();\n            if (docIDStart+docID >= limit)\n              break;\n            reader.deleteDocument(docID);\n            any = true;\n          }\n        }\n      } finally {\n        docs.close();\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletesFlushed.docIDs) {\n      int docID = docIdInt.intValue();\n      if (docID >= docIDStart && docID < docEnd) {\n        reader.deleteDocument(docID-docIDStart);\n        any = true;\n      }\n    }\n\n    // Delete by query\n    if (deletesFlushed.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletesFlushed.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (((long) docIDStart) + doc >= limit)\n                break;\n              reader.deleteDocument(doc);\n              any = true;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n    return any;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"955c32f886db6f6356c9fcdea6b1f1cb4effda24","date":1270581567,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#applyDeletes(IndexReader,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#applyDeletes(IndexReader,int).mjava","sourceNew":"  // Apply buffered delete terms, queries and docIDs to the\n  // provided reader\n  private final synchronized boolean applyDeletes(IndexReader reader, int docIDStart)\n    throws CorruptIndexException, IOException {\n\n    final int docEnd = docIDStart + reader.maxDoc();\n    boolean any = false;\n\n    assert checkDeleteTerm(null);\n\n    // Delete by term\n    if (deletesFlushed.terms.size() > 0) {\n      try {\n        Fields fields = reader.fields();\n        TermsEnum termsEnum = null;\n        \n        String currentField = null;\n        BytesRef termRef = new BytesRef();\n        DocsEnum docs = null;\n        \n        for (Entry<Term, BufferedDeletes.Num> entry: deletesFlushed.terms.entrySet()) {\n          Term term = entry.getKey();\n          // Since we visit terms sorted, we gain performance\n          // by re-using the same TermsEnum and seeking only\n          // forwards\n          if (term.field() != currentField) {\n            assert currentField == null || currentField.compareTo(term.field()) < 0;\n            currentField = term.field();\n            Terms terms = fields.terms(currentField);\n            if (terms != null) {\n              termsEnum = terms.iterator();\n            } else {\n              termsEnum = null;\n            }\n          }\n          \n          if (termsEnum == null) {\n            continue;\n          }\n          assert checkDeleteTerm(term);\n          \n          termRef.copy(term.text());\n          \n          if (termsEnum.seek(termRef, false) == TermsEnum.SeekStatus.FOUND) {\n            DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n            if (docsEnum != null) {\n              docs = docsEnum;\n              int limit = entry.getValue().getNum();\n              while (true) {\n                final int docID = docs.nextDoc();\n                if (docID == DocsEnum.NO_MORE_DOCS || docIDStart+docID >= limit) {\n                  break;\n                }\n                reader.deleteDocument(docID);\n                any = true;\n              }\n            }\n          }\n        }\n      } finally {\n        //docs.close();\n      }\n    }\n    // Delete by docID\n    for (Integer docIdInt : deletesFlushed.docIDs) {\n      int docID = docIdInt.intValue();\n      if (docID >= docIDStart && docID < docEnd) {\n        reader.deleteDocument(docID-docIDStart);\n        any = true;\n      }\n    }\n\n    // Delete by query\n    if (deletesFlushed.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletesFlushed.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (((long) docIDStart) + doc >= limit)\n                break;\n              reader.deleteDocument(doc);\n              any = true;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n    return any;\n  }\n\n","sourceOld":"  // Apply buffered delete terms, queries and docIDs to the\n  // provided reader\n  private final synchronized boolean applyDeletes(IndexReader reader, int docIDStart)\n    throws CorruptIndexException, IOException {\n\n    final int docEnd = docIDStart + reader.maxDoc();\n    boolean any = false;\n\n    assert checkDeleteTerm(null);\n\n    // Delete by term\n    if (deletesFlushed.terms.size() > 0) {\n      TermDocs docs = reader.termDocs();\n      try {\n        for (Entry<Term, BufferedDeletes.Num> entry: deletesFlushed.terms.entrySet()) {\n          Term term = entry.getKey();\n          // LUCENE-2086: we should be iterating a TreeMap,\n          // here, so terms better be in order:\n          assert checkDeleteTerm(term);\n          docs.seek(term);\n          int limit = entry.getValue().getNum();\n          while (docs.next()) {\n            int docID = docs.doc();\n            if (docIDStart+docID >= limit)\n              break;\n            reader.deleteDocument(docID);\n            any = true;\n          }\n        }\n      } finally {\n        docs.close();\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletesFlushed.docIDs) {\n      int docID = docIdInt.intValue();\n      if (docID >= docIDStart && docID < docEnd) {\n        reader.deleteDocument(docID-docIDStart);\n        any = true;\n      }\n    }\n\n    // Delete by query\n    if (deletesFlushed.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletesFlushed.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (((long) docIDStart) + doc >= limit)\n                break;\n              reader.deleteDocument(doc);\n              any = true;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n    return any;\n  }\n\n","bugFix":null,"bugIntro":["b43b719dab44d1ccc5ee5b6e01c50f1ee86bb76c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b43b719dab44d1ccc5ee5b6e01c50f1ee86bb76c","date":1270671893,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#applyDeletes(IndexReader,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#applyDeletes(IndexReader,int).mjava","sourceNew":"  // Apply buffered delete terms, queries and docIDs to the\n  // provided reader\n  private final synchronized boolean applyDeletes(IndexReader reader, int docIDStart)\n    throws CorruptIndexException, IOException {\n\n    final int docEnd = docIDStart + reader.maxDoc();\n    boolean any = false;\n\n    assert checkDeleteTerm(null);\n\n    // Delete by term\n    if (deletesFlushed.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return false;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      BytesRef termRef = new BytesRef();\n      DocsEnum docs = null;\n        \n      for (Entry<Term, BufferedDeletes.Num> entry: deletesFlushed.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        termRef.copy(term.text());\n          \n        if (termsEnum.seek(termRef, false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            int limit = entry.getValue().getNum();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docIDStart+docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              any = true;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletesFlushed.docIDs) {\n      int docID = docIdInt.intValue();\n      if (docID >= docIDStart && docID < docEnd) {\n        reader.deleteDocument(docID-docIDStart);\n        any = true;\n      }\n    }\n\n    // Delete by query\n    if (deletesFlushed.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletesFlushed.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (((long) docIDStart) + doc >= limit)\n                break;\n              reader.deleteDocument(doc);\n              any = true;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n    return any;\n  }\n\n","sourceOld":"  // Apply buffered delete terms, queries and docIDs to the\n  // provided reader\n  private final synchronized boolean applyDeletes(IndexReader reader, int docIDStart)\n    throws CorruptIndexException, IOException {\n\n    final int docEnd = docIDStart + reader.maxDoc();\n    boolean any = false;\n\n    assert checkDeleteTerm(null);\n\n    // Delete by term\n    if (deletesFlushed.terms.size() > 0) {\n      try {\n        Fields fields = reader.fields();\n        TermsEnum termsEnum = null;\n        \n        String currentField = null;\n        BytesRef termRef = new BytesRef();\n        DocsEnum docs = null;\n        \n        for (Entry<Term, BufferedDeletes.Num> entry: deletesFlushed.terms.entrySet()) {\n          Term term = entry.getKey();\n          // Since we visit terms sorted, we gain performance\n          // by re-using the same TermsEnum and seeking only\n          // forwards\n          if (term.field() != currentField) {\n            assert currentField == null || currentField.compareTo(term.field()) < 0;\n            currentField = term.field();\n            Terms terms = fields.terms(currentField);\n            if (terms != null) {\n              termsEnum = terms.iterator();\n            } else {\n              termsEnum = null;\n            }\n          }\n          \n          if (termsEnum == null) {\n            continue;\n          }\n          assert checkDeleteTerm(term);\n          \n          termRef.copy(term.text());\n          \n          if (termsEnum.seek(termRef, false) == TermsEnum.SeekStatus.FOUND) {\n            DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n            if (docsEnum != null) {\n              docs = docsEnum;\n              int limit = entry.getValue().getNum();\n              while (true) {\n                final int docID = docs.nextDoc();\n                if (docID == DocsEnum.NO_MORE_DOCS || docIDStart+docID >= limit) {\n                  break;\n                }\n                reader.deleteDocument(docID);\n                any = true;\n              }\n            }\n          }\n        }\n      } finally {\n        //docs.close();\n      }\n    }\n    // Delete by docID\n    for (Integer docIdInt : deletesFlushed.docIDs) {\n      int docID = docIdInt.intValue();\n      if (docID >= docIDStart && docID < docEnd) {\n        reader.deleteDocument(docID-docIDStart);\n        any = true;\n      }\n    }\n\n    // Delete by query\n    if (deletesFlushed.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletesFlushed.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (((long) docIDStart) + doc >= limit)\n                break;\n              reader.deleteDocument(doc);\n              any = true;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n    return any;\n  }\n\n","bugFix":["289ce46591c91edc14a0a068d1990324daf047a5","955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4f29ba80b723649f5feb7e37afe1a558dd2c1304","date":1278318805,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#applyDeletes(IndexReader,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#applyDeletes(IndexReader,int).mjava","sourceNew":"  // Apply buffered delete terms, queries and docIDs to the\n  // provided reader\n  private final synchronized boolean applyDeletes(IndexReader reader, int docIDStart)\n    throws CorruptIndexException, IOException {\n\n    final int docEnd = docIDStart + reader.maxDoc();\n    boolean any = false;\n\n    assert checkDeleteTerm(null);\n\n    // Delete by term\n    if (deletesFlushed.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return false;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term, BufferedDeletes.Num> entry: deletesFlushed.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            int limit = entry.getValue().getNum();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docIDStart+docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              any = true;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletesFlushed.docIDs) {\n      int docID = docIdInt.intValue();\n      if (docID >= docIDStart && docID < docEnd) {\n        reader.deleteDocument(docID-docIDStart);\n        any = true;\n      }\n    }\n\n    // Delete by query\n    if (deletesFlushed.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletesFlushed.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (((long) docIDStart) + doc >= limit)\n                break;\n              reader.deleteDocument(doc);\n              any = true;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n    return any;\n  }\n\n","sourceOld":"  // Apply buffered delete terms, queries and docIDs to the\n  // provided reader\n  private final synchronized boolean applyDeletes(IndexReader reader, int docIDStart)\n    throws CorruptIndexException, IOException {\n\n    final int docEnd = docIDStart + reader.maxDoc();\n    boolean any = false;\n\n    assert checkDeleteTerm(null);\n\n    // Delete by term\n    if (deletesFlushed.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return false;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      BytesRef termRef = new BytesRef();\n      DocsEnum docs = null;\n        \n      for (Entry<Term, BufferedDeletes.Num> entry: deletesFlushed.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        termRef.copy(term.text());\n          \n        if (termsEnum.seek(termRef, false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            int limit = entry.getValue().getNum();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docIDStart+docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              any = true;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletesFlushed.docIDs) {\n      int docID = docIdInt.intValue();\n      if (docID >= docIDStart && docID < docEnd) {\n        reader.deleteDocument(docID-docIDStart);\n        any = true;\n      }\n    }\n\n    // Delete by query\n    if (deletesFlushed.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletesFlushed.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (((long) docIDStart) + doc >= limit)\n                break;\n              reader.deleteDocument(doc);\n              any = true;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n    return any;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9b832cbed6eb3d54a8bb9339296bdda8eeb53014","date":1279708040,"type":4,"author":"Michael Busch","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#applyDeletes(IndexReader,int).mjava","sourceNew":null,"sourceOld":"  // Apply buffered delete terms, queries and docIDs to the\n  // provided reader\n  private final synchronized boolean applyDeletes(IndexReader reader, int docIDStart)\n    throws CorruptIndexException, IOException {\n\n    final int docEnd = docIDStart + reader.maxDoc();\n    boolean any = false;\n\n    assert checkDeleteTerm(null);\n\n    // Delete by term\n    if (deletesFlushed.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return false;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term, BufferedDeletes.Num> entry: deletesFlushed.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            int limit = entry.getValue().getNum();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docIDStart+docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              any = true;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletesFlushed.docIDs) {\n      int docID = docIdInt.intValue();\n      if (docID >= docIDStart && docID < docEnd) {\n        reader.deleteDocument(docID-docIDStart);\n        any = true;\n      }\n    }\n\n    // Delete by query\n    if (deletesFlushed.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletesFlushed.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (((long) docIDStart) + doc >= limit)\n                break;\n              reader.deleteDocument(doc);\n              any = true;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n    return any;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"334c1175813aea771a71728cd2c4ee4754fd0603","date":1279710173,"type":0,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#applyDeletes(IndexReader,int).mjava","pathOld":"/dev/null","sourceNew":"  // Apply buffered delete terms, queries and docIDs to the\n  // provided reader\n  private final synchronized boolean applyDeletes(IndexReader reader, int docIDStart)\n    throws CorruptIndexException, IOException {\n\n    final int docEnd = docIDStart + reader.maxDoc();\n    boolean any = false;\n\n    assert checkDeleteTerm(null);\n\n    // Delete by term\n    if (deletesFlushed.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return false;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term, BufferedDeletes.Num> entry: deletesFlushed.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            int limit = entry.getValue().getNum();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docIDStart+docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              any = true;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletesFlushed.docIDs) {\n      int docID = docIdInt.intValue();\n      if (docID >= docIDStart && docID < docEnd) {\n        reader.deleteDocument(docID-docIDStart);\n        any = true;\n      }\n    }\n\n    // Delete by query\n    if (deletesFlushed.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletesFlushed.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (((long) docIDStart) + doc >= limit)\n                break;\n              reader.deleteDocument(doc);\n              any = true;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n    return any;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8fe956d65251358d755c56f14fe8380644790e47","date":1279711318,"type":4,"author":"Michael Busch","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#applyDeletes(IndexReader,int).mjava","sourceNew":null,"sourceOld":"  // Apply buffered delete terms, queries and docIDs to the\n  // provided reader\n  private final synchronized boolean applyDeletes(IndexReader reader, int docIDStart)\n    throws CorruptIndexException, IOException {\n\n    final int docEnd = docIDStart + reader.maxDoc();\n    boolean any = false;\n\n    assert checkDeleteTerm(null);\n\n    // Delete by term\n    if (deletesFlushed.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return false;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      BytesRef termRef = new BytesRef();\n      DocsEnum docs = null;\n        \n      for (Entry<Term, BufferedDeletes.Num> entry: deletesFlushed.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        termRef.copy(term.text());\n          \n        if (termsEnum.seek(termRef, false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            int limit = entry.getValue().getNum();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docIDStart+docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              any = true;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletesFlushed.docIDs) {\n      int docID = docIdInt.intValue();\n      if (docID >= docIDStart && docID < docEnd) {\n        reader.deleteDocument(docID-docIDStart);\n        any = true;\n      }\n    }\n\n    // Delete by query\n    if (deletesFlushed.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletesFlushed.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (((long) docIDStart) + doc >= limit)\n                break;\n              reader.deleteDocument(doc);\n              any = true;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n    return any;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"163fe85a71d778fd2b7747f65ca27b54829e2e57","date":1279898785,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#applyDeletes(IndexReader,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#applyDeletes(IndexReader,int).mjava","sourceNew":"  // Apply buffered delete terms, queries and docIDs to the\n  // provided reader\n  private final synchronized boolean applyDeletes(IndexReader reader, int docIDStart)\n    throws CorruptIndexException, IOException {\n\n    final int docEnd = docIDStart + reader.maxDoc();\n    boolean any = false;\n\n    assert checkDeleteTerm(null);\n\n    // Delete by term\n    if (deletesFlushed.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return false;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term, BufferedDeletes.Num> entry: deletesFlushed.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n\n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            int limit = entry.getValue().getNum();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docIDStart+docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              any = true;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletesFlushed.docIDs) {\n      int docID = docIdInt.intValue();\n      if (docID >= docIDStart && docID < docEnd) {\n        reader.deleteDocument(docID-docIDStart);\n        any = true;\n      }\n    }\n\n    // Delete by query\n    if (deletesFlushed.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletesFlushed.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (((long) docIDStart) + doc >= limit)\n                break;\n              reader.deleteDocument(doc);\n              any = true;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n    return any;\n  }\n\n","sourceOld":"  // Apply buffered delete terms, queries and docIDs to the\n  // provided reader\n  private final synchronized boolean applyDeletes(IndexReader reader, int docIDStart)\n    throws CorruptIndexException, IOException {\n\n    final int docEnd = docIDStart + reader.maxDoc();\n    boolean any = false;\n\n    assert checkDeleteTerm(null);\n\n    // Delete by term\n    if (deletesFlushed.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return false;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term, BufferedDeletes.Num> entry: deletesFlushed.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            int limit = entry.getValue().getNum();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docIDStart+docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              any = true;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletesFlushed.docIDs) {\n      int docID = docIdInt.intValue();\n      if (docID >= docIDStart && docID < docEnd) {\n        reader.deleteDocument(docID-docIDStart);\n        any = true;\n      }\n    }\n\n    // Delete by query\n    if (deletesFlushed.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletesFlushed.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (((long) docIDStart) + doc >= limit)\n                break;\n              reader.deleteDocument(doc);\n              any = true;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n    return any;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"44fcbde6fb2ac44ee3b45e013e54a42911e689ff","date":1292065621,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/BufferedDeletes#applyDeletes(SegmentDeletes,SegmentReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#applyDeletes(IndexReader,int).mjava","sourceNew":"  private synchronized boolean applyDeletes(SegmentDeletes deletes, SegmentReader reader) throws IOException {\n    boolean any = false;\n\n    assert checkDeleteTerm(null);\n    \n    if (deletes.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return false;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term,Integer> entry: deletes.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            final int limit = entry.getValue();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              any = true;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletes.docIDs) {\n      int docID = docIdInt.intValue();\n      reader.deleteDocument(docID);\n      any = true;\n    }\n\n    // Delete by query\n    if (deletes.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletes.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (doc >= limit)\n                break;\n              reader.deleteDocument(doc);\n              any = true;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n    return any;\n  }\n\n","sourceOld":"  // Apply buffered delete terms, queries and docIDs to the\n  // provided reader\n  private final synchronized boolean applyDeletes(IndexReader reader, int docIDStart)\n    throws CorruptIndexException, IOException {\n\n    final int docEnd = docIDStart + reader.maxDoc();\n    boolean any = false;\n\n    assert checkDeleteTerm(null);\n\n    // Delete by term\n    if (deletesFlushed.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return false;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term, BufferedDeletes.Num> entry: deletesFlushed.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            int limit = entry.getValue().getNum();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docIDStart+docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              any = true;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletesFlushed.docIDs) {\n      int docID = docIdInt.intValue();\n      if (docID >= docIDStart && docID < docEnd) {\n        reader.deleteDocument(docID-docIDStart);\n        any = true;\n      }\n    }\n\n    // Delete by query\n    if (deletesFlushed.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletesFlushed.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (((long) docIDStart) + doc >= limit)\n                break;\n              reader.deleteDocument(doc);\n              any = true;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n    return any;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":5,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/BufferedDeletes#applyDeletes(SegmentDeletes,SegmentReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#applyDeletes(IndexReader,int).mjava","sourceNew":"  private synchronized long applyDeletes(SegmentDeletes deletes, SegmentReader reader) throws IOException {\n\n    long delCount = 0;\n\n    assert checkDeleteTerm(null);\n    \n    if (deletes.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return 0;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term,Integer> entry: deletes.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            final int limit = entry.getValue();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletes.docIDs) {\n      int docID = docIdInt.intValue();\n      reader.deleteDocument(docID);\n      delCount++;\n    }\n\n    // Delete by query\n    if (deletes.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletes.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (doc >= limit)\n                break;\n\n              reader.deleteDocument(doc);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  // Apply buffered delete terms, queries and docIDs to the\n  // provided reader\n  private final synchronized boolean applyDeletes(IndexReader reader, int docIDStart)\n    throws CorruptIndexException, IOException {\n\n    final int docEnd = docIDStart + reader.maxDoc();\n    boolean any = false;\n\n    assert checkDeleteTerm(null);\n\n    // Delete by term\n    if (deletesFlushed.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return false;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term, BufferedDeletes.Num> entry: deletesFlushed.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            int limit = entry.getValue().getNum();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docIDStart+docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              any = true;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletesFlushed.docIDs) {\n      int docID = docIdInt.intValue();\n      if (docID >= docIDStart && docID < docEnd) {\n        reader.deleteDocument(docID-docIDStart);\n        any = true;\n      }\n    }\n\n    // Delete by query\n    if (deletesFlushed.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletesFlushed.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (((long) docIDStart) + doc >= limit)\n                break;\n              reader.deleteDocument(doc);\n              any = true;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n    return any;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"44fcbde6fb2ac44ee3b45e013e54a42911e689ff":["334c1175813aea771a71728cd2c4ee4754fd0603"],"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["b43b719dab44d1ccc5ee5b6e01c50f1ee86bb76c"],"b43b719dab44d1ccc5ee5b6e01c50f1ee86bb76c":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"163fe85a71d778fd2b7747f65ca27b54829e2e57":["334c1175813aea771a71728cd2c4ee4754fd0603"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8fe956d65251358d755c56f14fe8380644790e47":["b43b719dab44d1ccc5ee5b6e01c50f1ee86bb76c"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["334c1175813aea771a71728cd2c4ee4754fd0603","44fcbde6fb2ac44ee3b45e013e54a42911e689ff"],"334c1175813aea771a71728cd2c4ee4754fd0603":["9b832cbed6eb3d54a8bb9339296bdda8eeb53014"],"9b832cbed6eb3d54a8bb9339296bdda8eeb53014":["4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["44fcbde6fb2ac44ee3b45e013e54a42911e689ff"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"44fcbde6fb2ac44ee3b45e013e54a42911e689ff":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["9b832cbed6eb3d54a8bb9339296bdda8eeb53014"],"b43b719dab44d1ccc5ee5b6e01c50f1ee86bb76c":["4f29ba80b723649f5feb7e37afe1a558dd2c1304","8fe956d65251358d755c56f14fe8380644790e47"],"163fe85a71d778fd2b7747f65ca27b54829e2e57":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"8fe956d65251358d755c56f14fe8380644790e47":[],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["b43b719dab44d1ccc5ee5b6e01c50f1ee86bb76c"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":[],"334c1175813aea771a71728cd2c4ee4754fd0603":["44fcbde6fb2ac44ee3b45e013e54a42911e689ff","163fe85a71d778fd2b7747f65ca27b54829e2e57","ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"9b832cbed6eb3d54a8bb9339296bdda8eeb53014":["334c1175813aea771a71728cd2c4ee4754fd0603"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["163fe85a71d778fd2b7747f65ca27b54829e2e57","8fe956d65251358d755c56f14fe8380644790e47","ab5cb6a74aefb78aa0569857970b9151dfe2e787","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}