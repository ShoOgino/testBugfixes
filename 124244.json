{"path":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","sourceNew":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    TokenStream stream = analyzer.tokenStream(\"field\", new StringReader(\"abcd   \"));\n    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n    stream = new CachingTokenFilter(stream);\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    Field f = new Field(\"field\", stream, customType);\n    doc.add(f);\n    doc.add(f);\n    w.addDocument(doc);\n    w.close();\n\n    IndexReader r = IndexReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null, true);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    TokenStream stream = analyzer.tokenStream(\"field\", new StringReader(\"abcd   \"));\n    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n    stream = new CachingTokenFilter(stream);\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    Field f = new Field(\"field\", stream, customType);\n    doc.add(f);\n    doc.add(f);\n    w.addDocument(doc);\n    w.close();\n\n    IndexReader r = IndexReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null, true);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f08557cdb6c60ac7b88a9342c983a20cd236e74f","date":1330954480,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","sourceNew":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    TokenStream stream = analyzer.tokenStream(\"field\", new StringReader(\"abcd   \"));\n    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n    stream = new CachingTokenFilter(stream);\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    Field f = new Field(\"field\", stream, customType);\n    doc.add(f);\n    doc.add(f);\n    w.addDocument(doc);\n    w.close();\n\n    IndexReader r = IndexReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null, true);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    TokenStream stream = analyzer.tokenStream(\"field\", new StringReader(\"abcd   \"));\n    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n    stream = new CachingTokenFilter(stream);\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    Field f = new Field(\"field\", stream, customType);\n    doc.add(f);\n    doc.add(f);\n    w.addDocument(doc);\n    w.close();\n\n    IndexReader r = IndexReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null, true);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","date":1331075828,"type":3,"author":"Ryan McKinley","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","sourceNew":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    TokenStream stream = analyzer.tokenStream(\"field\", new StringReader(\"abcd   \"));\n    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n    stream = new CachingTokenFilter(stream);\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    Field f = new Field(\"field\", stream, customType);\n    doc.add(f);\n    doc.add(f);\n    w.addDocument(doc);\n    w.close();\n\n    IndexReader r = IndexReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null, true);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    TokenStream stream = analyzer.tokenStream(\"field\", new StringReader(\"abcd   \"));\n    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n    stream = new CachingTokenFilter(stream);\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    Field f = new Field(\"field\", stream, customType);\n    doc.add(f);\n    doc.add(f);\n    w.addDocument(doc);\n    w.close();\n\n    IndexReader r = IndexReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null, true);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","sourceNew":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    TokenStream stream = analyzer.tokenStream(\"field\", new StringReader(\"abcd   \"));\n    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n    stream = new CachingTokenFilter(stream);\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    Field f = new Field(\"field\", stream, customType);\n    doc.add(f);\n    doc.add(f);\n    w.addDocument(doc);\n    w.close();\n\n    IndexReader r = IndexReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null, true);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    TokenStream stream = analyzer.tokenStream(\"field\", new StringReader(\"abcd   \"));\n    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n    stream = new CachingTokenFilter(stream);\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    Field f = new Field(\"field\", stream, customType);\n    doc.add(f);\n    doc.add(f);\n    w.addDocument(doc);\n    w.close();\n\n    IndexReader r = IndexReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null, true);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","date":1338430031,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","sourceNew":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    TokenStream stream = analyzer.tokenStream(\"field\", new StringReader(\"abcd   \"));\n    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n    stream = new CachingTokenFilter(stream);\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    Field f = new Field(\"field\", stream, customType);\n    doc.add(f);\n    doc.add(f);\n    w.addDocument(doc);\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null, true);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    TokenStream stream = analyzer.tokenStream(\"field\", new StringReader(\"abcd   \"));\n    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n    stream = new CachingTokenFilter(stream);\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    Field f = new Field(\"field\", stream, customType);\n    doc.add(f);\n    doc.add(f);\n    w.addDocument(doc);\n    w.close();\n\n    IndexReader r = IndexReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null, true);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","sourceNew":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    TokenStream stream = analyzer.tokenStream(\"field\", new StringReader(\"abcd   \"));\n    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n    stream = new CachingTokenFilter(stream);\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    Field f = new Field(\"field\", stream, customType);\n    doc.add(f);\n    doc.add(f);\n    w.addDocument(doc);\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null, true);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    TokenStream stream = analyzer.tokenStream(\"field\", new StringReader(\"abcd   \"));\n    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n    stream = new CachingTokenFilter(stream);\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    Field f = new Field(\"field\", stream, customType);\n    doc.add(f);\n    doc.add(f);\n    w.addDocument(doc);\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null, true);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"322360ac5185a8446d3e0b530b2068bef67cd3d5","date":1343669494,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","sourceNew":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    TokenStream stream = analyzer.tokenStream(\"field\", new StringReader(\"abcd   \"));\n    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n    stream = new CachingTokenFilter(stream);\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    Field f = new Field(\"field\", stream, customType);\n    doc.add(f);\n    doc.add(f);\n    w.addDocument(doc);\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    TokenStream stream = analyzer.tokenStream(\"field\", new StringReader(\"abcd   \"));\n    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n    stream = new CachingTokenFilter(stream);\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    Field f = new Field(\"field\", stream, customType);\n    doc.add(f);\n    doc.add(f);\n    w.addDocument(doc);\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null, true);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","sourceNew":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    TokenStream stream = analyzer.tokenStream(\"field\", new StringReader(\"abcd   \"));\n    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n    stream = new CachingTokenFilter(stream);\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    Field f = new Field(\"field\", stream, customType);\n    doc.add(f);\n    doc.add(f);\n    w.addDocument(doc);\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    TokenStream stream = analyzer.tokenStream(\"field\", new StringReader(\"abcd   \"));\n    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n    stream = new CachingTokenFilter(stream);\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    Field f = new Field(\"field\", stream, customType);\n    doc.add(f);\n    doc.add(f);\n    w.addDocument(doc);\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null, true);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","sourceNew":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    TokenStream stream = analyzer.tokenStream(\"field\", new StringReader(\"abcd   \"));\n    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n    stream = new CachingTokenFilter(stream);\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    Field f = new Field(\"field\", stream, customType);\n    doc.add(f);\n    doc.add(f);\n    w.addDocument(doc);\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    TokenStream stream = analyzer.tokenStream(\"field\", new StringReader(\"abcd   \"));\n    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n    stream = new CachingTokenFilter(stream);\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    Field f = new Field(\"field\", stream, customType);\n    doc.add(f);\n    doc.add(f);\n    w.addDocument(doc);\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null, true);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c83d6c4335f31cae14f625a222bc842f20073dcd","date":1373306148,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","sourceNew":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    TokenStream stream = analyzer.tokenStream(\"field\", \"abcd   \");\n    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n    stream = new CachingTokenFilter(stream);\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    Field f = new Field(\"field\", stream, customType);\n    doc.add(f);\n    doc.add(f);\n    w.addDocument(doc);\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    TokenStream stream = analyzer.tokenStream(\"field\", new StringReader(\"abcd   \"));\n    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n    stream = new CachingTokenFilter(stream);\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    Field f = new Field(\"field\", stream, customType);\n    doc.add(f);\n    doc.add(f);\n    w.addDocument(doc);\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"bugIntro":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","sourceNew":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    TokenStream stream = analyzer.tokenStream(\"field\", \"abcd   \");\n    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n    stream = new CachingTokenFilter(stream);\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    Field f = new Field(\"field\", stream, customType);\n    doc.add(f);\n    doc.add(f);\n    w.addDocument(doc);\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    TokenStream stream = analyzer.tokenStream(\"field\", new StringReader(\"abcd   \"));\n    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n    stream = new CachingTokenFilter(stream);\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    Field f = new Field(\"field\", stream, customType);\n    doc.add(f);\n    doc.add(f);\n    w.addDocument(doc);\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"782ed6a4b4ba50ec19734fc8db4e570ee193d627","date":1381127065,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","sourceNew":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    try (TokenStream stream = analyzer.tokenStream(\"field\", \"abcd   \")) {\n      stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n      TokenStream cachedStream = new CachingTokenFilter(stream);\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      Field f = new Field(\"field\", cachedStream, customType);\n      doc.add(f);\n      doc.add(f);\n      w.addDocument(doc);\n    }\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    TokenStream stream = analyzer.tokenStream(\"field\", \"abcd   \");\n    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n    stream = new CachingTokenFilter(stream);\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    Field f = new Field(\"field\", stream, customType);\n    doc.add(f);\n    doc.add(f);\n    w.addDocument(doc);\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":["3cc749c053615f5871f3b95715fe292f34e70a53","c83d6c4335f31cae14f625a222bc842f20073dcd","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4","04f07771a2a7dd3a395700665ed839c3dae2def2","1509f151d7692d84fae414b2b799ac06ba60fcb4","7e4db59c6b6c10e25322cfb41c4c19d78b4298bd","457c790b0d3d5883da64fb842ea54813004bb796"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","sourceNew":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    try (TokenStream stream = analyzer.tokenStream(\"field\", \"abcd   \")) {\n      stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n      TokenStream cachedStream = new CachingTokenFilter(stream);\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      Field f = new Field(\"field\", cachedStream, customType);\n      doc.add(f);\n      doc.add(f);\n      w.addDocument(doc);\n    }\n    w.shutdown();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    try (TokenStream stream = analyzer.tokenStream(\"field\", \"abcd   \")) {\n      stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n      TokenStream cachedStream = new CachingTokenFilter(stream);\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      Field f = new Field(\"field\", cachedStream, customType);\n      doc.add(f);\n      doc.add(f);\n      w.addDocument(doc);\n    }\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","sourceNew":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n    Document doc = new Document();\n    try (TokenStream stream = analyzer.tokenStream(\"field\", \"abcd   \")) {\n      stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n      TokenStream cachedStream = new CachingTokenFilter(stream);\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      Field f = new Field(\"field\", cachedStream, customType);\n      doc.add(f);\n      doc.add(f);\n      w.addDocument(doc);\n    }\n    w.shutdown();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));\n    Document doc = new Document();\n    try (TokenStream stream = analyzer.tokenStream(\"field\", \"abcd   \")) {\n      stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n      TokenStream cachedStream = new CachingTokenFilter(stream);\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      Field f = new Field(\"field\", cachedStream, customType);\n      doc.add(f);\n      doc.add(f);\n      w.addDocument(doc);\n    }\n    w.shutdown();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","sourceNew":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n    Document doc = new Document();\n    try (TokenStream stream = analyzer.tokenStream(\"field\", \"abcd   \")) {\n      stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n      TokenStream cachedStream = new CachingTokenFilter(stream);\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      Field f = new Field(\"field\", cachedStream, customType);\n      doc.add(f);\n      doc.add(f);\n      w.addDocument(doc);\n    }\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n    Document doc = new Document();\n    try (TokenStream stream = analyzer.tokenStream(\"field\", \"abcd   \")) {\n      stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n      TokenStream cachedStream = new CachingTokenFilter(stream);\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      Field f = new Field(\"field\", cachedStream, customType);\n      doc.add(f);\n      doc.add(f);\n      w.addDocument(doc);\n    }\n    w.shutdown();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55244759f906151d96839f8451dee793acb06e75","date":1418999882,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","sourceNew":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n    Document doc = new Document();\n    try (TokenStream stream = new CachingTokenFilter(analyzer.tokenStream(\"field\", \"abcd   \"))) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      Field f = new Field(\"field\", stream, customType);\n      doc.add(f);\n      doc.add(f);\n      w.addDocument(doc);\n    }\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n    Document doc = new Document();\n    try (TokenStream stream = analyzer.tokenStream(\"field\", \"abcd   \")) {\n      stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?\n      TokenStream cachedStream = new CachingTokenFilter(stream);\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      Field f = new Field(\"field\", cachedStream, customType);\n      doc.add(f);\n      doc.add(f);\n      w.addDocument(doc);\n    }\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","sourceNew":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n    Document doc = new Document();\n    try (TokenStream stream = new CachingTokenFilter(analyzer.tokenStream(\"field\", \"abcd   \"))) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      Field f = new Field(\"field\", stream, customType);\n      doc.add(f);\n      doc.add(f);\n      w.addDocument(doc);\n    }\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    PostingsEnum dpEnum = termsEnum.postings(null, null, PostingsEnum.FLAG_ALL);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n    Document doc = new Document();\n    try (TokenStream stream = new CachingTokenFilter(analyzer.tokenStream(\"field\", \"abcd   \"))) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      Field f = new Field(\"field\", stream, customType);\n      doc.add(f);\n      doc.add(f);\n      w.addDocument(doc);\n    }\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e73063b92d958076ef4ae8beb5f493e8ccdcecb4","date":1424177215,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","sourceNew":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n    Document doc = new Document();\n    try (TokenStream stream = new CachingTokenFilter(analyzer.tokenStream(\"field\", \"abcd   \"))) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      Field f = new Field(\"field\", stream, customType);\n      doc.add(f);\n      doc.add(f);\n      w.addDocument(doc);\n    }\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    PostingsEnum dpEnum = termsEnum.postings(null, null, PostingsEnum.ALL);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n    Document doc = new Document();\n    try (TokenStream stream = new CachingTokenFilter(analyzer.tokenStream(\"field\", \"abcd   \"))) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      Field f = new Field(\"field\", stream, customType);\n      doc.add(f);\n      doc.add(f);\n      w.addDocument(doc);\n    }\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    PostingsEnum dpEnum = termsEnum.postings(null, null, PostingsEnum.FLAG_ALL);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","date":1428522487,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","sourceNew":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n    Document doc = new Document();\n    try (TokenStream stream = new CachingTokenFilter(analyzer.tokenStream(\"field\", \"abcd   \"))) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      Field f = new Field(\"field\", stream, customType);\n      doc.add(f);\n      doc.add(f);\n      w.addDocument(doc);\n    }\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator();\n    assertNotNull(termsEnum.next());\n    PostingsEnum dpEnum = termsEnum.postings(null, null, PostingsEnum.ALL);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n    Document doc = new Document();\n    try (TokenStream stream = new CachingTokenFilter(analyzer.tokenStream(\"field\", \"abcd   \"))) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      Field f = new Field(\"field\", stream, customType);\n      doc.add(f);\n      doc.add(f);\n      w.addDocument(doc);\n    }\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator(null);\n    assertNotNull(termsEnum.next());\n    PostingsEnum dpEnum = termsEnum.postings(null, null, PostingsEnum.ALL);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter#testEndOffsetPositionWithCachingTokenFilter().mjava","sourceNew":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n    Document doc = new Document();\n    try (TokenStream stream = new CachingTokenFilter(analyzer.tokenStream(\"field\", \"abcd   \"))) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      Field f = new Field(\"field\", stream, customType);\n      doc.add(f);\n      doc.add(f);\n      w.addDocument(doc);\n    }\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator();\n    assertNotNull(termsEnum.next());\n    PostingsEnum dpEnum = termsEnum.postings(null, PostingsEnum.ALL);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1448\n  public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n    Document doc = new Document();\n    try (TokenStream stream = new CachingTokenFilter(analyzer.tokenStream(\"field\", \"abcd   \"))) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      Field f = new Field(\"field\", stream, customType);\n      doc.add(f);\n      doc.add(f);\n      w.addDocument(doc);\n    }\n    w.close();\n\n    IndexReader r = DirectoryReader.open(dir);\n    TermsEnum termsEnum = r.getTermVectors(0).terms(\"field\").iterator();\n    assertNotNull(termsEnum.next());\n    PostingsEnum dpEnum = termsEnum.postings(null, null, PostingsEnum.ALL);\n    assertEquals(2, termsEnum.totalTermFreq());\n\n    assertTrue(dpEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    dpEnum.nextPosition();\n    assertEquals(0, dpEnum.startOffset());\n    assertEquals(4, dpEnum.endOffset());\n\n    dpEnum.nextPosition();\n    assertEquals(8, dpEnum.startOffset());\n    assertEquals(12, dpEnum.endOffset());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, dpEnum.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["322360ac5185a8446d3e0b530b2068bef67cd3d5","c83d6c4335f31cae14f625a222bc842f20073dcd"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"f08557cdb6c60ac7b88a9342c983a20cd236e74f":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"782ed6a4b4ba50ec19734fc8db4e570ee193d627":["c83d6c4335f31cae14f625a222bc842f20073dcd"],"51f5280f31484820499077f41fcdfe92d527d9dc":["55244759f906151d96839f8451dee793acb06e75"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"c83d6c4335f31cae14f625a222bc842f20073dcd":["322360ac5185a8446d3e0b530b2068bef67cd3d5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["04f07771a2a7dd3a395700665ed839c3dae2def2","322360ac5185a8446d3e0b530b2068bef67cd3d5"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["04f07771a2a7dd3a395700665ed839c3dae2def2","322360ac5185a8446d3e0b530b2068bef67cd3d5"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"55244759f906151d96839f8451dee793acb06e75":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"322360ac5185a8446d3e0b530b2068bef67cd3d5":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0f4464508ee83288c8c4585b533f9faaa93aa314"]},"commit2Childs":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"f08557cdb6c60ac7b88a9342c983a20cd236e74f":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"782ed6a4b4ba50ec19734fc8db4e570ee193d627":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"51f5280f31484820499077f41fcdfe92d527d9dc":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","322360ac5185a8446d3e0b530b2068bef67cd3d5"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"c83d6c4335f31cae14f625a222bc842f20073dcd":["37a0f60745e53927c4c876cfe5b5a58170f0646c","782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["55244759f906151d96839f8451dee793acb06e75"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":[],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"55244759f906151d96839f8451dee793acb06e75":["51f5280f31484820499077f41fcdfe92d527d9dc"],"322360ac5185a8446d3e0b530b2068bef67cd3d5":["37a0f60745e53927c4c876cfe5b5a58170f0646c","c83d6c4335f31cae14f625a222bc842f20073dcd","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["37a0f60745e53927c4c876cfe5b5a58170f0646c","9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}