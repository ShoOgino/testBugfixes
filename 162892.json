{"path":"lucene/contrib/misc/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","commits":[{"id":"46855a2e3c096d06b604f73733ed9fefa822ba45","date":1305654486,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"/dev/null","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setMergeScheduler(cachedDir.getMergeScheduler());\n    RandomIndexWriter w = new RandomIndexWriter(random, cachedDir, conf);\n    w.w.setInfoStream(VERBOSE ? System.out : null);\n    final LineFileDocs docs = new LineFileDocs(random);    \n    final int numDocs = _TestUtil.nextInt(random, 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    IndexReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random.nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final IndexReader r2 = r.reopen();\n          if (r2 != r) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c3a8a449466c1ff7ce2274fe73dab487256964b4","date":1305735867,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"/dev/null","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setMergeScheduler(cachedDir.getMergeScheduler());\n    RandomIndexWriter w = new RandomIndexWriter(random, cachedDir, conf);\n    w.w.setInfoStream(VERBOSE ? System.out : null);\n    final LineFileDocs docs = new LineFileDocs(random);    \n    final int numDocs = _TestUtil.nextInt(random, 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    IndexReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random.nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final IndexReader r2 = r.reopen();\n          if (r2 != r) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":0,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"/dev/null","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setMergeScheduler(cachedDir.getMergeScheduler());\n    RandomIndexWriter w = new RandomIndexWriter(random, cachedDir, conf);\n    w.w.setInfoStream(VERBOSE ? System.out : null);\n    final LineFileDocs docs = new LineFileDocs(random);    \n    final int numDocs = _TestUtil.nextInt(random, 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    IndexReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random.nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final IndexReader r2 = r.reopen();\n          if (r2 != r) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a40a638d82918f2d8a40500c22745ec76b1aac5d","date":1309097847,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    RandomIndexWriter w = new RandomIndexWriter(random, cachedDir, conf);\n    w.w.setInfoStream(VERBOSE ? System.out : null);\n    final LineFileDocs docs = new LineFileDocs(random);    \n    final int numDocs = _TestUtil.nextInt(random, 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    IndexReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random.nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final IndexReader r2 = r.reopen();\n          if (r2 != r) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setMergeScheduler(cachedDir.getMergeScheduler());\n    RandomIndexWriter w = new RandomIndexWriter(random, cachedDir, conf);\n    w.w.setInfoStream(VERBOSE ? System.out : null);\n    final LineFileDocs docs = new LineFileDocs(random);    \n    final int numDocs = _TestUtil.nextInt(random, 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    IndexReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random.nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final IndexReader r2 = r.reopen();\n          if (r2 != r) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ddc4c914be86e34b54f70023f45a60fa7f04e929","date":1310115160,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    RandomIndexWriter w = new RandomIndexWriter(random, cachedDir, conf);\n    w.w.setInfoStream(VERBOSE ? System.out : null);\n    final LineFileDocs docs = new LineFileDocs(random);    \n    final int numDocs = _TestUtil.nextInt(random, 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    IndexReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random.nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final IndexReader r2 = r.reopen();\n          if (r2 != r) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setMergeScheduler(cachedDir.getMergeScheduler());\n    RandomIndexWriter w = new RandomIndexWriter(random, cachedDir, conf);\n    w.w.setInfoStream(VERBOSE ? System.out : null);\n    final LineFileDocs docs = new LineFileDocs(random);    \n    final int numDocs = _TestUtil.nextInt(random, 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    IndexReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random.nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final IndexReader r2 = r.reopen();\n          if (r2 != r) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5d004d0e0b3f65bb40da76d476d659d7888270e8","date":1310158940,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    RandomIndexWriter w = new RandomIndexWriter(random, cachedDir, conf);\n    w.w.setInfoStream(VERBOSE ? System.out : null);\n    final LineFileDocs docs = new LineFileDocs(random);    \n    final int numDocs = _TestUtil.nextInt(random, 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    IndexReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random.nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final IndexReader r2 = r.reopen();\n          if (r2 != r) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setMergeScheduler(cachedDir.getMergeScheduler());\n    RandomIndexWriter w = new RandomIndexWriter(random, cachedDir, conf);\n    w.w.setInfoStream(VERBOSE ? System.out : null);\n    final LineFileDocs docs = new LineFileDocs(random);    \n    final int numDocs = _TestUtil.nextInt(random, 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    IndexReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random.nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final IndexReader r2 = r.reopen();\n          if (r2 != r) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8a8259c922a83abc544609227a60d48e5ee93e7e","date":1317679620,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    RandomIndexWriter w = new RandomIndexWriter(random, cachedDir, conf);\n    w.w.setInfoStream(VERBOSE ? System.out : null);\n    final LineFileDocs docs = new LineFileDocs(random);    \n    final int numDocs = _TestUtil.nextInt(random, 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    IndexReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random.nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final IndexReader r2 = IndexReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    RandomIndexWriter w = new RandomIndexWriter(random, cachedDir, conf);\n    w.w.setInfoStream(VERBOSE ? System.out : null);\n    final LineFileDocs docs = new LineFileDocs(random);    \n    final int numDocs = _TestUtil.nextInt(random, 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    IndexReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random.nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final IndexReader r2 = r.reopen();\n          if (r2 != r) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    RandomIndexWriter w = new RandomIndexWriter(random, cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random);    \n    final int numDocs = _TestUtil.nextInt(random, 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    IndexReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random.nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final IndexReader r2 = IndexReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    RandomIndexWriter w = new RandomIndexWriter(random, cachedDir, conf);\n    w.w.setInfoStream(VERBOSE ? System.out : null);\n    final LineFileDocs docs = new LineFileDocs(random);    \n    final int numDocs = _TestUtil.nextInt(random, 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    IndexReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random.nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final IndexReader r2 = IndexReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"df16fc2e9b615e0138edac46655ae628f5d098ad","date":1320876869,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    RandomIndexWriter w = new RandomIndexWriter(random, cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random);    \n    final int numDocs = _TestUtil.nextInt(random, 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    IndexReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random.nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final IndexReader r2 = IndexReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    RandomIndexWriter w = new RandomIndexWriter(random, cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random);    \n    final int numDocs = _TestUtil.nextInt(random, 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    IndexReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random.nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final IndexReader r2 = IndexReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"8a8259c922a83abc544609227a60d48e5ee93e7e":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"c3a8a449466c1ff7ce2274fe73dab487256964b4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","46855a2e3c096d06b604f73733ed9fefa822ba45"],"df16fc2e9b615e0138edac46655ae628f5d098ad":["06584e6e98d592b34e1329b384182f368d2025e8"],"46855a2e3c096d06b604f73733ed9fefa822ba45":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"06584e6e98d592b34e1329b384182f368d2025e8":["8a8259c922a83abc544609227a60d48e5ee93e7e"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","46855a2e3c096d06b604f73733ed9fefa822ba45"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a40a638d82918f2d8a40500c22745ec76b1aac5d":["46855a2e3c096d06b604f73733ed9fefa822ba45"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":["a3776dccca01c11e7046323cfad46a3b4a471233","ddc4c914be86e34b54f70023f45a60fa7f04e929"],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["46855a2e3c096d06b604f73733ed9fefa822ba45","a40a638d82918f2d8a40500c22745ec76b1aac5d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["df16fc2e9b615e0138edac46655ae628f5d098ad"]},"commit2Childs":{"8a8259c922a83abc544609227a60d48e5ee93e7e":["06584e6e98d592b34e1329b384182f368d2025e8"],"c3a8a449466c1ff7ce2274fe73dab487256964b4":[],"46855a2e3c096d06b604f73733ed9fefa822ba45":["c3a8a449466c1ff7ce2274fe73dab487256964b4","a3776dccca01c11e7046323cfad46a3b4a471233","a40a638d82918f2d8a40500c22745ec76b1aac5d","ddc4c914be86e34b54f70023f45a60fa7f04e929"],"df16fc2e9b615e0138edac46655ae628f5d098ad":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"06584e6e98d592b34e1329b384182f368d2025e8":["df16fc2e9b615e0138edac46655ae628f5d098ad"],"a3776dccca01c11e7046323cfad46a3b4a471233":["5d004d0e0b3f65bb40da76d476d659d7888270e8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c3a8a449466c1ff7ce2274fe73dab487256964b4","46855a2e3c096d06b604f73733ed9fefa822ba45","a3776dccca01c11e7046323cfad46a3b4a471233"],"a40a638d82918f2d8a40500c22745ec76b1aac5d":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":[],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["8a8259c922a83abc544609227a60d48e5ee93e7e","5d004d0e0b3f65bb40da76d476d659d7888270e8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c3a8a449466c1ff7ce2274fe73dab487256964b4","5d004d0e0b3f65bb40da76d476d659d7888270e8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}