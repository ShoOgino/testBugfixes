{"path":"solr/core/src/java/org/apache/solr/search/JoinQuery.JoinQueryWeight#getDocSetEnumerate().mjava","commits":[{"id":"1e1491db4de13536b70146fc5a8f03101f0f84de","date":1593014806,"type":1,"author":"Atri Sharma","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery.JoinQueryWeight#getDocSetEnumerate().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSetEnumerate().mjava","sourceNew":"    public DocSet getDocSetEnumerate() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      Bits fastForRandomSet;\n      if (minDocFreqFrom <= 0) {\n        fastForRandomSet = null;\n      } else {\n        fastForRandomSet = fromSet.getBits();\n      }\n\n\n      LeafReader fromReader = fromSearcher.getSlowAtomicReader();\n      LeafReader toReader = fromSearcher==toSearcher ? fromReader : toSearcher.getSlowAtomicReader();\n      Terms terms = fromReader.terms(fromField);\n      Terms toTerms = toReader.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.empty();\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getLiveDocsBits();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getLiveDocsBits();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.postingsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.postingsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.postingsEnum = fromDeState.termsEnum.postings(fromDeState.postingsEnum, PostingsEnum.NONE);\n          PostingsEnum postingsEnum = fromDeState.postingsEnum;\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.get(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.get(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).getBits().clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.postingsEnum = toDeState.termsEnum.postings(toDeState.postingsEnum, PostingsEnum.NONE);\n              toDeState.postingsEnum = BitsFilteredPostingsEnum.wrap(toDeState.postingsEnum, toDeState.liveDocs);\n              PostingsEnum postingsEnum = toDeState.postingsEnum;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.addAllTo(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.empty();\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSetEnumerate() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      Bits fastForRandomSet;\n      if (minDocFreqFrom <= 0) {\n        fastForRandomSet = null;\n      } else {\n        fastForRandomSet = fromSet.getBits();\n      }\n\n\n      LeafReader fromReader = fromSearcher.getSlowAtomicReader();\n      LeafReader toReader = fromSearcher==toSearcher ? fromReader : toSearcher.getSlowAtomicReader();\n      Terms terms = fromReader.terms(fromField);\n      Terms toTerms = toReader.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.empty();\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getLiveDocsBits();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getLiveDocsBits();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.postingsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.postingsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.postingsEnum = fromDeState.termsEnum.postings(fromDeState.postingsEnum, PostingsEnum.NONE);\n          PostingsEnum postingsEnum = fromDeState.postingsEnum;\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.get(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.get(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).getBits().clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.postingsEnum = toDeState.termsEnum.postings(toDeState.postingsEnum, PostingsEnum.NONE);\n              toDeState.postingsEnum = BitsFilteredPostingsEnum.wrap(toDeState.postingsEnum, toDeState.liveDocs);\n              PostingsEnum postingsEnum = toDeState.postingsEnum;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.addAllTo(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.empty();\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"1e1491db4de13536b70146fc5a8f03101f0f84de":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["1e1491db4de13536b70146fc5a8f03101f0f84de"]},"commit2Childs":{"1e1491db4de13536b70146fc5a8f03101f0f84de":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1e1491db4de13536b70146fc5a8f03101f0f84de"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}