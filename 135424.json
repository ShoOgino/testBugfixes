{"path":"lucene/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","commits":[{"id":"8be580b58bcc650d428f3f22de81cadcf51d650a","date":1325279655,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#setUp().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {                  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setUseCompoundFile(true);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new Field(\"noTV\", English.intToEnglish(i), TextField.TYPE_STORED));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.close();\n    searcher = newSearcher(reader);\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {                  \n    super.setUp();\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setUseCompoundFile(true);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new Field(\"noTV\", English.intToEnglish(i), TextField.TYPE_STORED));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.close();\n    searcher = newSearcher(reader);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {                  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setUseCompoundFile(true);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new Field(\"noTV\", English.intToEnglish(i), TextField.TYPE_STORED));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.close();\n    searcher = newSearcher(reader);\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {                  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setUseCompoundFile(true);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new Field(\"noTV\", English.intToEnglish(i), TextField.TYPE_STORED));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.close();\n    searcher = newSearcher(reader);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["8be580b58bcc650d428f3f22de81cadcf51d650a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8be580b58bcc650d428f3f22de81cadcf51d650a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["8be580b58bcc650d428f3f22de81cadcf51d650a"],"8be580b58bcc650d428f3f22de81cadcf51d650a":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}