{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers#testCustomMaxTokenLength().mjava","commits":[{"id":"367e74558f41dfa2d24b323440dcb2d653ad1a29","date":1496009928,"type":0,"author":"Erick Erickson","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers#testCustomMaxTokenLength().mjava","pathOld":"/dev/null","sourceNew":"  /*\n   * tests the max word length passed as parameter - tokenizer will split at the passed position char no matter what happens\n   */\n  public void testCustomMaxTokenLength() throws IOException {\n\n    StringBuilder builder = new StringBuilder();\n    for (int i = 0; i < 100; i++) {\n      builder.append(\"A\");\n    }\n    Tokenizer tokenizer = new LowerCaseTokenizer(newAttributeFactory(), 100);\n    // Tricky, passing two copies of the string to the reader....\n    tokenizer.setReader(new StringReader(builder.toString() + builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString().toLowerCase(Locale.ROOT), \n        builder.toString().toLowerCase(Locale.ROOT) });\n\n    Exception e = expectThrows(IllegalArgumentException.class, () ->\n        new LowerCaseTokenizer(newAttributeFactory(), -1));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: -1\", e.getMessage());\n\n    tokenizer = new LetterTokenizer(newAttributeFactory(), 100);\n    tokenizer.setReader(new StringReader(builder.toString() + builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString(), builder.toString()});\n\n\n    // Let's test that we can get a token longer than 255 through.\n    builder.setLength(0);\n    for (int i = 0; i < 500; i++) {\n      builder.append(\"Z\");\n    }\n    tokenizer = new LetterTokenizer(newAttributeFactory(), 500);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString()});\n\n    \n    // Just to be sure what is happening here, token lengths of zero make no sense, \n    // Let's try the edge cases, token > I/O buffer (4096)\n    builder.setLength(0);\n    for (int i = 0; i < 600; i++) {\n      builder.append(\"aUrOkIjq\"); // 600 * 8 = 4800 chars.\n    }\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new LowerCaseTokenizer(newAttributeFactory(), 0));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", e.getMessage());\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new LowerCaseTokenizer(newAttributeFactory(), 10_000_000));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 10000000\", e.getMessage());\n\n    tokenizer = new LowerCaseTokenizer(newAttributeFactory(), 4800);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString().toLowerCase(Locale.ROOT)});\n\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new KeywordTokenizer(newAttributeFactory(), 0));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", e.getMessage());\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new KeywordTokenizer(newAttributeFactory(), 10_000_000));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 10000000\", e.getMessage());\n\n\n    tokenizer = new KeywordTokenizer(newAttributeFactory(), 4800);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString()});\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new LetterTokenizer(newAttributeFactory(), 0));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", e.getMessage());\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new LetterTokenizer(newAttributeFactory(), 2_000_000));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 2000000\", e.getMessage());\n\n    tokenizer = new LetterTokenizer(newAttributeFactory(), 4800);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString()});\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new WhitespaceTokenizer(newAttributeFactory(), 0));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", e.getMessage());\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new WhitespaceTokenizer(newAttributeFactory(), 3_000_000));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 3000000\", e.getMessage());\n\n    tokenizer = new WhitespaceTokenizer(newAttributeFactory(), 4800);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString()});\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d1f5728f32a4a256b36cfabd7a2636452f599bb9","date":1496231774,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers#testCustomMaxTokenLength().mjava","pathOld":"/dev/null","sourceNew":"  /*\n   * tests the max word length passed as parameter - tokenizer will split at the passed position char no matter what happens\n   */\n  public void testCustomMaxTokenLength() throws IOException {\n\n    StringBuilder builder = new StringBuilder();\n    for (int i = 0; i < 100; i++) {\n      builder.append(\"A\");\n    }\n    Tokenizer tokenizer = new LowerCaseTokenizer(newAttributeFactory(), 100);\n    // Tricky, passing two copies of the string to the reader....\n    tokenizer.setReader(new StringReader(builder.toString() + builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString().toLowerCase(Locale.ROOT), \n        builder.toString().toLowerCase(Locale.ROOT) });\n\n    Exception e = expectThrows(IllegalArgumentException.class, () ->\n        new LowerCaseTokenizer(newAttributeFactory(), -1));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: -1\", e.getMessage());\n\n    tokenizer = new LetterTokenizer(newAttributeFactory(), 100);\n    tokenizer.setReader(new StringReader(builder.toString() + builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString(), builder.toString()});\n\n\n    // Let's test that we can get a token longer than 255 through.\n    builder.setLength(0);\n    for (int i = 0; i < 500; i++) {\n      builder.append(\"Z\");\n    }\n    tokenizer = new LetterTokenizer(newAttributeFactory(), 500);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString()});\n\n    \n    // Just to be sure what is happening here, token lengths of zero make no sense, \n    // Let's try the edge cases, token > I/O buffer (4096)\n    builder.setLength(0);\n    for (int i = 0; i < 600; i++) {\n      builder.append(\"aUrOkIjq\"); // 600 * 8 = 4800 chars.\n    }\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new LowerCaseTokenizer(newAttributeFactory(), 0));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", e.getMessage());\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new LowerCaseTokenizer(newAttributeFactory(), 10_000_000));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 10000000\", e.getMessage());\n\n    tokenizer = new LowerCaseTokenizer(newAttributeFactory(), 4800);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString().toLowerCase(Locale.ROOT)});\n\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new KeywordTokenizer(newAttributeFactory(), 0));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", e.getMessage());\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new KeywordTokenizer(newAttributeFactory(), 10_000_000));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 10000000\", e.getMessage());\n\n\n    tokenizer = new KeywordTokenizer(newAttributeFactory(), 4800);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString()});\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new LetterTokenizer(newAttributeFactory(), 0));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", e.getMessage());\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new LetterTokenizer(newAttributeFactory(), 2_000_000));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 2000000\", e.getMessage());\n\n    tokenizer = new LetterTokenizer(newAttributeFactory(), 4800);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString()});\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new WhitespaceTokenizer(newAttributeFactory(), 0));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", e.getMessage());\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new WhitespaceTokenizer(newAttributeFactory(), 3_000_000));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 3000000\", e.getMessage());\n\n    tokenizer = new WhitespaceTokenizer(newAttributeFactory(), 4800);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString()});\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e9017cf144952056066919f1ebc7897ff9bd71b1","date":1496757600,"type":0,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers#testCustomMaxTokenLength().mjava","pathOld":"/dev/null","sourceNew":"  /*\n   * tests the max word length passed as parameter - tokenizer will split at the passed position char no matter what happens\n   */\n  public void testCustomMaxTokenLength() throws IOException {\n\n    StringBuilder builder = new StringBuilder();\n    for (int i = 0; i < 100; i++) {\n      builder.append(\"A\");\n    }\n    Tokenizer tokenizer = new LowerCaseTokenizer(newAttributeFactory(), 100);\n    // Tricky, passing two copies of the string to the reader....\n    tokenizer.setReader(new StringReader(builder.toString() + builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString().toLowerCase(Locale.ROOT), \n        builder.toString().toLowerCase(Locale.ROOT) });\n\n    Exception e = expectThrows(IllegalArgumentException.class, () ->\n        new LowerCaseTokenizer(newAttributeFactory(), -1));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: -1\", e.getMessage());\n\n    tokenizer = new LetterTokenizer(newAttributeFactory(), 100);\n    tokenizer.setReader(new StringReader(builder.toString() + builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString(), builder.toString()});\n\n\n    // Let's test that we can get a token longer than 255 through.\n    builder.setLength(0);\n    for (int i = 0; i < 500; i++) {\n      builder.append(\"Z\");\n    }\n    tokenizer = new LetterTokenizer(newAttributeFactory(), 500);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString()});\n\n    \n    // Just to be sure what is happening here, token lengths of zero make no sense, \n    // Let's try the edge cases, token > I/O buffer (4096)\n    builder.setLength(0);\n    for (int i = 0; i < 600; i++) {\n      builder.append(\"aUrOkIjq\"); // 600 * 8 = 4800 chars.\n    }\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new LowerCaseTokenizer(newAttributeFactory(), 0));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", e.getMessage());\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new LowerCaseTokenizer(newAttributeFactory(), 10_000_000));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 10000000\", e.getMessage());\n\n    tokenizer = new LowerCaseTokenizer(newAttributeFactory(), 4800);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString().toLowerCase(Locale.ROOT)});\n\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new KeywordTokenizer(newAttributeFactory(), 0));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", e.getMessage());\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new KeywordTokenizer(newAttributeFactory(), 10_000_000));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 10000000\", e.getMessage());\n\n\n    tokenizer = new KeywordTokenizer(newAttributeFactory(), 4800);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString()});\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new LetterTokenizer(newAttributeFactory(), 0));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", e.getMessage());\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new LetterTokenizer(newAttributeFactory(), 2_000_000));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 2000000\", e.getMessage());\n\n    tokenizer = new LetterTokenizer(newAttributeFactory(), 4800);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString()});\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new WhitespaceTokenizer(newAttributeFactory(), 0));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", e.getMessage());\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new WhitespaceTokenizer(newAttributeFactory(), 3_000_000));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 3000000\", e.getMessage());\n\n    tokenizer = new WhitespaceTokenizer(newAttributeFactory(), 4800);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString()});\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3780f02dd8e07e1feb00e1a4f522c4dedb85d9c0","date":1537441025,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers#testCustomMaxTokenLength().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers#testCustomMaxTokenLength().mjava","sourceNew":"  /*\n   * tests the max word length passed as parameter - tokenizer will split at the passed position char no matter what happens\n   */\n  public void testCustomMaxTokenLength() throws IOException {\n\n    StringBuilder builder = new StringBuilder();\n    for (int i = 0; i < 100; i++) {\n      builder.append(\"A\");\n    }\n    Tokenizer tokenizer = new LetterTokenizer(newAttributeFactory(), 100);\n    // Tricky, passing two copies of the string to the reader....\n    tokenizer.setReader(new StringReader(builder.toString() + builder.toString()));\n    assertTokenStreamContents(new LowerCaseFilter(tokenizer), new String[]{builder.toString().toLowerCase(Locale.ROOT),\n        builder.toString().toLowerCase(Locale.ROOT) });\n\n    Exception e = expectThrows(IllegalArgumentException.class, () ->\n        new LetterTokenizer(newAttributeFactory(), -1));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: -1\", e.getMessage());\n\n    tokenizer = new LetterTokenizer(newAttributeFactory(), 100);\n    tokenizer.setReader(new StringReader(builder.toString() + builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString(), builder.toString()});\n\n\n    // Let's test that we can get a token longer than 255 through.\n    builder.setLength(0);\n    for (int i = 0; i < 500; i++) {\n      builder.append(\"Z\");\n    }\n    tokenizer = new LetterTokenizer(newAttributeFactory(), 500);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString()});\n\n    \n    // Just to be sure what is happening here, token lengths of zero make no sense, \n    // Let's try the edge cases, token > I/O buffer (4096)\n    builder.setLength(0);\n    for (int i = 0; i < 600; i++) {\n      builder.append(\"aUrOkIjq\"); // 600 * 8 = 4800 chars.\n    }\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new LetterTokenizer(newAttributeFactory(), 0));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", e.getMessage());\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new LetterTokenizer(newAttributeFactory(), 10_000_000));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 10000000\", e.getMessage());\n\n    tokenizer = new LetterTokenizer(newAttributeFactory(), 4800);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(new LowerCaseFilter(tokenizer), new String[]{builder.toString().toLowerCase(Locale.ROOT)});\n\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new KeywordTokenizer(newAttributeFactory(), 0));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", e.getMessage());\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new KeywordTokenizer(newAttributeFactory(), 10_000_000));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 10000000\", e.getMessage());\n\n\n    tokenizer = new KeywordTokenizer(newAttributeFactory(), 4800);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString()});\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new LetterTokenizer(newAttributeFactory(), 0));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", e.getMessage());\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new LetterTokenizer(newAttributeFactory(), 2_000_000));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 2000000\", e.getMessage());\n\n    tokenizer = new LetterTokenizer(newAttributeFactory(), 4800);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString()});\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new WhitespaceTokenizer(newAttributeFactory(), 0));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", e.getMessage());\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new WhitespaceTokenizer(newAttributeFactory(), 3_000_000));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 3000000\", e.getMessage());\n\n    tokenizer = new WhitespaceTokenizer(newAttributeFactory(), 4800);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString()});\n\n  }\n\n","sourceOld":"  /*\n   * tests the max word length passed as parameter - tokenizer will split at the passed position char no matter what happens\n   */\n  public void testCustomMaxTokenLength() throws IOException {\n\n    StringBuilder builder = new StringBuilder();\n    for (int i = 0; i < 100; i++) {\n      builder.append(\"A\");\n    }\n    Tokenizer tokenizer = new LowerCaseTokenizer(newAttributeFactory(), 100);\n    // Tricky, passing two copies of the string to the reader....\n    tokenizer.setReader(new StringReader(builder.toString() + builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString().toLowerCase(Locale.ROOT), \n        builder.toString().toLowerCase(Locale.ROOT) });\n\n    Exception e = expectThrows(IllegalArgumentException.class, () ->\n        new LowerCaseTokenizer(newAttributeFactory(), -1));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: -1\", e.getMessage());\n\n    tokenizer = new LetterTokenizer(newAttributeFactory(), 100);\n    tokenizer.setReader(new StringReader(builder.toString() + builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString(), builder.toString()});\n\n\n    // Let's test that we can get a token longer than 255 through.\n    builder.setLength(0);\n    for (int i = 0; i < 500; i++) {\n      builder.append(\"Z\");\n    }\n    tokenizer = new LetterTokenizer(newAttributeFactory(), 500);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString()});\n\n    \n    // Just to be sure what is happening here, token lengths of zero make no sense, \n    // Let's try the edge cases, token > I/O buffer (4096)\n    builder.setLength(0);\n    for (int i = 0; i < 600; i++) {\n      builder.append(\"aUrOkIjq\"); // 600 * 8 = 4800 chars.\n    }\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new LowerCaseTokenizer(newAttributeFactory(), 0));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", e.getMessage());\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new LowerCaseTokenizer(newAttributeFactory(), 10_000_000));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 10000000\", e.getMessage());\n\n    tokenizer = new LowerCaseTokenizer(newAttributeFactory(), 4800);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString().toLowerCase(Locale.ROOT)});\n\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new KeywordTokenizer(newAttributeFactory(), 0));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", e.getMessage());\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new KeywordTokenizer(newAttributeFactory(), 10_000_000));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 10000000\", e.getMessage());\n\n\n    tokenizer = new KeywordTokenizer(newAttributeFactory(), 4800);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString()});\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new LetterTokenizer(newAttributeFactory(), 0));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", e.getMessage());\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new LetterTokenizer(newAttributeFactory(), 2_000_000));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 2000000\", e.getMessage());\n\n    tokenizer = new LetterTokenizer(newAttributeFactory(), 4800);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString()});\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new WhitespaceTokenizer(newAttributeFactory(), 0));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", e.getMessage());\n\n    e = expectThrows(IllegalArgumentException.class, () ->\n        new WhitespaceTokenizer(newAttributeFactory(), 3_000_000));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 3000000\", e.getMessage());\n\n    tokenizer = new WhitespaceTokenizer(newAttributeFactory(), 4800);\n    tokenizer.setReader(new StringReader(builder.toString()));\n    assertTokenStreamContents(tokenizer, new String[]{builder.toString()});\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3780f02dd8e07e1feb00e1a4f522c4dedb85d9c0":["d1f5728f32a4a256b36cfabd7a2636452f599bb9"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","367e74558f41dfa2d24b323440dcb2d653ad1a29"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"367e74558f41dfa2d24b323440dcb2d653ad1a29":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"d1f5728f32a4a256b36cfabd7a2636452f599bb9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","367e74558f41dfa2d24b323440dcb2d653ad1a29"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3780f02dd8e07e1feb00e1a4f522c4dedb85d9c0"]},"commit2Childs":{"3780f02dd8e07e1feb00e1a4f522c4dedb85d9c0":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e9017cf144952056066919f1ebc7897ff9bd71b1":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["e9017cf144952056066919f1ebc7897ff9bd71b1","367e74558f41dfa2d24b323440dcb2d653ad1a29","d1f5728f32a4a256b36cfabd7a2636452f599bb9"],"367e74558f41dfa2d24b323440dcb2d653ad1a29":["e9017cf144952056066919f1ebc7897ff9bd71b1","d1f5728f32a4a256b36cfabd7a2636452f599bb9"],"d1f5728f32a4a256b36cfabd7a2636452f599bb9":["3780f02dd8e07e1feb00e1a4f522c4dedb85d9c0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["e9017cf144952056066919f1ebc7897ff9bd71b1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}