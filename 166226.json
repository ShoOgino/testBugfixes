{"path":"backwards/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","commits":[{"id":"480d01e5b0ef8efb136d51670fec297ae5ae2c9c","date":1268821447,"type":0,"author":"Uwe Schindler","isMerge":false,"pathNew":"backwards/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","pathOld":"/dev/null","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = state.segmentName + \".\" + IndexFileNames.NORMS_EXTENSION;\n    state.flushedFiles.add(normsFileName);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List<NormsWriterPerField> toMerge = byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte(defaultNorm);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/backwards/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","pathOld":"backwards/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = state.segmentName + \".\" + IndexFileNames.NORMS_EXTENSION;\n    state.flushedFiles.add(normsFileName);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List<NormsWriterPerField> toMerge = byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte(defaultNorm);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = state.segmentName + \".\" + IndexFileNames.NORMS_EXTENSION;\n    state.flushedFiles.add(normsFileName);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List<NormsWriterPerField> toMerge = byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte(defaultNorm);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"480d01e5b0ef8efb136d51670fec297ae5ae2c9c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["480d01e5b0ef8efb136d51670fec297ae5ae2c9c"]},"commit2Childs":{"480d01e5b0ef8efb136d51670fec297ae5ae2c9c":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["480d01e5b0ef8efb136d51670fec297ae5ae2c9c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}