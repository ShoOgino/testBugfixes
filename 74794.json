{"path":"contrib/analyzers/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest#shingleFilterTest(int,Token[],Token[],int[],String[]).mjava","commits":[{"id":"c3e328a6f2b163170d23e06008798fb82c27af8b","date":1206825093,"type":0,"author":"Grant Ingersoll","isMerge":false,"pathNew":"contrib/analyzers/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest#shingleFilterTest(int,Token[],Token[],int[],String[]).mjava","pathOld":"/dev/null","sourceNew":"  protected void shingleFilterTest(int n, Token[] testToken, Token[] tokens,\n                                   int[] positionIncrements, String[] types)\n    throws IOException {\n\n    TokenStream filter = new ShingleFilter(new TestTokenStream(testToken), n);\n    Token token;\n    int i = 0;\n\n    while ((token = filter.next()) != null) {\n      String termText = new String(token.termBuffer(), 0, token.termLength());\n      String goldText\n        = new String(tokens[i].termBuffer(), 0, tokens[i].termLength());\n      assertEquals(\"Wrong termText\", goldText, termText);\n      assertEquals(\"Wrong startOffset for token \\\"\" + termText + \"\\\"\",\n                   tokens[i].startOffset(), token.startOffset());\n      assertEquals(\"Wrong endOffset for token \\\"\" + termText + \"\\\"\",\n                   tokens[i].endOffset(), token.endOffset());\n      assertEquals(\"Wrong positionIncrement for token \\\"\" + termText + \"\\\"\",\n                   positionIncrements[i], token.getPositionIncrement());\n      assertEquals(\"Wrong type for token \\\"\" + termText + \"\\\"\",\n                   types[i], token.type());\n      i++;\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2867ce10d36cd9f7ec1d66d346eb968b7f96f4b3","date":1210989348,"type":3,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"contrib/analyzers/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest#shingleFilterTest(int,Token[],Token[],int[],String[]).mjava","pathOld":"contrib/analyzers/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest#shingleFilterTest(int,Token[],Token[],int[],String[]).mjava","sourceNew":"  protected void shingleFilterTest(int maxSize, Token[] tokensToShingle, Token[] tokensToCompare,\n                                   int[] positionIncrements, String[] types)\n    throws IOException {\n\n    TokenStream filter = new ShingleFilter(new TestTokenStream(tokensToShingle), maxSize);\n    Token token;\n    int i = 0;\n\n    while ((token = filter.next()) != null) {\n      String termText = new String(token.termBuffer(), 0, token.termLength());\n      String goldText\n        = new String(tokensToCompare[i].termBuffer(), 0, tokensToCompare[i].termLength());\n      assertEquals(\"Wrong termText\", goldText, termText);\n      assertEquals(\"Wrong startOffset for token \\\"\" + termText + \"\\\"\",\n          tokensToCompare[i].startOffset(), token.startOffset());\n      assertEquals(\"Wrong endOffset for token \\\"\" + termText + \"\\\"\",\n          tokensToCompare[i].endOffset(), token.endOffset());\n      assertEquals(\"Wrong positionIncrement for token \\\"\" + termText + \"\\\"\",\n          positionIncrements[i], token.getPositionIncrement());\n      assertEquals(\"Wrong type for token \\\"\" + termText + \"\\\"\", types[i], token.type());\n      i++;\n    }\n  }\n\n","sourceOld":"  protected void shingleFilterTest(int n, Token[] testToken, Token[] tokens,\n                                   int[] positionIncrements, String[] types)\n    throws IOException {\n\n    TokenStream filter = new ShingleFilter(new TestTokenStream(testToken), n);\n    Token token;\n    int i = 0;\n\n    while ((token = filter.next()) != null) {\n      String termText = new String(token.termBuffer(), 0, token.termLength());\n      String goldText\n        = new String(tokens[i].termBuffer(), 0, tokens[i].termLength());\n      assertEquals(\"Wrong termText\", goldText, termText);\n      assertEquals(\"Wrong startOffset for token \\\"\" + termText + \"\\\"\",\n                   tokens[i].startOffset(), token.startOffset());\n      assertEquals(\"Wrong endOffset for token \\\"\" + termText + \"\\\"\",\n                   tokens[i].endOffset(), token.endOffset());\n      assertEquals(\"Wrong positionIncrement for token \\\"\" + termText + \"\\\"\",\n                   positionIncrements[i], token.getPositionIncrement());\n      assertEquals(\"Wrong type for token \\\"\" + termText + \"\\\"\",\n                   types[i], token.type());\n      i++;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e2cb543b41c145f33390f460ee743d6693c9c6c","date":1219243087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/analyzers/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest#shingleFilterTest(int,Token[],Token[],int[],String[]).mjava","pathOld":"contrib/analyzers/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest#shingleFilterTest(int,Token[],Token[],int[],String[]).mjava","sourceNew":"  protected void shingleFilterTest(int maxSize, Token[] tokensToShingle, Token[] tokensToCompare,\n                                   int[] positionIncrements, String[] types)\n    throws IOException {\n\n    TokenStream filter = new ShingleFilter(new TestTokenStream(tokensToShingle), maxSize);\n    int i = 0;\n    final Token reusableToken = new Token();\n    for (Token nextToken = filter.next(reusableToken); nextToken != null; nextToken = filter.next(reusableToken)) {\n      String termText = nextToken.term();\n      String goldText = tokensToCompare[i].term();\n      assertEquals(\"Wrong termText\", goldText, termText);\n      assertEquals(\"Wrong startOffset for token \\\"\" + termText + \"\\\"\",\n          tokensToCompare[i].startOffset(), nextToken.startOffset());\n      assertEquals(\"Wrong endOffset for token \\\"\" + termText + \"\\\"\",\n          tokensToCompare[i].endOffset(), nextToken.endOffset());\n      assertEquals(\"Wrong positionIncrement for token \\\"\" + termText + \"\\\"\",\n          positionIncrements[i], nextToken.getPositionIncrement());\n      assertEquals(\"Wrong type for token \\\"\" + termText + \"\\\"\", types[i], nextToken.type());\n      i++;\n    }\n  }\n\n","sourceOld":"  protected void shingleFilterTest(int maxSize, Token[] tokensToShingle, Token[] tokensToCompare,\n                                   int[] positionIncrements, String[] types)\n    throws IOException {\n\n    TokenStream filter = new ShingleFilter(new TestTokenStream(tokensToShingle), maxSize);\n    Token token;\n    int i = 0;\n\n    while ((token = filter.next()) != null) {\n      String termText = new String(token.termBuffer(), 0, token.termLength());\n      String goldText\n        = new String(tokensToCompare[i].termBuffer(), 0, tokensToCompare[i].termLength());\n      assertEquals(\"Wrong termText\", goldText, termText);\n      assertEquals(\"Wrong startOffset for token \\\"\" + termText + \"\\\"\",\n          tokensToCompare[i].startOffset(), token.startOffset());\n      assertEquals(\"Wrong endOffset for token \\\"\" + termText + \"\\\"\",\n          tokensToCompare[i].endOffset(), token.endOffset());\n      assertEquals(\"Wrong positionIncrement for token \\\"\" + termText + \"\\\"\",\n          positionIncrements[i], token.getPositionIncrement());\n      assertEquals(\"Wrong type for token \\\"\" + termText + \"\\\"\", types[i], token.type());\n      i++;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dd745d580729e528151b58aeda87ef82f1b95c9b","date":1248369082,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest#shingleFilterTest(int,Token[],Token[],int[],String[]).mjava","pathOld":"contrib/analyzers/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest#shingleFilterTest(int,Token[],Token[],int[],String[]).mjava","sourceNew":"  protected void shingleFilterTest(int maxSize, Token[] tokensToShingle, Token[] tokensToCompare,\n                                   int[] positionIncrements, String[] types)\n    throws IOException {\n\n    TokenStream filter = new ShingleFilter(new TestTokenStream(tokensToShingle), maxSize);\n    int i = 0;\n    final Token reusableToken = new Token();\n    for (Token nextToken = filter.next(reusableToken); nextToken != null; nextToken = filter.next(reusableToken)) {\n      String termText = nextToken.term();\n      String goldText = tokensToCompare[i].term();\n      assertEquals(\"Wrong termText\", goldText, termText);\n      assertEquals(\"Wrong startOffset for token \\\"\" + termText + \"\\\"\",\n          tokensToCompare[i].startOffset(), nextToken.startOffset());\n      assertEquals(\"Wrong endOffset for token \\\"\" + termText + \"\\\"\",\n          tokensToCompare[i].endOffset(), nextToken.endOffset());\n      assertEquals(\"Wrong positionIncrement for token \\\"\" + termText + \"\\\"\",\n          positionIncrements[i], nextToken.getPositionIncrement());\n      assertEquals(\"Wrong type for token \\\"\" + termText + \"\\\"\", types[i], nextToken.type());\n      i++;\n    }\n  }\n\n","sourceOld":"  protected void shingleFilterTest(int maxSize, Token[] tokensToShingle, Token[] tokensToCompare,\n                                   int[] positionIncrements, String[] types)\n    throws IOException {\n\n    TokenStream filter = new ShingleFilter(new TestTokenStream(tokensToShingle), maxSize);\n    int i = 0;\n    final Token reusableToken = new Token();\n    for (Token nextToken = filter.next(reusableToken); nextToken != null; nextToken = filter.next(reusableToken)) {\n      String termText = nextToken.term();\n      String goldText = tokensToCompare[i].term();\n      assertEquals(\"Wrong termText\", goldText, termText);\n      assertEquals(\"Wrong startOffset for token \\\"\" + termText + \"\\\"\",\n          tokensToCompare[i].startOffset(), nextToken.startOffset());\n      assertEquals(\"Wrong endOffset for token \\\"\" + termText + \"\\\"\",\n          tokensToCompare[i].endOffset(), nextToken.endOffset());\n      assertEquals(\"Wrong positionIncrement for token \\\"\" + termText + \"\\\"\",\n          positionIncrements[i], nextToken.getPositionIncrement());\n      assertEquals(\"Wrong type for token \\\"\" + termText + \"\\\"\", types[i], nextToken.type());\n      i++;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["2867ce10d36cd9f7ec1d66d346eb968b7f96f4b3"],"dd745d580729e528151b58aeda87ef82f1b95c9b":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["dd745d580729e528151b58aeda87ef82f1b95c9b"],"2867ce10d36cd9f7ec1d66d346eb968b7f96f4b3":["c3e328a6f2b163170d23e06008798fb82c27af8b"],"c3e328a6f2b163170d23e06008798fb82c27af8b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["dd745d580729e528151b58aeda87ef82f1b95c9b"],"dd745d580729e528151b58aeda87ef82f1b95c9b":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c3e328a6f2b163170d23e06008798fb82c27af8b"],"2867ce10d36cd9f7ec1d66d346eb968b7f96f4b3":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"c3e328a6f2b163170d23e06008798fb82c27af8b":["2867ce10d36cd9f7ec1d66d346eb968b7f96f4b3"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}