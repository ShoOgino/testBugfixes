{"path":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#testSplitWithChaosMonkey().mjava","commits":[{"id":"6a23ab64d81a448ad6ec571cbfc9599cc09b4e4b","date":1473679846,"type":0,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#testSplitWithChaosMonkey().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testSplitWithChaosMonkey() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    List<StoppableIndexingThread> indexers = new ArrayList<>();\n    try {\n      for (int i = 0; i < 1; i++) {\n        StoppableIndexingThread thread = new StoppableIndexingThread(controlClient, cloudClient, String.valueOf(i), true);\n        indexers.add(thread);\n        thread.start();\n      }\n      Thread.sleep(1000); // give the indexers some time to do their work\n    } catch (Exception e) {\n      log.error(\"Error in test\", e);\n    } finally {\n      for (StoppableIndexingThread indexer : indexers) {\n        indexer.safeStop();\n        indexer.join();\n      }\n    }\n\n    cloudClient.commit();\n    controlClient.commit();\n\n    AtomicBoolean stop = new AtomicBoolean();\n    AtomicBoolean killed = new AtomicBoolean(false);\n    Runnable monkey = new Runnable() {\n      @Override\n      public void run() {\n        ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n        zkStateReader.registerCollectionStateWatcher(AbstractDistribZkTestBase.DEFAULT_COLLECTION, new CollectionStateWatcher() {\n          @Override\n          public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n            if (stop.get()) {\n              return true; // abort and remove the watch\n            }\n            Slice slice = collectionState.getSlice(SHARD1_0);\n            if (slice != null && slice.getReplicas().size() > 1) {\n              // ensure that only one watcher invocation thread can kill!\n              if (killed.compareAndSet(false, true))  {\n                log.info(\"Monkey thread found 2 replicas for {} {}\", AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n                CloudJettyRunner cjetty = shardToLeaderJetty.get(SHARD1);\n                try {\n                  Thread.sleep(1000 + random().nextInt(500));\n                  ChaosMonkey.kill(cjetty);\n                  stop.set(true);\n                  return true;\n                } catch (Exception e) {\n                  log.error(\"Monkey unable to kill jetty at port \" + cjetty.jetty.getLocalPort(), e);\n                }\n              }\n            }\n            log.info(\"Monkey thread found only one replica for {} {}\", AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n            return false;\n          }\n        });\n      }\n    };\n\n    Thread monkeyThread = null;\n    /*\n     somehow the cluster state object inside this zk state reader has static copy of the collection which is never updated\n     so any call to waitForRecoveriesToFinish just keeps looping until timeout.\n     We workaround by explicitly registering the collection as an interesting one so that it is watched by ZkStateReader\n     see SOLR-9440. Todo remove this hack after SOLR-9440 is fixed.\n    */\n    cloudClient.getZkStateReader().registerCore(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n\n    monkeyThread = new Thread(monkey);\n    monkeyThread.start();\n    try {\n      CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n      splitShard.setShardName(SHARD1);\n      String asyncId = splitShard.processAsync(cloudClient);\n      RequestStatusState splitStatus = null;\n      try {\n        splitStatus = CollectionAdminRequest.requestStatus(asyncId).waitFor(cloudClient, 120);\n      } catch (Exception e) {\n        log.warn(\"Failed to get request status, maybe because the overseer node was shutdown by monkey\", e);\n      }\n\n      // we don't care if the split failed because we are injecting faults and it is likely\n      // that the split has failed but in any case we want to assert that all docs that got\n      // indexed are available in SolrCloud and if the split succeeded then all replicas of the sub-shard\n      // must be consistent (i.e. have same numdocs)\n\n      log.info(\"Shard split request state is COMPLETED\");\n      stop.set(true);\n      monkeyThread.join();\n      Set<String> addFails = new HashSet<>();\n      Set<String> deleteFails = new HashSet<>();\n      for (StoppableIndexingThread indexer : indexers) {\n        addFails.addAll(indexer.getAddFails());\n        deleteFails.addAll(indexer.getDeleteFails());\n      }\n\n      CloudJettyRunner cjetty = shardToLeaderJetty.get(SHARD1);\n      log.info(\"Starting shard1 leader jetty at port {}\", cjetty.jetty.getLocalPort());\n      ChaosMonkey.start(cjetty.jetty);\n      cloudClient.getZkStateReader().forceUpdateCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n      log.info(\"Current collection state: {}\", printClusterStateInfo(AbstractDistribZkTestBase.DEFAULT_COLLECTION));\n\n      boolean replicaCreationsFailed = false;\n      if (splitStatus == RequestStatusState.FAILED)  {\n        // either one or more replica creation failed (because it may have been created on the same parent shard leader node)\n        // or the split may have failed while trying to soft-commit *after* all replicas have been created\n        // the latter counts as a successful switch even if the API doesn't say so\n        // so we must find a way to distinguish between the two\n        // an easy way to do that is to look at the sub-shard replicas and check if the replica core actually exists\n        // instead of existing solely inside the cluster state\n        DocCollection collectionState = cloudClient.getZkStateReader().getClusterState().getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n        Slice slice10 = collectionState.getSlice(SHARD1_0);\n        Slice slice11 = collectionState.getSlice(SHARD1_1);\n        if (slice10 != null && slice11 != null) {\n          for (Replica replica : slice10) {\n            if (!doesReplicaCoreExist(replica)) {\n              replicaCreationsFailed = true;\n              break;\n            }\n          }\n          for (Replica replica : slice11) {\n            if (!doesReplicaCoreExist(replica)) {\n              replicaCreationsFailed = true;\n              break;\n            }\n          }\n        }\n      }\n\n      // true if sub-shard states switch to 'active' eventually\n      AtomicBoolean areSubShardsActive = new AtomicBoolean(false);\n\n      if (!replicaCreationsFailed)  {\n        // all sub-shard replicas were created successfully so all cores must recover eventually\n        waitForRecoveriesToFinish(AbstractDistribZkTestBase.DEFAULT_COLLECTION, true);\n        // let's wait for the overseer to switch shard states\n        CountDownLatch latch = new CountDownLatch(1);\n        cloudClient.getZkStateReader().registerCollectionStateWatcher(AbstractDistribZkTestBase.DEFAULT_COLLECTION, new CollectionStateWatcher() {\n          @Override\n          public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n            Slice parent = collectionState.getSlice(SHARD1);\n            Slice slice10 = collectionState.getSlice(SHARD1_0);\n            Slice slice11 = collectionState.getSlice(SHARD1_1);\n            if (slice10 != null && slice11 != null &&\n                parent.getState() == Slice.State.INACTIVE &&\n                slice10.getState() == Slice.State.ACTIVE &&\n                slice11.getState() == Slice.State.ACTIVE) {\n              areSubShardsActive.set(true);\n              latch.countDown();\n              return true; // removes the watch\n            } else if (slice10 != null && slice11 != null &&\n                parent.getState() == Slice.State.ACTIVE &&\n                slice10.getState() == Slice.State.RECOVERY_FAILED &&\n                slice11.getState() == Slice.State.RECOVERY_FAILED) {\n              areSubShardsActive.set(false);\n              latch.countDown();\n              return true;\n            }\n            return false;\n          }\n        });\n\n        latch.await(2, TimeUnit.MINUTES);\n\n        if (latch.getCount() != 0)  {\n          // sanity check\n          fail(\"We think that split was successful but sub-shard states were not updated even after 2 minutes.\");\n        }\n      }\n\n      cloudClient.commit(); // for visibility of results on sub-shards\n\n      checkShardConsistency(true, true, addFails, deleteFails);\n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      assertEquals(\"Found \" + ctrlDocs + \" control docs and \" + cloudClientDocs + \" cloud docs\", ctrlDocs, cloudClientDocs);\n\n      // check consistency of sub-shard replica explicitly because checkShardConsistency methods doesn't\n      // handle new shards/replica so well.\n      if (areSubShardsActive.get()) {\n        ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n        DocCollection collection = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n        int numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_0));\n        assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_0\", 2, numReplicasChecked);\n        numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_1));\n        assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_1\", 2, numReplicasChecked);\n      }\n    } finally {\n      stop.set(true);\n      monkeyThread.join();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f","bb222a3f9d9421d5c95afce73013fbd8de07ea1f","685af99397b6da31116a2cac747ed255d217d080","685af99397b6da31116a2cac747ed255d217d080"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"89424def13674ea17829b41c5883c54ecc31a132","date":1473767373,"type":0,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#testSplitWithChaosMonkey().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testSplitWithChaosMonkey() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    List<StoppableIndexingThread> indexers = new ArrayList<>();\n    try {\n      for (int i = 0; i < 1; i++) {\n        StoppableIndexingThread thread = new StoppableIndexingThread(controlClient, cloudClient, String.valueOf(i), true);\n        indexers.add(thread);\n        thread.start();\n      }\n      Thread.sleep(1000); // give the indexers some time to do their work\n    } catch (Exception e) {\n      log.error(\"Error in test\", e);\n    } finally {\n      for (StoppableIndexingThread indexer : indexers) {\n        indexer.safeStop();\n        indexer.join();\n      }\n    }\n\n    cloudClient.commit();\n    controlClient.commit();\n\n    AtomicBoolean stop = new AtomicBoolean();\n    AtomicBoolean killed = new AtomicBoolean(false);\n    Runnable monkey = new Runnable() {\n      @Override\n      public void run() {\n        ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n        zkStateReader.registerCollectionStateWatcher(AbstractDistribZkTestBase.DEFAULT_COLLECTION, new CollectionStateWatcher() {\n          @Override\n          public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n            if (stop.get()) {\n              return true; // abort and remove the watch\n            }\n            Slice slice = collectionState.getSlice(SHARD1_0);\n            if (slice != null && slice.getReplicas().size() > 1) {\n              // ensure that only one watcher invocation thread can kill!\n              if (killed.compareAndSet(false, true))  {\n                log.info(\"Monkey thread found 2 replicas for {} {}\", AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n                CloudJettyRunner cjetty = shardToLeaderJetty.get(SHARD1);\n                try {\n                  Thread.sleep(1000 + random().nextInt(500));\n                  ChaosMonkey.kill(cjetty);\n                  stop.set(true);\n                  return true;\n                } catch (Exception e) {\n                  log.error(\"Monkey unable to kill jetty at port \" + cjetty.jetty.getLocalPort(), e);\n                }\n              }\n            }\n            log.info(\"Monkey thread found only one replica for {} {}\", AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n            return false;\n          }\n        });\n      }\n    };\n\n    Thread monkeyThread = null;\n    /*\n     somehow the cluster state object inside this zk state reader has static copy of the collection which is never updated\n     so any call to waitForRecoveriesToFinish just keeps looping until timeout.\n     We workaround by explicitly registering the collection as an interesting one so that it is watched by ZkStateReader\n     see SOLR-9440. Todo remove this hack after SOLR-9440 is fixed.\n    */\n    cloudClient.getZkStateReader().registerCore(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n\n    monkeyThread = new Thread(monkey);\n    monkeyThread.start();\n    try {\n      CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n      splitShard.setShardName(SHARD1);\n      String asyncId = splitShard.processAsync(cloudClient);\n      RequestStatusState splitStatus = null;\n      try {\n        splitStatus = CollectionAdminRequest.requestStatus(asyncId).waitFor(cloudClient, 120);\n      } catch (Exception e) {\n        log.warn(\"Failed to get request status, maybe because the overseer node was shutdown by monkey\", e);\n      }\n\n      // we don't care if the split failed because we are injecting faults and it is likely\n      // that the split has failed but in any case we want to assert that all docs that got\n      // indexed are available in SolrCloud and if the split succeeded then all replicas of the sub-shard\n      // must be consistent (i.e. have same numdocs)\n\n      log.info(\"Shard split request state is COMPLETED\");\n      stop.set(true);\n      monkeyThread.join();\n      Set<String> addFails = new HashSet<>();\n      Set<String> deleteFails = new HashSet<>();\n      for (StoppableIndexingThread indexer : indexers) {\n        addFails.addAll(indexer.getAddFails());\n        deleteFails.addAll(indexer.getDeleteFails());\n      }\n\n      CloudJettyRunner cjetty = shardToLeaderJetty.get(SHARD1);\n      log.info(\"Starting shard1 leader jetty at port {}\", cjetty.jetty.getLocalPort());\n      ChaosMonkey.start(cjetty.jetty);\n      cloudClient.getZkStateReader().forceUpdateCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n      log.info(\"Current collection state: {}\", printClusterStateInfo(AbstractDistribZkTestBase.DEFAULT_COLLECTION));\n\n      boolean replicaCreationsFailed = false;\n      if (splitStatus == RequestStatusState.FAILED)  {\n        // either one or more replica creation failed (because it may have been created on the same parent shard leader node)\n        // or the split may have failed while trying to soft-commit *after* all replicas have been created\n        // the latter counts as a successful switch even if the API doesn't say so\n        // so we must find a way to distinguish between the two\n        // an easy way to do that is to look at the sub-shard replicas and check if the replica core actually exists\n        // instead of existing solely inside the cluster state\n        DocCollection collectionState = cloudClient.getZkStateReader().getClusterState().getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n        Slice slice10 = collectionState.getSlice(SHARD1_0);\n        Slice slice11 = collectionState.getSlice(SHARD1_1);\n        if (slice10 != null && slice11 != null) {\n          for (Replica replica : slice10) {\n            if (!doesReplicaCoreExist(replica)) {\n              replicaCreationsFailed = true;\n              break;\n            }\n          }\n          for (Replica replica : slice11) {\n            if (!doesReplicaCoreExist(replica)) {\n              replicaCreationsFailed = true;\n              break;\n            }\n          }\n        }\n      }\n\n      // true if sub-shard states switch to 'active' eventually\n      AtomicBoolean areSubShardsActive = new AtomicBoolean(false);\n\n      if (!replicaCreationsFailed)  {\n        // all sub-shard replicas were created successfully so all cores must recover eventually\n        waitForRecoveriesToFinish(AbstractDistribZkTestBase.DEFAULT_COLLECTION, true);\n        // let's wait for the overseer to switch shard states\n        CountDownLatch latch = new CountDownLatch(1);\n        cloudClient.getZkStateReader().registerCollectionStateWatcher(AbstractDistribZkTestBase.DEFAULT_COLLECTION, new CollectionStateWatcher() {\n          @Override\n          public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n            Slice parent = collectionState.getSlice(SHARD1);\n            Slice slice10 = collectionState.getSlice(SHARD1_0);\n            Slice slice11 = collectionState.getSlice(SHARD1_1);\n            if (slice10 != null && slice11 != null &&\n                parent.getState() == Slice.State.INACTIVE &&\n                slice10.getState() == Slice.State.ACTIVE &&\n                slice11.getState() == Slice.State.ACTIVE) {\n              areSubShardsActive.set(true);\n              latch.countDown();\n              return true; // removes the watch\n            } else if (slice10 != null && slice11 != null &&\n                parent.getState() == Slice.State.ACTIVE &&\n                slice10.getState() == Slice.State.RECOVERY_FAILED &&\n                slice11.getState() == Slice.State.RECOVERY_FAILED) {\n              areSubShardsActive.set(false);\n              latch.countDown();\n              return true;\n            }\n            return false;\n          }\n        });\n\n        latch.await(2, TimeUnit.MINUTES);\n\n        if (latch.getCount() != 0)  {\n          // sanity check\n          fail(\"We think that split was successful but sub-shard states were not updated even after 2 minutes.\");\n        }\n      }\n\n      cloudClient.commit(); // for visibility of results on sub-shards\n\n      checkShardConsistency(true, true, addFails, deleteFails);\n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      assertEquals(\"Found \" + ctrlDocs + \" control docs and \" + cloudClientDocs + \" cloud docs\", ctrlDocs, cloudClientDocs);\n\n      // check consistency of sub-shard replica explicitly because checkShardConsistency methods doesn't\n      // handle new shards/replica so well.\n      if (areSubShardsActive.get()) {\n        ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n        DocCollection collection = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n        int numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_0));\n        assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_0\", 2, numReplicasChecked);\n        numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_1));\n        assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_1\", 2, numReplicasChecked);\n      }\n    } finally {\n      stop.set(true);\n      monkeyThread.join();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#testSplitWithChaosMonkey().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testSplitWithChaosMonkey() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    List<StoppableIndexingThread> indexers = new ArrayList<>();\n    try {\n      for (int i = 0; i < 1; i++) {\n        StoppableIndexingThread thread = new StoppableIndexingThread(controlClient, cloudClient, String.valueOf(i), true);\n        indexers.add(thread);\n        thread.start();\n      }\n      Thread.sleep(1000); // give the indexers some time to do their work\n    } catch (Exception e) {\n      log.error(\"Error in test\", e);\n    } finally {\n      for (StoppableIndexingThread indexer : indexers) {\n        indexer.safeStop();\n        indexer.join();\n      }\n    }\n\n    cloudClient.commit();\n    controlClient.commit();\n\n    AtomicBoolean stop = new AtomicBoolean();\n    AtomicBoolean killed = new AtomicBoolean(false);\n    Runnable monkey = new Runnable() {\n      @Override\n      public void run() {\n        ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n        zkStateReader.registerCollectionStateWatcher(AbstractDistribZkTestBase.DEFAULT_COLLECTION, new CollectionStateWatcher() {\n          @Override\n          public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n            if (stop.get()) {\n              return true; // abort and remove the watch\n            }\n            Slice slice = collectionState.getSlice(SHARD1_0);\n            if (slice != null && slice.getReplicas().size() > 1) {\n              // ensure that only one watcher invocation thread can kill!\n              if (killed.compareAndSet(false, true))  {\n                log.info(\"Monkey thread found 2 replicas for {} {}\", AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n                CloudJettyRunner cjetty = shardToLeaderJetty.get(SHARD1);\n                try {\n                  Thread.sleep(1000 + random().nextInt(500));\n                  ChaosMonkey.kill(cjetty);\n                  stop.set(true);\n                  return true;\n                } catch (Exception e) {\n                  log.error(\"Monkey unable to kill jetty at port \" + cjetty.jetty.getLocalPort(), e);\n                }\n              }\n            }\n            log.info(\"Monkey thread found only one replica for {} {}\", AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n            return false;\n          }\n        });\n      }\n    };\n\n    Thread monkeyThread = null;\n    /*\n     somehow the cluster state object inside this zk state reader has static copy of the collection which is never updated\n     so any call to waitForRecoveriesToFinish just keeps looping until timeout.\n     We workaround by explicitly registering the collection as an interesting one so that it is watched by ZkStateReader\n     see SOLR-9440. Todo remove this hack after SOLR-9440 is fixed.\n    */\n    cloudClient.getZkStateReader().registerCore(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n\n    monkeyThread = new Thread(monkey);\n    monkeyThread.start();\n    try {\n      CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n      splitShard.setShardName(SHARD1);\n      String asyncId = splitShard.processAsync(cloudClient);\n      RequestStatusState splitStatus = null;\n      try {\n        splitStatus = CollectionAdminRequest.requestStatus(asyncId).waitFor(cloudClient, 120);\n      } catch (Exception e) {\n        log.warn(\"Failed to get request status, maybe because the overseer node was shutdown by monkey\", e);\n      }\n\n      // we don't care if the split failed because we are injecting faults and it is likely\n      // that the split has failed but in any case we want to assert that all docs that got\n      // indexed are available in SolrCloud and if the split succeeded then all replicas of the sub-shard\n      // must be consistent (i.e. have same numdocs)\n\n      log.info(\"Shard split request state is COMPLETED\");\n      stop.set(true);\n      monkeyThread.join();\n      Set<String> addFails = new HashSet<>();\n      Set<String> deleteFails = new HashSet<>();\n      for (StoppableIndexingThread indexer : indexers) {\n        addFails.addAll(indexer.getAddFails());\n        deleteFails.addAll(indexer.getDeleteFails());\n      }\n\n      CloudJettyRunner cjetty = shardToLeaderJetty.get(SHARD1);\n      log.info(\"Starting shard1 leader jetty at port {}\", cjetty.jetty.getLocalPort());\n      ChaosMonkey.start(cjetty.jetty);\n      cloudClient.getZkStateReader().forceUpdateCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n      log.info(\"Current collection state: {}\", printClusterStateInfo(AbstractDistribZkTestBase.DEFAULT_COLLECTION));\n\n      boolean replicaCreationsFailed = false;\n      if (splitStatus == RequestStatusState.FAILED)  {\n        // either one or more replica creation failed (because it may have been created on the same parent shard leader node)\n        // or the split may have failed while trying to soft-commit *after* all replicas have been created\n        // the latter counts as a successful switch even if the API doesn't say so\n        // so we must find a way to distinguish between the two\n        // an easy way to do that is to look at the sub-shard replicas and check if the replica core actually exists\n        // instead of existing solely inside the cluster state\n        DocCollection collectionState = cloudClient.getZkStateReader().getClusterState().getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n        Slice slice10 = collectionState.getSlice(SHARD1_0);\n        Slice slice11 = collectionState.getSlice(SHARD1_1);\n        if (slice10 != null && slice11 != null) {\n          for (Replica replica : slice10) {\n            if (!doesReplicaCoreExist(replica)) {\n              replicaCreationsFailed = true;\n              break;\n            }\n          }\n          for (Replica replica : slice11) {\n            if (!doesReplicaCoreExist(replica)) {\n              replicaCreationsFailed = true;\n              break;\n            }\n          }\n        }\n      }\n\n      // true if sub-shard states switch to 'active' eventually\n      AtomicBoolean areSubShardsActive = new AtomicBoolean(false);\n\n      if (!replicaCreationsFailed)  {\n        // all sub-shard replicas were created successfully so all cores must recover eventually\n        waitForRecoveriesToFinish(AbstractDistribZkTestBase.DEFAULT_COLLECTION, true);\n        // let's wait for the overseer to switch shard states\n        CountDownLatch latch = new CountDownLatch(1);\n        cloudClient.getZkStateReader().registerCollectionStateWatcher(AbstractDistribZkTestBase.DEFAULT_COLLECTION, new CollectionStateWatcher() {\n          @Override\n          public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n            Slice parent = collectionState.getSlice(SHARD1);\n            Slice slice10 = collectionState.getSlice(SHARD1_0);\n            Slice slice11 = collectionState.getSlice(SHARD1_1);\n            if (slice10 != null && slice11 != null &&\n                parent.getState() == Slice.State.INACTIVE &&\n                slice10.getState() == Slice.State.ACTIVE &&\n                slice11.getState() == Slice.State.ACTIVE) {\n              areSubShardsActive.set(true);\n              latch.countDown();\n              return true; // removes the watch\n            } else if (slice10 != null && slice11 != null &&\n                parent.getState() == Slice.State.ACTIVE &&\n                slice10.getState() == Slice.State.RECOVERY_FAILED &&\n                slice11.getState() == Slice.State.RECOVERY_FAILED) {\n              areSubShardsActive.set(false);\n              latch.countDown();\n              return true;\n            }\n            return false;\n          }\n        });\n\n        latch.await(2, TimeUnit.MINUTES);\n\n        if (latch.getCount() != 0)  {\n          // sanity check\n          fail(\"We think that split was successful but sub-shard states were not updated even after 2 minutes.\");\n        }\n      }\n\n      cloudClient.commit(); // for visibility of results on sub-shards\n\n      checkShardConsistency(true, true, addFails, deleteFails);\n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      assertEquals(\"Found \" + ctrlDocs + \" control docs and \" + cloudClientDocs + \" cloud docs\", ctrlDocs, cloudClientDocs);\n\n      // check consistency of sub-shard replica explicitly because checkShardConsistency methods doesn't\n      // handle new shards/replica so well.\n      if (areSubShardsActive.get()) {\n        ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n        DocCollection collection = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n        int numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_0));\n        assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_0\", 2, numReplicasChecked);\n        numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_1));\n        assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_1\", 2, numReplicasChecked);\n      }\n    } finally {\n      stop.set(true);\n      monkeyThread.join();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#testSplitWithChaosMonkey().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testSplitWithChaosMonkey() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    List<StoppableIndexingThread> indexers = new ArrayList<>();\n    try {\n      for (int i = 0; i < 1; i++) {\n        StoppableIndexingThread thread = new StoppableIndexingThread(controlClient, cloudClient, String.valueOf(i), true);\n        indexers.add(thread);\n        thread.start();\n      }\n      Thread.sleep(1000); // give the indexers some time to do their work\n    } catch (Exception e) {\n      log.error(\"Error in test\", e);\n    } finally {\n      for (StoppableIndexingThread indexer : indexers) {\n        indexer.safeStop();\n        indexer.join();\n      }\n    }\n\n    cloudClient.commit();\n    controlClient.commit();\n\n    AtomicBoolean stop = new AtomicBoolean();\n    AtomicBoolean killed = new AtomicBoolean(false);\n    Runnable monkey = new Runnable() {\n      @Override\n      public void run() {\n        ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n        zkStateReader.registerCollectionStateWatcher(AbstractDistribZkTestBase.DEFAULT_COLLECTION, new CollectionStateWatcher() {\n          @Override\n          public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n            if (stop.get()) {\n              return true; // abort and remove the watch\n            }\n            Slice slice = collectionState.getSlice(SHARD1_0);\n            if (slice != null && slice.getReplicas().size() > 1) {\n              // ensure that only one watcher invocation thread can kill!\n              if (killed.compareAndSet(false, true))  {\n                log.info(\"Monkey thread found 2 replicas for {} {}\", AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n                CloudJettyRunner cjetty = shardToLeaderJetty.get(SHARD1);\n                try {\n                  Thread.sleep(1000 + random().nextInt(500));\n                  ChaosMonkey.kill(cjetty);\n                  stop.set(true);\n                  return true;\n                } catch (Exception e) {\n                  log.error(\"Monkey unable to kill jetty at port \" + cjetty.jetty.getLocalPort(), e);\n                }\n              }\n            }\n            log.info(\"Monkey thread found only one replica for {} {}\", AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n            return false;\n          }\n        });\n      }\n    };\n\n    Thread monkeyThread = null;\n    /*\n     somehow the cluster state object inside this zk state reader has static copy of the collection which is never updated\n     so any call to waitForRecoveriesToFinish just keeps looping until timeout.\n     We workaround by explicitly registering the collection as an interesting one so that it is watched by ZkStateReader\n     see SOLR-9440. Todo remove this hack after SOLR-9440 is fixed.\n    */\n    cloudClient.getZkStateReader().registerCore(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n\n    monkeyThread = new Thread(monkey);\n    monkeyThread.start();\n    try {\n      CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n      splitShard.setShardName(SHARD1);\n      String asyncId = splitShard.processAsync(cloudClient);\n      RequestStatusState splitStatus = null;\n      try {\n        splitStatus = CollectionAdminRequest.requestStatus(asyncId).waitFor(cloudClient, 120);\n      } catch (Exception e) {\n        log.warn(\"Failed to get request status, maybe because the overseer node was shutdown by monkey\", e);\n      }\n\n      // we don't care if the split failed because we are injecting faults and it is likely\n      // that the split has failed but in any case we want to assert that all docs that got\n      // indexed are available in SolrCloud and if the split succeeded then all replicas of the sub-shard\n      // must be consistent (i.e. have same numdocs)\n\n      log.info(\"Shard split request state is COMPLETED\");\n      stop.set(true);\n      monkeyThread.join();\n      Set<String> addFails = new HashSet<>();\n      Set<String> deleteFails = new HashSet<>();\n      for (StoppableIndexingThread indexer : indexers) {\n        addFails.addAll(indexer.getAddFails());\n        deleteFails.addAll(indexer.getDeleteFails());\n      }\n\n      CloudJettyRunner cjetty = shardToLeaderJetty.get(SHARD1);\n      log.info(\"Starting shard1 leader jetty at port {}\", cjetty.jetty.getLocalPort());\n      ChaosMonkey.start(cjetty.jetty);\n      cloudClient.getZkStateReader().forceUpdateCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n      log.info(\"Current collection state: {}\", printClusterStateInfo(AbstractDistribZkTestBase.DEFAULT_COLLECTION));\n\n      boolean replicaCreationsFailed = false;\n      if (splitStatus == RequestStatusState.FAILED)  {\n        // either one or more replica creation failed (because it may have been created on the same parent shard leader node)\n        // or the split may have failed while trying to soft-commit *after* all replicas have been created\n        // the latter counts as a successful switch even if the API doesn't say so\n        // so we must find a way to distinguish between the two\n        // an easy way to do that is to look at the sub-shard replicas and check if the replica core actually exists\n        // instead of existing solely inside the cluster state\n        DocCollection collectionState = cloudClient.getZkStateReader().getClusterState().getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n        Slice slice10 = collectionState.getSlice(SHARD1_0);\n        Slice slice11 = collectionState.getSlice(SHARD1_1);\n        if (slice10 != null && slice11 != null) {\n          for (Replica replica : slice10) {\n            if (!doesReplicaCoreExist(replica)) {\n              replicaCreationsFailed = true;\n              break;\n            }\n          }\n          for (Replica replica : slice11) {\n            if (!doesReplicaCoreExist(replica)) {\n              replicaCreationsFailed = true;\n              break;\n            }\n          }\n        }\n      }\n\n      // true if sub-shard states switch to 'active' eventually\n      AtomicBoolean areSubShardsActive = new AtomicBoolean(false);\n\n      if (!replicaCreationsFailed)  {\n        // all sub-shard replicas were created successfully so all cores must recover eventually\n        waitForRecoveriesToFinish(AbstractDistribZkTestBase.DEFAULT_COLLECTION, true);\n        // let's wait for the overseer to switch shard states\n        CountDownLatch latch = new CountDownLatch(1);\n        cloudClient.getZkStateReader().registerCollectionStateWatcher(AbstractDistribZkTestBase.DEFAULT_COLLECTION, new CollectionStateWatcher() {\n          @Override\n          public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n            Slice parent = collectionState.getSlice(SHARD1);\n            Slice slice10 = collectionState.getSlice(SHARD1_0);\n            Slice slice11 = collectionState.getSlice(SHARD1_1);\n            if (slice10 != null && slice11 != null &&\n                parent.getState() == Slice.State.INACTIVE &&\n                slice10.getState() == Slice.State.ACTIVE &&\n                slice11.getState() == Slice.State.ACTIVE) {\n              areSubShardsActive.set(true);\n              latch.countDown();\n              return true; // removes the watch\n            } else if (slice10 != null && slice11 != null &&\n                parent.getState() == Slice.State.ACTIVE &&\n                slice10.getState() == Slice.State.RECOVERY_FAILED &&\n                slice11.getState() == Slice.State.RECOVERY_FAILED) {\n              areSubShardsActive.set(false);\n              latch.countDown();\n              return true;\n            }\n            return false;\n          }\n        });\n\n        latch.await(2, TimeUnit.MINUTES);\n\n        if (latch.getCount() != 0)  {\n          // sanity check\n          fail(\"We think that split was successful but sub-shard states were not updated even after 2 minutes.\");\n        }\n      }\n\n      cloudClient.commit(); // for visibility of results on sub-shards\n\n      checkShardConsistency(true, true, addFails, deleteFails);\n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      assertEquals(\"Found \" + ctrlDocs + \" control docs and \" + cloudClientDocs + \" cloud docs\", ctrlDocs, cloudClientDocs);\n\n      // check consistency of sub-shard replica explicitly because checkShardConsistency methods doesn't\n      // handle new shards/replica so well.\n      if (areSubShardsActive.get()) {\n        ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n        DocCollection collection = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n        int numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_0));\n        assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_0\", 2, numReplicasChecked);\n        numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_1));\n        assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_1\", 2, numReplicasChecked);\n      }\n    } finally {\n      stop.set(true);\n      monkeyThread.join();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"06a8aa646edc93b6f7e76ded12e41f1e6c04e7c4","date":1509452916,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#testSplitWithChaosMonkey().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#testSplitWithChaosMonkey().mjava","sourceNew":"  @Test\n  public void testSplitWithChaosMonkey() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    List<StoppableIndexingThread> indexers = new ArrayList<>();\n    try {\n      for (int i = 0; i < 1; i++) {\n        StoppableIndexingThread thread = new StoppableIndexingThread(controlClient, cloudClient, String.valueOf(i), true);\n        indexers.add(thread);\n        thread.start();\n      }\n      Thread.sleep(1000); // give the indexers some time to do their work\n    } catch (Exception e) {\n      log.error(\"Error in test\", e);\n    } finally {\n      for (StoppableIndexingThread indexer : indexers) {\n        indexer.safeStop();\n        indexer.join();\n      }\n    }\n\n    cloudClient.commit();\n    controlClient.commit();\n\n    AtomicBoolean stop = new AtomicBoolean();\n    AtomicBoolean killed = new AtomicBoolean(false);\n    Runnable monkey = new Runnable() {\n      @Override\n      public void run() {\n        ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n        zkStateReader.registerCollectionStateWatcher(AbstractDistribZkTestBase.DEFAULT_COLLECTION, new CollectionStateWatcher() {\n          @Override\n          public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n            if (stop.get()) {\n              return true; // abort and remove the watch\n            }\n            Slice slice = collectionState.getSlice(SHARD1_0);\n            if (slice != null && slice.getReplicas().size() > 1) {\n              // ensure that only one watcher invocation thread can kill!\n              if (killed.compareAndSet(false, true))  {\n                log.info(\"Monkey thread found 2 replicas for {} {}\", AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n                CloudJettyRunner cjetty = shardToLeaderJetty.get(SHARD1);\n                try {\n                  Thread.sleep(1000 + random().nextInt(500));\n                  ChaosMonkey.kill(cjetty);\n                  stop.set(true);\n                  return true;\n                } catch (Exception e) {\n                  log.error(\"Monkey unable to kill jetty at port \" + cjetty.jetty.getLocalPort(), e);\n                }\n              }\n            }\n            log.info(\"Monkey thread found only one replica for {} {}\", AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n            return false;\n          }\n        });\n      }\n    };\n\n    Thread monkeyThread = null;\n    monkeyThread = new Thread(monkey);\n    monkeyThread.start();\n    try {\n      CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n      splitShard.setShardName(SHARD1);\n      String asyncId = splitShard.processAsync(cloudClient);\n      RequestStatusState splitStatus = null;\n      try {\n        splitStatus = CollectionAdminRequest.requestStatus(asyncId).waitFor(cloudClient, 120);\n      } catch (Exception e) {\n        log.warn(\"Failed to get request status, maybe because the overseer node was shutdown by monkey\", e);\n      }\n\n      // we don't care if the split failed because we are injecting faults and it is likely\n      // that the split has failed but in any case we want to assert that all docs that got\n      // indexed are available in SolrCloud and if the split succeeded then all replicas of the sub-shard\n      // must be consistent (i.e. have same numdocs)\n\n      log.info(\"Shard split request state is COMPLETED\");\n      stop.set(true);\n      monkeyThread.join();\n      Set<String> addFails = new HashSet<>();\n      Set<String> deleteFails = new HashSet<>();\n      for (StoppableIndexingThread indexer : indexers) {\n        addFails.addAll(indexer.getAddFails());\n        deleteFails.addAll(indexer.getDeleteFails());\n      }\n\n      CloudJettyRunner cjetty = shardToLeaderJetty.get(SHARD1);\n      log.info(\"Starting shard1 leader jetty at port {}\", cjetty.jetty.getLocalPort());\n      ChaosMonkey.start(cjetty.jetty);\n      cloudClient.getZkStateReader().forceUpdateCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n      log.info(\"Current collection state: {}\", printClusterStateInfo(AbstractDistribZkTestBase.DEFAULT_COLLECTION));\n\n      boolean replicaCreationsFailed = false;\n      if (splitStatus == RequestStatusState.FAILED)  {\n        // either one or more replica creation failed (because it may have been created on the same parent shard leader node)\n        // or the split may have failed while trying to soft-commit *after* all replicas have been created\n        // the latter counts as a successful switch even if the API doesn't say so\n        // so we must find a way to distinguish between the two\n        // an easy way to do that is to look at the sub-shard replicas and check if the replica core actually exists\n        // instead of existing solely inside the cluster state\n        DocCollection collectionState = cloudClient.getZkStateReader().getClusterState().getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n        Slice slice10 = collectionState.getSlice(SHARD1_0);\n        Slice slice11 = collectionState.getSlice(SHARD1_1);\n        if (slice10 != null && slice11 != null) {\n          for (Replica replica : slice10) {\n            if (!doesReplicaCoreExist(replica)) {\n              replicaCreationsFailed = true;\n              break;\n            }\n          }\n          for (Replica replica : slice11) {\n            if (!doesReplicaCoreExist(replica)) {\n              replicaCreationsFailed = true;\n              break;\n            }\n          }\n        }\n      }\n\n      // true if sub-shard states switch to 'active' eventually\n      AtomicBoolean areSubShardsActive = new AtomicBoolean(false);\n\n      if (!replicaCreationsFailed)  {\n        // all sub-shard replicas were created successfully so all cores must recover eventually\n        waitForRecoveriesToFinish(AbstractDistribZkTestBase.DEFAULT_COLLECTION, true);\n        // let's wait for the overseer to switch shard states\n        CountDownLatch latch = new CountDownLatch(1);\n        cloudClient.getZkStateReader().registerCollectionStateWatcher(AbstractDistribZkTestBase.DEFAULT_COLLECTION, new CollectionStateWatcher() {\n          @Override\n          public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n            Slice parent = collectionState.getSlice(SHARD1);\n            Slice slice10 = collectionState.getSlice(SHARD1_0);\n            Slice slice11 = collectionState.getSlice(SHARD1_1);\n            if (slice10 != null && slice11 != null &&\n                parent.getState() == Slice.State.INACTIVE &&\n                slice10.getState() == Slice.State.ACTIVE &&\n                slice11.getState() == Slice.State.ACTIVE) {\n              areSubShardsActive.set(true);\n              latch.countDown();\n              return true; // removes the watch\n            } else if (slice10 != null && slice11 != null &&\n                parent.getState() == Slice.State.ACTIVE &&\n                slice10.getState() == Slice.State.RECOVERY_FAILED &&\n                slice11.getState() == Slice.State.RECOVERY_FAILED) {\n              areSubShardsActive.set(false);\n              latch.countDown();\n              return true;\n            }\n            return false;\n          }\n        });\n\n        latch.await(2, TimeUnit.MINUTES);\n\n        if (latch.getCount() != 0)  {\n          // sanity check\n          fail(\"We think that split was successful but sub-shard states were not updated even after 2 minutes.\");\n        }\n      }\n\n      cloudClient.commit(); // for visibility of results on sub-shards\n\n      checkShardConsistency(true, true, addFails, deleteFails);\n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      assertEquals(\"Found \" + ctrlDocs + \" control docs and \" + cloudClientDocs + \" cloud docs\", ctrlDocs, cloudClientDocs);\n\n      // check consistency of sub-shard replica explicitly because checkShardConsistency methods doesn't\n      // handle new shards/replica so well.\n      if (areSubShardsActive.get()) {\n        ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n        DocCollection collection = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n        int numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_0));\n        assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_0\", 2, numReplicasChecked);\n        numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_1));\n        assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_1\", 2, numReplicasChecked);\n      }\n    } finally {\n      stop.set(true);\n      monkeyThread.join();\n    }\n  }\n\n","sourceOld":"  @Test\n  public void testSplitWithChaosMonkey() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    List<StoppableIndexingThread> indexers = new ArrayList<>();\n    try {\n      for (int i = 0; i < 1; i++) {\n        StoppableIndexingThread thread = new StoppableIndexingThread(controlClient, cloudClient, String.valueOf(i), true);\n        indexers.add(thread);\n        thread.start();\n      }\n      Thread.sleep(1000); // give the indexers some time to do their work\n    } catch (Exception e) {\n      log.error(\"Error in test\", e);\n    } finally {\n      for (StoppableIndexingThread indexer : indexers) {\n        indexer.safeStop();\n        indexer.join();\n      }\n    }\n\n    cloudClient.commit();\n    controlClient.commit();\n\n    AtomicBoolean stop = new AtomicBoolean();\n    AtomicBoolean killed = new AtomicBoolean(false);\n    Runnable monkey = new Runnable() {\n      @Override\n      public void run() {\n        ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n        zkStateReader.registerCollectionStateWatcher(AbstractDistribZkTestBase.DEFAULT_COLLECTION, new CollectionStateWatcher() {\n          @Override\n          public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n            if (stop.get()) {\n              return true; // abort and remove the watch\n            }\n            Slice slice = collectionState.getSlice(SHARD1_0);\n            if (slice != null && slice.getReplicas().size() > 1) {\n              // ensure that only one watcher invocation thread can kill!\n              if (killed.compareAndSet(false, true))  {\n                log.info(\"Monkey thread found 2 replicas for {} {}\", AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n                CloudJettyRunner cjetty = shardToLeaderJetty.get(SHARD1);\n                try {\n                  Thread.sleep(1000 + random().nextInt(500));\n                  ChaosMonkey.kill(cjetty);\n                  stop.set(true);\n                  return true;\n                } catch (Exception e) {\n                  log.error(\"Monkey unable to kill jetty at port \" + cjetty.jetty.getLocalPort(), e);\n                }\n              }\n            }\n            log.info(\"Monkey thread found only one replica for {} {}\", AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n            return false;\n          }\n        });\n      }\n    };\n\n    Thread monkeyThread = null;\n    /*\n     somehow the cluster state object inside this zk state reader has static copy of the collection which is never updated\n     so any call to waitForRecoveriesToFinish just keeps looping until timeout.\n     We workaround by explicitly registering the collection as an interesting one so that it is watched by ZkStateReader\n     see SOLR-9440. Todo remove this hack after SOLR-9440 is fixed.\n    */\n    cloudClient.getZkStateReader().registerCore(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n\n    monkeyThread = new Thread(monkey);\n    monkeyThread.start();\n    try {\n      CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n      splitShard.setShardName(SHARD1);\n      String asyncId = splitShard.processAsync(cloudClient);\n      RequestStatusState splitStatus = null;\n      try {\n        splitStatus = CollectionAdminRequest.requestStatus(asyncId).waitFor(cloudClient, 120);\n      } catch (Exception e) {\n        log.warn(\"Failed to get request status, maybe because the overseer node was shutdown by monkey\", e);\n      }\n\n      // we don't care if the split failed because we are injecting faults and it is likely\n      // that the split has failed but in any case we want to assert that all docs that got\n      // indexed are available in SolrCloud and if the split succeeded then all replicas of the sub-shard\n      // must be consistent (i.e. have same numdocs)\n\n      log.info(\"Shard split request state is COMPLETED\");\n      stop.set(true);\n      monkeyThread.join();\n      Set<String> addFails = new HashSet<>();\n      Set<String> deleteFails = new HashSet<>();\n      for (StoppableIndexingThread indexer : indexers) {\n        addFails.addAll(indexer.getAddFails());\n        deleteFails.addAll(indexer.getDeleteFails());\n      }\n\n      CloudJettyRunner cjetty = shardToLeaderJetty.get(SHARD1);\n      log.info(\"Starting shard1 leader jetty at port {}\", cjetty.jetty.getLocalPort());\n      ChaosMonkey.start(cjetty.jetty);\n      cloudClient.getZkStateReader().forceUpdateCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n      log.info(\"Current collection state: {}\", printClusterStateInfo(AbstractDistribZkTestBase.DEFAULT_COLLECTION));\n\n      boolean replicaCreationsFailed = false;\n      if (splitStatus == RequestStatusState.FAILED)  {\n        // either one or more replica creation failed (because it may have been created on the same parent shard leader node)\n        // or the split may have failed while trying to soft-commit *after* all replicas have been created\n        // the latter counts as a successful switch even if the API doesn't say so\n        // so we must find a way to distinguish between the two\n        // an easy way to do that is to look at the sub-shard replicas and check if the replica core actually exists\n        // instead of existing solely inside the cluster state\n        DocCollection collectionState = cloudClient.getZkStateReader().getClusterState().getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n        Slice slice10 = collectionState.getSlice(SHARD1_0);\n        Slice slice11 = collectionState.getSlice(SHARD1_1);\n        if (slice10 != null && slice11 != null) {\n          for (Replica replica : slice10) {\n            if (!doesReplicaCoreExist(replica)) {\n              replicaCreationsFailed = true;\n              break;\n            }\n          }\n          for (Replica replica : slice11) {\n            if (!doesReplicaCoreExist(replica)) {\n              replicaCreationsFailed = true;\n              break;\n            }\n          }\n        }\n      }\n\n      // true if sub-shard states switch to 'active' eventually\n      AtomicBoolean areSubShardsActive = new AtomicBoolean(false);\n\n      if (!replicaCreationsFailed)  {\n        // all sub-shard replicas were created successfully so all cores must recover eventually\n        waitForRecoveriesToFinish(AbstractDistribZkTestBase.DEFAULT_COLLECTION, true);\n        // let's wait for the overseer to switch shard states\n        CountDownLatch latch = new CountDownLatch(1);\n        cloudClient.getZkStateReader().registerCollectionStateWatcher(AbstractDistribZkTestBase.DEFAULT_COLLECTION, new CollectionStateWatcher() {\n          @Override\n          public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n            Slice parent = collectionState.getSlice(SHARD1);\n            Slice slice10 = collectionState.getSlice(SHARD1_0);\n            Slice slice11 = collectionState.getSlice(SHARD1_1);\n            if (slice10 != null && slice11 != null &&\n                parent.getState() == Slice.State.INACTIVE &&\n                slice10.getState() == Slice.State.ACTIVE &&\n                slice11.getState() == Slice.State.ACTIVE) {\n              areSubShardsActive.set(true);\n              latch.countDown();\n              return true; // removes the watch\n            } else if (slice10 != null && slice11 != null &&\n                parent.getState() == Slice.State.ACTIVE &&\n                slice10.getState() == Slice.State.RECOVERY_FAILED &&\n                slice11.getState() == Slice.State.RECOVERY_FAILED) {\n              areSubShardsActive.set(false);\n              latch.countDown();\n              return true;\n            }\n            return false;\n          }\n        });\n\n        latch.await(2, TimeUnit.MINUTES);\n\n        if (latch.getCount() != 0)  {\n          // sanity check\n          fail(\"We think that split was successful but sub-shard states were not updated even after 2 minutes.\");\n        }\n      }\n\n      cloudClient.commit(); // for visibility of results on sub-shards\n\n      checkShardConsistency(true, true, addFails, deleteFails);\n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      assertEquals(\"Found \" + ctrlDocs + \" control docs and \" + cloudClientDocs + \" cloud docs\", ctrlDocs, cloudClientDocs);\n\n      // check consistency of sub-shard replica explicitly because checkShardConsistency methods doesn't\n      // handle new shards/replica so well.\n      if (areSubShardsActive.get()) {\n        ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n        DocCollection collection = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n        int numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_0));\n        assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_0\", 2, numReplicasChecked);\n        numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_1));\n        assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_1\", 2, numReplicasChecked);\n      }\n    } finally {\n      stop.set(true);\n      monkeyThread.join();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6146c07c0dee1ae1e42926167acd127fed5ef59d","date":1516129420,"type":5,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/ShardSplitTest#testSplitWithChaosMonkey().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#testSplitWithChaosMonkey().mjava","sourceNew":"  @Test\n  public void testSplitWithChaosMonkey() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    List<StoppableIndexingThread> indexers = new ArrayList<>();\n    try {\n      for (int i = 0; i < 1; i++) {\n        StoppableIndexingThread thread = new StoppableIndexingThread(controlClient, cloudClient, String.valueOf(i), true);\n        indexers.add(thread);\n        thread.start();\n      }\n      Thread.sleep(1000); // give the indexers some time to do their work\n    } catch (Exception e) {\n      log.error(\"Error in test\", e);\n    } finally {\n      for (StoppableIndexingThread indexer : indexers) {\n        indexer.safeStop();\n        indexer.join();\n      }\n    }\n\n    cloudClient.commit();\n    controlClient.commit();\n\n    AtomicBoolean stop = new AtomicBoolean();\n    AtomicBoolean killed = new AtomicBoolean(false);\n    Runnable monkey = new Runnable() {\n      @Override\n      public void run() {\n        ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n        zkStateReader.registerCollectionStateWatcher(AbstractDistribZkTestBase.DEFAULT_COLLECTION, new CollectionStateWatcher() {\n          @Override\n          public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n            if (stop.get()) {\n              return true; // abort and remove the watch\n            }\n            Slice slice = collectionState.getSlice(SHARD1_0);\n            if (slice != null && slice.getReplicas().size() > 1) {\n              // ensure that only one watcher invocation thread can kill!\n              if (killed.compareAndSet(false, true))  {\n                log.info(\"Monkey thread found 2 replicas for {} {}\", AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n                CloudJettyRunner cjetty = shardToLeaderJetty.get(SHARD1);\n                try {\n                  Thread.sleep(1000 + random().nextInt(500));\n                  ChaosMonkey.kill(cjetty);\n                  stop.set(true);\n                  return true;\n                } catch (Exception e) {\n                  log.error(\"Monkey unable to kill jetty at port \" + cjetty.jetty.getLocalPort(), e);\n                }\n              }\n            }\n            log.info(\"Monkey thread found only one replica for {} {}\", AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n            return false;\n          }\n        });\n      }\n    };\n\n    Thread monkeyThread = null;\n    monkeyThread = new Thread(monkey);\n    monkeyThread.start();\n    try {\n      CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n      splitShard.setShardName(SHARD1);\n      String asyncId = splitShard.processAsync(cloudClient);\n      RequestStatusState splitStatus = null;\n      try {\n        splitStatus = CollectionAdminRequest.requestStatus(asyncId).waitFor(cloudClient, 120);\n      } catch (Exception e) {\n        log.warn(\"Failed to get request status, maybe because the overseer node was shutdown by monkey\", e);\n      }\n\n      // we don't care if the split failed because we are injecting faults and it is likely\n      // that the split has failed but in any case we want to assert that all docs that got\n      // indexed are available in SolrCloud and if the split succeeded then all replicas of the sub-shard\n      // must be consistent (i.e. have same numdocs)\n\n      log.info(\"Shard split request state is COMPLETED\");\n      stop.set(true);\n      monkeyThread.join();\n      Set<String> addFails = new HashSet<>();\n      Set<String> deleteFails = new HashSet<>();\n      for (StoppableIndexingThread indexer : indexers) {\n        addFails.addAll(indexer.getAddFails());\n        deleteFails.addAll(indexer.getDeleteFails());\n      }\n\n      CloudJettyRunner cjetty = shardToLeaderJetty.get(SHARD1);\n      log.info(\"Starting shard1 leader jetty at port {}\", cjetty.jetty.getLocalPort());\n      ChaosMonkey.start(cjetty.jetty);\n      cloudClient.getZkStateReader().forceUpdateCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n      log.info(\"Current collection state: {}\", printClusterStateInfo(AbstractDistribZkTestBase.DEFAULT_COLLECTION));\n\n      boolean replicaCreationsFailed = false;\n      if (splitStatus == RequestStatusState.FAILED)  {\n        // either one or more replica creation failed (because it may have been created on the same parent shard leader node)\n        // or the split may have failed while trying to soft-commit *after* all replicas have been created\n        // the latter counts as a successful switch even if the API doesn't say so\n        // so we must find a way to distinguish between the two\n        // an easy way to do that is to look at the sub-shard replicas and check if the replica core actually exists\n        // instead of existing solely inside the cluster state\n        DocCollection collectionState = cloudClient.getZkStateReader().getClusterState().getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n        Slice slice10 = collectionState.getSlice(SHARD1_0);\n        Slice slice11 = collectionState.getSlice(SHARD1_1);\n        if (slice10 != null && slice11 != null) {\n          for (Replica replica : slice10) {\n            if (!doesReplicaCoreExist(replica)) {\n              replicaCreationsFailed = true;\n              break;\n            }\n          }\n          for (Replica replica : slice11) {\n            if (!doesReplicaCoreExist(replica)) {\n              replicaCreationsFailed = true;\n              break;\n            }\n          }\n        }\n      }\n\n      // true if sub-shard states switch to 'active' eventually\n      AtomicBoolean areSubShardsActive = new AtomicBoolean(false);\n\n      if (!replicaCreationsFailed)  {\n        // all sub-shard replicas were created successfully so all cores must recover eventually\n        waitForRecoveriesToFinish(AbstractDistribZkTestBase.DEFAULT_COLLECTION, true);\n        // let's wait for the overseer to switch shard states\n        CountDownLatch latch = new CountDownLatch(1);\n        cloudClient.getZkStateReader().registerCollectionStateWatcher(AbstractDistribZkTestBase.DEFAULT_COLLECTION, new CollectionStateWatcher() {\n          @Override\n          public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n            Slice parent = collectionState.getSlice(SHARD1);\n            Slice slice10 = collectionState.getSlice(SHARD1_0);\n            Slice slice11 = collectionState.getSlice(SHARD1_1);\n            if (slice10 != null && slice11 != null &&\n                parent.getState() == Slice.State.INACTIVE &&\n                slice10.getState() == Slice.State.ACTIVE &&\n                slice11.getState() == Slice.State.ACTIVE) {\n              areSubShardsActive.set(true);\n              latch.countDown();\n              return true; // removes the watch\n            } else if (slice10 != null && slice11 != null &&\n                parent.getState() == Slice.State.ACTIVE &&\n                slice10.getState() == Slice.State.RECOVERY_FAILED &&\n                slice11.getState() == Slice.State.RECOVERY_FAILED) {\n              areSubShardsActive.set(false);\n              latch.countDown();\n              return true;\n            }\n            return false;\n          }\n        });\n\n        latch.await(2, TimeUnit.MINUTES);\n\n        if (latch.getCount() != 0)  {\n          // sanity check\n          fail(\"We think that split was successful but sub-shard states were not updated even after 2 minutes.\");\n        }\n      }\n\n      cloudClient.commit(); // for visibility of results on sub-shards\n\n      checkShardConsistency(true, true, addFails, deleteFails);\n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      assertEquals(\"Found \" + ctrlDocs + \" control docs and \" + cloudClientDocs + \" cloud docs\", ctrlDocs, cloudClientDocs);\n\n      // check consistency of sub-shard replica explicitly because checkShardConsistency methods doesn't\n      // handle new shards/replica so well.\n      if (areSubShardsActive.get()) {\n        ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n        DocCollection collection = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n        int numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_0));\n        assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_0\", 2, numReplicasChecked);\n        numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_1));\n        assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_1\", 2, numReplicasChecked);\n      }\n    } finally {\n      stop.set(true);\n      monkeyThread.join();\n    }\n  }\n\n","sourceOld":"  @Test\n  public void testSplitWithChaosMonkey() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    List<StoppableIndexingThread> indexers = new ArrayList<>();\n    try {\n      for (int i = 0; i < 1; i++) {\n        StoppableIndexingThread thread = new StoppableIndexingThread(controlClient, cloudClient, String.valueOf(i), true);\n        indexers.add(thread);\n        thread.start();\n      }\n      Thread.sleep(1000); // give the indexers some time to do their work\n    } catch (Exception e) {\n      log.error(\"Error in test\", e);\n    } finally {\n      for (StoppableIndexingThread indexer : indexers) {\n        indexer.safeStop();\n        indexer.join();\n      }\n    }\n\n    cloudClient.commit();\n    controlClient.commit();\n\n    AtomicBoolean stop = new AtomicBoolean();\n    AtomicBoolean killed = new AtomicBoolean(false);\n    Runnable monkey = new Runnable() {\n      @Override\n      public void run() {\n        ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n        zkStateReader.registerCollectionStateWatcher(AbstractDistribZkTestBase.DEFAULT_COLLECTION, new CollectionStateWatcher() {\n          @Override\n          public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n            if (stop.get()) {\n              return true; // abort and remove the watch\n            }\n            Slice slice = collectionState.getSlice(SHARD1_0);\n            if (slice != null && slice.getReplicas().size() > 1) {\n              // ensure that only one watcher invocation thread can kill!\n              if (killed.compareAndSet(false, true))  {\n                log.info(\"Monkey thread found 2 replicas for {} {}\", AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n                CloudJettyRunner cjetty = shardToLeaderJetty.get(SHARD1);\n                try {\n                  Thread.sleep(1000 + random().nextInt(500));\n                  ChaosMonkey.kill(cjetty);\n                  stop.set(true);\n                  return true;\n                } catch (Exception e) {\n                  log.error(\"Monkey unable to kill jetty at port \" + cjetty.jetty.getLocalPort(), e);\n                }\n              }\n            }\n            log.info(\"Monkey thread found only one replica for {} {}\", AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n            return false;\n          }\n        });\n      }\n    };\n\n    Thread monkeyThread = null;\n    monkeyThread = new Thread(monkey);\n    monkeyThread.start();\n    try {\n      CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n      splitShard.setShardName(SHARD1);\n      String asyncId = splitShard.processAsync(cloudClient);\n      RequestStatusState splitStatus = null;\n      try {\n        splitStatus = CollectionAdminRequest.requestStatus(asyncId).waitFor(cloudClient, 120);\n      } catch (Exception e) {\n        log.warn(\"Failed to get request status, maybe because the overseer node was shutdown by monkey\", e);\n      }\n\n      // we don't care if the split failed because we are injecting faults and it is likely\n      // that the split has failed but in any case we want to assert that all docs that got\n      // indexed are available in SolrCloud and if the split succeeded then all replicas of the sub-shard\n      // must be consistent (i.e. have same numdocs)\n\n      log.info(\"Shard split request state is COMPLETED\");\n      stop.set(true);\n      monkeyThread.join();\n      Set<String> addFails = new HashSet<>();\n      Set<String> deleteFails = new HashSet<>();\n      for (StoppableIndexingThread indexer : indexers) {\n        addFails.addAll(indexer.getAddFails());\n        deleteFails.addAll(indexer.getDeleteFails());\n      }\n\n      CloudJettyRunner cjetty = shardToLeaderJetty.get(SHARD1);\n      log.info(\"Starting shard1 leader jetty at port {}\", cjetty.jetty.getLocalPort());\n      ChaosMonkey.start(cjetty.jetty);\n      cloudClient.getZkStateReader().forceUpdateCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n      log.info(\"Current collection state: {}\", printClusterStateInfo(AbstractDistribZkTestBase.DEFAULT_COLLECTION));\n\n      boolean replicaCreationsFailed = false;\n      if (splitStatus == RequestStatusState.FAILED)  {\n        // either one or more replica creation failed (because it may have been created on the same parent shard leader node)\n        // or the split may have failed while trying to soft-commit *after* all replicas have been created\n        // the latter counts as a successful switch even if the API doesn't say so\n        // so we must find a way to distinguish between the two\n        // an easy way to do that is to look at the sub-shard replicas and check if the replica core actually exists\n        // instead of existing solely inside the cluster state\n        DocCollection collectionState = cloudClient.getZkStateReader().getClusterState().getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n        Slice slice10 = collectionState.getSlice(SHARD1_0);\n        Slice slice11 = collectionState.getSlice(SHARD1_1);\n        if (slice10 != null && slice11 != null) {\n          for (Replica replica : slice10) {\n            if (!doesReplicaCoreExist(replica)) {\n              replicaCreationsFailed = true;\n              break;\n            }\n          }\n          for (Replica replica : slice11) {\n            if (!doesReplicaCoreExist(replica)) {\n              replicaCreationsFailed = true;\n              break;\n            }\n          }\n        }\n      }\n\n      // true if sub-shard states switch to 'active' eventually\n      AtomicBoolean areSubShardsActive = new AtomicBoolean(false);\n\n      if (!replicaCreationsFailed)  {\n        // all sub-shard replicas were created successfully so all cores must recover eventually\n        waitForRecoveriesToFinish(AbstractDistribZkTestBase.DEFAULT_COLLECTION, true);\n        // let's wait for the overseer to switch shard states\n        CountDownLatch latch = new CountDownLatch(1);\n        cloudClient.getZkStateReader().registerCollectionStateWatcher(AbstractDistribZkTestBase.DEFAULT_COLLECTION, new CollectionStateWatcher() {\n          @Override\n          public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n            Slice parent = collectionState.getSlice(SHARD1);\n            Slice slice10 = collectionState.getSlice(SHARD1_0);\n            Slice slice11 = collectionState.getSlice(SHARD1_1);\n            if (slice10 != null && slice11 != null &&\n                parent.getState() == Slice.State.INACTIVE &&\n                slice10.getState() == Slice.State.ACTIVE &&\n                slice11.getState() == Slice.State.ACTIVE) {\n              areSubShardsActive.set(true);\n              latch.countDown();\n              return true; // removes the watch\n            } else if (slice10 != null && slice11 != null &&\n                parent.getState() == Slice.State.ACTIVE &&\n                slice10.getState() == Slice.State.RECOVERY_FAILED &&\n                slice11.getState() == Slice.State.RECOVERY_FAILED) {\n              areSubShardsActive.set(false);\n              latch.countDown();\n              return true;\n            }\n            return false;\n          }\n        });\n\n        latch.await(2, TimeUnit.MINUTES);\n\n        if (latch.getCount() != 0)  {\n          // sanity check\n          fail(\"We think that split was successful but sub-shard states were not updated even after 2 minutes.\");\n        }\n      }\n\n      cloudClient.commit(); // for visibility of results on sub-shards\n\n      checkShardConsistency(true, true, addFails, deleteFails);\n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      assertEquals(\"Found \" + ctrlDocs + \" control docs and \" + cloudClientDocs + \" cloud docs\", ctrlDocs, cloudClientDocs);\n\n      // check consistency of sub-shard replica explicitly because checkShardConsistency methods doesn't\n      // handle new shards/replica so well.\n      if (areSubShardsActive.get()) {\n        ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n        DocCollection collection = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n        int numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_0));\n        assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_0\", 2, numReplicasChecked);\n        numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_1));\n        assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_1\", 2, numReplicasChecked);\n      }\n    } finally {\n      stop.set(true);\n      monkeyThread.join();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b94236357aaa22b76c10629851fe4e376e0cea82","date":1516710914,"type":5,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/ShardSplitTest#testSplitWithChaosMonkey().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#testSplitWithChaosMonkey().mjava","sourceNew":"  @Test\n  public void testSplitWithChaosMonkey() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    List<StoppableIndexingThread> indexers = new ArrayList<>();\n    try {\n      for (int i = 0; i < 1; i++) {\n        StoppableIndexingThread thread = new StoppableIndexingThread(controlClient, cloudClient, String.valueOf(i), true);\n        indexers.add(thread);\n        thread.start();\n      }\n      Thread.sleep(1000); // give the indexers some time to do their work\n    } catch (Exception e) {\n      log.error(\"Error in test\", e);\n    } finally {\n      for (StoppableIndexingThread indexer : indexers) {\n        indexer.safeStop();\n        indexer.join();\n      }\n    }\n\n    cloudClient.commit();\n    controlClient.commit();\n\n    AtomicBoolean stop = new AtomicBoolean();\n    AtomicBoolean killed = new AtomicBoolean(false);\n    Runnable monkey = new Runnable() {\n      @Override\n      public void run() {\n        ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n        zkStateReader.registerCollectionStateWatcher(AbstractDistribZkTestBase.DEFAULT_COLLECTION, new CollectionStateWatcher() {\n          @Override\n          public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n            if (stop.get()) {\n              return true; // abort and remove the watch\n            }\n            Slice slice = collectionState.getSlice(SHARD1_0);\n            if (slice != null && slice.getReplicas().size() > 1) {\n              // ensure that only one watcher invocation thread can kill!\n              if (killed.compareAndSet(false, true))  {\n                log.info(\"Monkey thread found 2 replicas for {} {}\", AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n                CloudJettyRunner cjetty = shardToLeaderJetty.get(SHARD1);\n                try {\n                  Thread.sleep(1000 + random().nextInt(500));\n                  ChaosMonkey.kill(cjetty);\n                  stop.set(true);\n                  return true;\n                } catch (Exception e) {\n                  log.error(\"Monkey unable to kill jetty at port \" + cjetty.jetty.getLocalPort(), e);\n                }\n              }\n            }\n            log.info(\"Monkey thread found only one replica for {} {}\", AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n            return false;\n          }\n        });\n      }\n    };\n\n    Thread monkeyThread = null;\n    monkeyThread = new Thread(monkey);\n    monkeyThread.start();\n    try {\n      CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n      splitShard.setShardName(SHARD1);\n      String asyncId = splitShard.processAsync(cloudClient);\n      RequestStatusState splitStatus = null;\n      try {\n        splitStatus = CollectionAdminRequest.requestStatus(asyncId).waitFor(cloudClient, 120);\n      } catch (Exception e) {\n        log.warn(\"Failed to get request status, maybe because the overseer node was shutdown by monkey\", e);\n      }\n\n      // we don't care if the split failed because we are injecting faults and it is likely\n      // that the split has failed but in any case we want to assert that all docs that got\n      // indexed are available in SolrCloud and if the split succeeded then all replicas of the sub-shard\n      // must be consistent (i.e. have same numdocs)\n\n      log.info(\"Shard split request state is COMPLETED\");\n      stop.set(true);\n      monkeyThread.join();\n      Set<String> addFails = new HashSet<>();\n      Set<String> deleteFails = new HashSet<>();\n      for (StoppableIndexingThread indexer : indexers) {\n        addFails.addAll(indexer.getAddFails());\n        deleteFails.addAll(indexer.getDeleteFails());\n      }\n\n      CloudJettyRunner cjetty = shardToLeaderJetty.get(SHARD1);\n      log.info(\"Starting shard1 leader jetty at port {}\", cjetty.jetty.getLocalPort());\n      ChaosMonkey.start(cjetty.jetty);\n      cloudClient.getZkStateReader().forceUpdateCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n      log.info(\"Current collection state: {}\", printClusterStateInfo(AbstractDistribZkTestBase.DEFAULT_COLLECTION));\n\n      boolean replicaCreationsFailed = false;\n      if (splitStatus == RequestStatusState.FAILED)  {\n        // either one or more replica creation failed (because it may have been created on the same parent shard leader node)\n        // or the split may have failed while trying to soft-commit *after* all replicas have been created\n        // the latter counts as a successful switch even if the API doesn't say so\n        // so we must find a way to distinguish between the two\n        // an easy way to do that is to look at the sub-shard replicas and check if the replica core actually exists\n        // instead of existing solely inside the cluster state\n        DocCollection collectionState = cloudClient.getZkStateReader().getClusterState().getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n        Slice slice10 = collectionState.getSlice(SHARD1_0);\n        Slice slice11 = collectionState.getSlice(SHARD1_1);\n        if (slice10 != null && slice11 != null) {\n          for (Replica replica : slice10) {\n            if (!doesReplicaCoreExist(replica)) {\n              replicaCreationsFailed = true;\n              break;\n            }\n          }\n          for (Replica replica : slice11) {\n            if (!doesReplicaCoreExist(replica)) {\n              replicaCreationsFailed = true;\n              break;\n            }\n          }\n        }\n      }\n\n      // true if sub-shard states switch to 'active' eventually\n      AtomicBoolean areSubShardsActive = new AtomicBoolean(false);\n\n      if (!replicaCreationsFailed)  {\n        // all sub-shard replicas were created successfully so all cores must recover eventually\n        waitForRecoveriesToFinish(AbstractDistribZkTestBase.DEFAULT_COLLECTION, true);\n        // let's wait for the overseer to switch shard states\n        CountDownLatch latch = new CountDownLatch(1);\n        cloudClient.getZkStateReader().registerCollectionStateWatcher(AbstractDistribZkTestBase.DEFAULT_COLLECTION, new CollectionStateWatcher() {\n          @Override\n          public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n            Slice parent = collectionState.getSlice(SHARD1);\n            Slice slice10 = collectionState.getSlice(SHARD1_0);\n            Slice slice11 = collectionState.getSlice(SHARD1_1);\n            if (slice10 != null && slice11 != null &&\n                parent.getState() == Slice.State.INACTIVE &&\n                slice10.getState() == Slice.State.ACTIVE &&\n                slice11.getState() == Slice.State.ACTIVE) {\n              areSubShardsActive.set(true);\n              latch.countDown();\n              return true; // removes the watch\n            } else if (slice10 != null && slice11 != null &&\n                parent.getState() == Slice.State.ACTIVE &&\n                slice10.getState() == Slice.State.RECOVERY_FAILED &&\n                slice11.getState() == Slice.State.RECOVERY_FAILED) {\n              areSubShardsActive.set(false);\n              latch.countDown();\n              return true;\n            }\n            return false;\n          }\n        });\n\n        latch.await(2, TimeUnit.MINUTES);\n\n        if (latch.getCount() != 0)  {\n          // sanity check\n          fail(\"We think that split was successful but sub-shard states were not updated even after 2 minutes.\");\n        }\n      }\n\n      cloudClient.commit(); // for visibility of results on sub-shards\n\n      checkShardConsistency(true, true, addFails, deleteFails);\n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      assertEquals(\"Found \" + ctrlDocs + \" control docs and \" + cloudClientDocs + \" cloud docs\", ctrlDocs, cloudClientDocs);\n\n      // check consistency of sub-shard replica explicitly because checkShardConsistency methods doesn't\n      // handle new shards/replica so well.\n      if (areSubShardsActive.get()) {\n        ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n        DocCollection collection = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n        int numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_0));\n        assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_0\", 2, numReplicasChecked);\n        numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_1));\n        assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_1\", 2, numReplicasChecked);\n      }\n    } finally {\n      stop.set(true);\n      monkeyThread.join();\n    }\n  }\n\n","sourceOld":"  @Test\n  public void testSplitWithChaosMonkey() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    List<StoppableIndexingThread> indexers = new ArrayList<>();\n    try {\n      for (int i = 0; i < 1; i++) {\n        StoppableIndexingThread thread = new StoppableIndexingThread(controlClient, cloudClient, String.valueOf(i), true);\n        indexers.add(thread);\n        thread.start();\n      }\n      Thread.sleep(1000); // give the indexers some time to do their work\n    } catch (Exception e) {\n      log.error(\"Error in test\", e);\n    } finally {\n      for (StoppableIndexingThread indexer : indexers) {\n        indexer.safeStop();\n        indexer.join();\n      }\n    }\n\n    cloudClient.commit();\n    controlClient.commit();\n\n    AtomicBoolean stop = new AtomicBoolean();\n    AtomicBoolean killed = new AtomicBoolean(false);\n    Runnable monkey = new Runnable() {\n      @Override\n      public void run() {\n        ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n        zkStateReader.registerCollectionStateWatcher(AbstractDistribZkTestBase.DEFAULT_COLLECTION, new CollectionStateWatcher() {\n          @Override\n          public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n            if (stop.get()) {\n              return true; // abort and remove the watch\n            }\n            Slice slice = collectionState.getSlice(SHARD1_0);\n            if (slice != null && slice.getReplicas().size() > 1) {\n              // ensure that only one watcher invocation thread can kill!\n              if (killed.compareAndSet(false, true))  {\n                log.info(\"Monkey thread found 2 replicas for {} {}\", AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n                CloudJettyRunner cjetty = shardToLeaderJetty.get(SHARD1);\n                try {\n                  Thread.sleep(1000 + random().nextInt(500));\n                  ChaosMonkey.kill(cjetty);\n                  stop.set(true);\n                  return true;\n                } catch (Exception e) {\n                  log.error(\"Monkey unable to kill jetty at port \" + cjetty.jetty.getLocalPort(), e);\n                }\n              }\n            }\n            log.info(\"Monkey thread found only one replica for {} {}\", AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n            return false;\n          }\n        });\n      }\n    };\n\n    Thread monkeyThread = null;\n    monkeyThread = new Thread(monkey);\n    monkeyThread.start();\n    try {\n      CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n      splitShard.setShardName(SHARD1);\n      String asyncId = splitShard.processAsync(cloudClient);\n      RequestStatusState splitStatus = null;\n      try {\n        splitStatus = CollectionAdminRequest.requestStatus(asyncId).waitFor(cloudClient, 120);\n      } catch (Exception e) {\n        log.warn(\"Failed to get request status, maybe because the overseer node was shutdown by monkey\", e);\n      }\n\n      // we don't care if the split failed because we are injecting faults and it is likely\n      // that the split has failed but in any case we want to assert that all docs that got\n      // indexed are available in SolrCloud and if the split succeeded then all replicas of the sub-shard\n      // must be consistent (i.e. have same numdocs)\n\n      log.info(\"Shard split request state is COMPLETED\");\n      stop.set(true);\n      monkeyThread.join();\n      Set<String> addFails = new HashSet<>();\n      Set<String> deleteFails = new HashSet<>();\n      for (StoppableIndexingThread indexer : indexers) {\n        addFails.addAll(indexer.getAddFails());\n        deleteFails.addAll(indexer.getDeleteFails());\n      }\n\n      CloudJettyRunner cjetty = shardToLeaderJetty.get(SHARD1);\n      log.info(\"Starting shard1 leader jetty at port {}\", cjetty.jetty.getLocalPort());\n      ChaosMonkey.start(cjetty.jetty);\n      cloudClient.getZkStateReader().forceUpdateCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n      log.info(\"Current collection state: {}\", printClusterStateInfo(AbstractDistribZkTestBase.DEFAULT_COLLECTION));\n\n      boolean replicaCreationsFailed = false;\n      if (splitStatus == RequestStatusState.FAILED)  {\n        // either one or more replica creation failed (because it may have been created on the same parent shard leader node)\n        // or the split may have failed while trying to soft-commit *after* all replicas have been created\n        // the latter counts as a successful switch even if the API doesn't say so\n        // so we must find a way to distinguish between the two\n        // an easy way to do that is to look at the sub-shard replicas and check if the replica core actually exists\n        // instead of existing solely inside the cluster state\n        DocCollection collectionState = cloudClient.getZkStateReader().getClusterState().getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n        Slice slice10 = collectionState.getSlice(SHARD1_0);\n        Slice slice11 = collectionState.getSlice(SHARD1_1);\n        if (slice10 != null && slice11 != null) {\n          for (Replica replica : slice10) {\n            if (!doesReplicaCoreExist(replica)) {\n              replicaCreationsFailed = true;\n              break;\n            }\n          }\n          for (Replica replica : slice11) {\n            if (!doesReplicaCoreExist(replica)) {\n              replicaCreationsFailed = true;\n              break;\n            }\n          }\n        }\n      }\n\n      // true if sub-shard states switch to 'active' eventually\n      AtomicBoolean areSubShardsActive = new AtomicBoolean(false);\n\n      if (!replicaCreationsFailed)  {\n        // all sub-shard replicas were created successfully so all cores must recover eventually\n        waitForRecoveriesToFinish(AbstractDistribZkTestBase.DEFAULT_COLLECTION, true);\n        // let's wait for the overseer to switch shard states\n        CountDownLatch latch = new CountDownLatch(1);\n        cloudClient.getZkStateReader().registerCollectionStateWatcher(AbstractDistribZkTestBase.DEFAULT_COLLECTION, new CollectionStateWatcher() {\n          @Override\n          public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n            Slice parent = collectionState.getSlice(SHARD1);\n            Slice slice10 = collectionState.getSlice(SHARD1_0);\n            Slice slice11 = collectionState.getSlice(SHARD1_1);\n            if (slice10 != null && slice11 != null &&\n                parent.getState() == Slice.State.INACTIVE &&\n                slice10.getState() == Slice.State.ACTIVE &&\n                slice11.getState() == Slice.State.ACTIVE) {\n              areSubShardsActive.set(true);\n              latch.countDown();\n              return true; // removes the watch\n            } else if (slice10 != null && slice11 != null &&\n                parent.getState() == Slice.State.ACTIVE &&\n                slice10.getState() == Slice.State.RECOVERY_FAILED &&\n                slice11.getState() == Slice.State.RECOVERY_FAILED) {\n              areSubShardsActive.set(false);\n              latch.countDown();\n              return true;\n            }\n            return false;\n          }\n        });\n\n        latch.await(2, TimeUnit.MINUTES);\n\n        if (latch.getCount() != 0)  {\n          // sanity check\n          fail(\"We think that split was successful but sub-shard states were not updated even after 2 minutes.\");\n        }\n      }\n\n      cloudClient.commit(); // for visibility of results on sub-shards\n\n      checkShardConsistency(true, true, addFails, deleteFails);\n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      assertEquals(\"Found \" + ctrlDocs + \" control docs and \" + cloudClientDocs + \" cloud docs\", ctrlDocs, cloudClientDocs);\n\n      // check consistency of sub-shard replica explicitly because checkShardConsistency methods doesn't\n      // handle new shards/replica so well.\n      if (areSubShardsActive.get()) {\n        ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n        DocCollection collection = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n        int numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_0));\n        assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_0\", 2, numReplicasChecked);\n        numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_1));\n        assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_1\", 2, numReplicasChecked);\n      }\n    } finally {\n      stop.set(true);\n      monkeyThread.join();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"b94236357aaa22b76c10629851fe4e376e0cea82":["06a8aa646edc93b6f7e76ded12e41f1e6c04e7c4","6146c07c0dee1ae1e42926167acd127fed5ef59d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","89424def13674ea17829b41c5883c54ecc31a132"],"6a23ab64d81a448ad6ec571cbfc9599cc09b4e4b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"6146c07c0dee1ae1e42926167acd127fed5ef59d":["06a8aa646edc93b6f7e76ded12e41f1e6c04e7c4"],"06a8aa646edc93b6f7e76ded12e41f1e6c04e7c4":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b94236357aaa22b76c10629851fe4e376e0cea82"],"89424def13674ea17829b41c5883c54ecc31a132":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","6a23ab64d81a448ad6ec571cbfc9599cc09b4e4b"]},"commit2Childs":{"b94236357aaa22b76c10629851fe4e376e0cea82":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","6a23ab64d81a448ad6ec571cbfc9599cc09b4e4b","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","89424def13674ea17829b41c5883c54ecc31a132"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["06a8aa646edc93b6f7e76ded12e41f1e6c04e7c4","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"6a23ab64d81a448ad6ec571cbfc9599cc09b4e4b":["89424def13674ea17829b41c5883c54ecc31a132"],"6146c07c0dee1ae1e42926167acd127fed5ef59d":["b94236357aaa22b76c10629851fe4e376e0cea82"],"06a8aa646edc93b6f7e76ded12e41f1e6c04e7c4":["b94236357aaa22b76c10629851fe4e376e0cea82","6146c07c0dee1ae1e42926167acd127fed5ef59d"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"89424def13674ea17829b41c5883c54ecc31a132":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}