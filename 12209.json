{"path":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,IndexOutput,IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","commits":[{"id":"78e689a3b60e84c75dc6dd7b181a71fc19ef8482","date":1591689554,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,IndexOutput,IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public Runnable merge(IndexOutput metaOut, IndexOutput indexOut, IndexOutput dataOut, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    OneDimensionBKDWriter oneDimWriter = new OneDimensionBKDWriter(metaOut, indexOut, dataOut);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      oneDimWriter.add(reader.state.scratchDataPackedValue, reader.docID);\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n    }\n\n    return oneDimWriter.finish();\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    OneDimensionBKDWriter oneDimWriter = new OneDimensionBKDWriter(out);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      oneDimWriter.add(reader.state.scratchDataPackedValue, reader.docID);\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n    }\n\n    return oneDimWriter.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bb94bf667d51f9c390c99d97afb36b7caab6b6e9","date":1599548621,"type":3,"author":"Ignacio Vera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,IndexOutput,IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,IndexOutput,IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public Runnable merge(IndexOutput metaOut, IndexOutput indexOut, IndexOutput dataOut, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(config.bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    OneDimensionBKDWriter oneDimWriter = new OneDimensionBKDWriter(metaOut, indexOut, dataOut);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      oneDimWriter.add(reader.state.scratchDataPackedValue, reader.docID);\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n    }\n\n    return oneDimWriter.finish();\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public Runnable merge(IndexOutput metaOut, IndexOutput indexOut, IndexOutput dataOut, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    OneDimensionBKDWriter oneDimWriter = new OneDimensionBKDWriter(metaOut, indexOut, dataOut);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      oneDimWriter.add(reader.state.scratchDataPackedValue, reader.docID);\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n    }\n\n    return oneDimWriter.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"bb94bf667d51f9c390c99d97afb36b7caab6b6e9":["78e689a3b60e84c75dc6dd7b181a71fc19ef8482"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"78e689a3b60e84c75dc6dd7b181a71fc19ef8482":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["bb94bf667d51f9c390c99d97afb36b7caab6b6e9"]},"commit2Childs":{"bb94bf667d51f9c390c99d97afb36b7caab6b6e9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["78e689a3b60e84c75dc6dd7b181a71fc19ef8482"],"78e689a3b60e84c75dc6dd7b181a71fc19ef8482":["bb94bf667d51f9c390c99d97afb36b7caab6b6e9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}