{"path":"lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsWriter#flush().mjava","commits":[{"id":"3d5291145ae0cea7e6e6a2379f3a32643bf71bf6","date":1411857884,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsWriter#flush().mjava","pathOld":"/dev/null","sourceNew":"  private void flush() throws IOException {\n    final int chunkDocs = pendingDocs.size();\n    assert chunkDocs > 0 : chunkDocs;\n\n    // write the index file\n    indexWriter.writeIndex(chunkDocs, vectorsStream.getFilePointer());\n\n    final int docBase = numDocs - chunkDocs;\n    vectorsStream.writeVInt(docBase);\n    vectorsStream.writeVInt(chunkDocs);\n\n    // total number of fields of the chunk\n    final int totalFields = flushNumFields(chunkDocs);\n\n    if (totalFields > 0) {\n      // unique field numbers (sorted)\n      final int[] fieldNums = flushFieldNums();\n      // offsets in the array of unique field numbers\n      flushFields(totalFields, fieldNums);\n      // flags (does the field have positions, offsets, payloads?)\n      flushFlags(totalFields, fieldNums);\n      // number of terms of each field\n      flushNumTerms(totalFields);\n      // prefix and suffix lengths for each field\n      flushTermLengths();\n      // term freqs - 1 (because termFreq is always >=1) for each term\n      flushTermFreqs();\n      // positions for all terms, when enabled\n      flushPositions();\n      // offsets for all terms, when enabled\n      flushOffsets(fieldNums);\n      // payload lengths for all terms, when enabled\n      flushPayloadLengths();\n\n      // compress terms and payloads and write them to the output\n      compressor.compress(termSuffixes.bytes, 0, termSuffixes.length, vectorsStream);\n    }\n\n    // reset\n    pendingDocs.clear();\n    curDoc = null;\n    curField = null;\n    termSuffixes.length = 0;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9bb9a29a5e71a90295f175df8919802993142c9a","date":1412517673,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsWriter#flush().mjava","pathOld":"/dev/null","sourceNew":"  private void flush() throws IOException {\n    final int chunkDocs = pendingDocs.size();\n    assert chunkDocs > 0 : chunkDocs;\n\n    // write the index file\n    indexWriter.writeIndex(chunkDocs, vectorsStream.getFilePointer());\n\n    final int docBase = numDocs - chunkDocs;\n    vectorsStream.writeVInt(docBase);\n    vectorsStream.writeVInt(chunkDocs);\n\n    // total number of fields of the chunk\n    final int totalFields = flushNumFields(chunkDocs);\n\n    if (totalFields > 0) {\n      // unique field numbers (sorted)\n      final int[] fieldNums = flushFieldNums();\n      // offsets in the array of unique field numbers\n      flushFields(totalFields, fieldNums);\n      // flags (does the field have positions, offsets, payloads?)\n      flushFlags(totalFields, fieldNums);\n      // number of terms of each field\n      flushNumTerms(totalFields);\n      // prefix and suffix lengths for each field\n      flushTermLengths();\n      // term freqs - 1 (because termFreq is always >=1) for each term\n      flushTermFreqs();\n      // positions for all terms, when enabled\n      flushPositions();\n      // offsets for all terms, when enabled\n      flushOffsets(fieldNums);\n      // payload lengths for all terms, when enabled\n      flushPayloadLengths();\n\n      // compress terms and payloads and write them to the output\n      compressor.compress(termSuffixes.bytes, 0, termSuffixes.length, vectorsStream);\n    }\n\n    // reset\n    pendingDocs.clear();\n    curDoc = null;\n    curField = null;\n    termSuffixes.length = 0;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"71387d8cb6923eb831b17a8b734608ba2e21c653","date":1414126093,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsWriter#flush().mjava","sourceNew":null,"sourceOld":"  private void flush() throws IOException {\n    final int chunkDocs = pendingDocs.size();\n    assert chunkDocs > 0 : chunkDocs;\n\n    // write the index file\n    indexWriter.writeIndex(chunkDocs, vectorsStream.getFilePointer());\n\n    final int docBase = numDocs - chunkDocs;\n    vectorsStream.writeVInt(docBase);\n    vectorsStream.writeVInt(chunkDocs);\n\n    // total number of fields of the chunk\n    final int totalFields = flushNumFields(chunkDocs);\n\n    if (totalFields > 0) {\n      // unique field numbers (sorted)\n      final int[] fieldNums = flushFieldNums();\n      // offsets in the array of unique field numbers\n      flushFields(totalFields, fieldNums);\n      // flags (does the field have positions, offsets, payloads?)\n      flushFlags(totalFields, fieldNums);\n      // number of terms of each field\n      flushNumTerms(totalFields);\n      // prefix and suffix lengths for each field\n      flushTermLengths();\n      // term freqs - 1 (because termFreq is always >=1) for each term\n      flushTermFreqs();\n      // positions for all terms, when enabled\n      flushPositions();\n      // offsets for all terms, when enabled\n      flushOffsets(fieldNums);\n      // payload lengths for all terms, when enabled\n      flushPayloadLengths();\n\n      // compress terms and payloads and write them to the output\n      compressor.compress(termSuffixes.bytes, 0, termSuffixes.length, vectorsStream);\n    }\n\n    // reset\n    pendingDocs.clear();\n    curDoc = null;\n    curField = null;\n    termSuffixes.length = 0;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"9bb9a29a5e71a90295f175df8919802993142c9a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3d5291145ae0cea7e6e6a2379f3a32643bf71bf6"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3d5291145ae0cea7e6e6a2379f3a32643bf71bf6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"71387d8cb6923eb831b17a8b734608ba2e21c653":["9bb9a29a5e71a90295f175df8919802993142c9a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["71387d8cb6923eb831b17a8b734608ba2e21c653"]},"commit2Childs":{"9bb9a29a5e71a90295f175df8919802993142c9a":["71387d8cb6923eb831b17a8b734608ba2e21c653"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9bb9a29a5e71a90295f175df8919802993142c9a","3d5291145ae0cea7e6e6a2379f3a32643bf71bf6"],"3d5291145ae0cea7e6e6a2379f3a32643bf71bf6":["9bb9a29a5e71a90295f175df8919802993142c9a"],"71387d8cb6923eb831b17a8b734608ba2e21c653":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}