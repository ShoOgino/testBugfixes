{"path":"lucene/core/src/java/org/apache/lucene/index/IndexingChain#processDocument(int,Iterable[#-extends-IndexableField]).mjava","commits":[{"id":"7a6f8af01d9b3067b143bbdc0a492720e2af97cf","date":1600157724,"type":1,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexingChain#processDocument(int,Iterable[#-extends-IndexableField]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument(int,Iterable[#-extends-IndexableField]).mjava","sourceNew":"  void processDocument(int docID, Iterable<? extends IndexableField> document) throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    startStoredFields(docID);\n    try {\n      for (IndexableField field : document) {\n        fieldCount = processField(docID, field, fieldGen, fieldCount);\n      }\n    } finally {\n      if (hasHitAbortingException == false) {\n        // Finish each indexed field name seen in the document:\n        for (int i=0;i<fieldCount;i++) {\n          fields[i].finish(docID);\n        }\n        finishStoredFields();\n      }\n    }\n\n    try {\n      termsHash.finishDocument(docID);\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      abortingExceptionConsumer.accept(th);\n      throw th;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(int docID, Iterable<? extends IndexableField> document) throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    startStoredFields(docID);\n    try {\n      for (IndexableField field : document) {\n        fieldCount = processField(docID, field, fieldGen, fieldCount);\n      }\n    } finally {\n      if (hasHitAbortingException == false) {\n        // Finish each indexed field name seen in the document:\n        for (int i=0;i<fieldCount;i++) {\n          fields[i].finish(docID);\n        }\n        finishStoredFields();\n      }\n    }\n\n    try {\n      termsHash.finishDocument(docID);\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      abortingExceptionConsumer.accept(th);\n      throw th;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"680b6449f09827f58fe987aff279e014c311d966","date":1600247985,"type":1,"author":"noblepaul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexingChain#processDocument(int,Iterable[#-extends-IndexableField]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument(int,Iterable[#-extends-IndexableField]).mjava","sourceNew":"  void processDocument(int docID, Iterable<? extends IndexableField> document) throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    startStoredFields(docID);\n    try {\n      for (IndexableField field : document) {\n        fieldCount = processField(docID, field, fieldGen, fieldCount);\n      }\n    } finally {\n      if (hasHitAbortingException == false) {\n        // Finish each indexed field name seen in the document:\n        for (int i=0;i<fieldCount;i++) {\n          fields[i].finish(docID);\n        }\n        finishStoredFields();\n      }\n    }\n\n    try {\n      termsHash.finishDocument(docID);\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      abortingExceptionConsumer.accept(th);\n      throw th;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(int docID, Iterable<? extends IndexableField> document) throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    startStoredFields(docID);\n    try {\n      for (IndexableField field : document) {\n        fieldCount = processField(docID, field, fieldGen, fieldCount);\n      }\n    } finally {\n      if (hasHitAbortingException == false) {\n        // Finish each indexed field name seen in the document:\n        for (int i=0;i<fieldCount;i++) {\n          fields[i].finish(docID);\n        }\n        finishStoredFields();\n      }\n    }\n\n    try {\n      termsHash.finishDocument(docID);\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      abortingExceptionConsumer.accept(th);\n      throw th;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"680b6449f09827f58fe987aff279e014c311d966":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","7a6f8af01d9b3067b143bbdc0a492720e2af97cf"],"7a6f8af01d9b3067b143bbdc0a492720e2af97cf":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["680b6449f09827f58fe987aff279e014c311d966"]},"commit2Childs":{"680b6449f09827f58fe987aff279e014c311d966":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"7a6f8af01d9b3067b143bbdc0a492720e2af97cf":["680b6449f09827f58fe987aff279e014c311d966"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["680b6449f09827f58fe987aff279e014c311d966","7a6f8af01d9b3067b143bbdc0a492720e2af97cf"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}