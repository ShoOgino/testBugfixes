{"path":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,long,InfoStream).mjava","commits":[{"id":"f4363cd33f6eff7fb4753574a441e2d18c1022a4","date":1498067235,"type":1,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,long,InfoStream).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","sourceNew":"  public synchronized boolean writeFieldUpdates(Directory dir, long maxDelGen, InfoStream infoStream) throws IOException {\n\n    long startTimeNS = System.nanoTime();\n    \n    assert Thread.holdsLock(writer);\n\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n\n    boolean any = false;\n    int count = 0;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, (a, b) -> Long.compare(a.delGen, b.delGen));\n      count += updates.size();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, writer.segmentInfos.getIndexCreatedVersionMajor(), IOContext.READONCE);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - info.getDelCount() - pendingDeleteCount);\n      boolean success2 = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        success2 = true;\n      } finally {\n        if (success2 == false) {\n          newReader.decRef();\n        }\n      }\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    // wrote new files, should checkpoint()\n    writer.checkpointNoSIS();\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n\n    return true;\n  }\n\n","sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, writer.segmentInfos.getIndexCreatedVersionMajor(), IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.NUMERIC);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.BINARY);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":1,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,long,InfoStream).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","sourceNew":"  public synchronized boolean writeFieldUpdates(Directory dir, long maxDelGen, InfoStream infoStream) throws IOException {\n\n    long startTimeNS = System.nanoTime();\n    \n    assert Thread.holdsLock(writer);\n\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n\n    boolean any = false;\n    int count = 0;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, (a, b) -> Long.compare(a.delGen, b.delGen));\n      count += updates.size();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, writer.segmentInfos.getIndexCreatedVersionMajor(), IOContext.READONCE);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - info.getDelCount() - pendingDeleteCount);\n      boolean success2 = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        success2 = true;\n      } finally {\n        if (success2 == false) {\n          newReader.decRef();\n        }\n      }\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    // wrote new files, should checkpoint()\n    writer.checkpointNoSIS();\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n\n    return true;\n  }\n\n","sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, writer.segmentInfos.getIndexCreatedVersionMajor(), IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.NUMERIC);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.BINARY);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,long,InfoStream).mjava","pathOld":"/dev/null","sourceNew":"  public synchronized boolean writeFieldUpdates(Directory dir, long maxDelGen, InfoStream infoStream) throws IOException {\n\n    long startTimeNS = System.nanoTime();\n    \n    assert Thread.holdsLock(writer);\n\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n\n    boolean any = false;\n    int count = 0;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, (a, b) -> Long.compare(a.delGen, b.delGen));\n      count += updates.size();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, writer.segmentInfos.getIndexCreatedVersionMajor(), IOContext.READONCE);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - info.getDelCount() - pendingDeleteCount);\n      boolean success2 = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        success2 = true;\n      } finally {\n        if (success2 == false) {\n          newReader.decRef();\n        }\n      }\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    // wrote new files, should checkpoint()\n    writer.checkpointNoSIS();\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n\n    return true;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d60c1bb96a28a26d197c36299f7b6c9c5da617a1","date":1522484702,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,long,InfoStream).mjava","sourceNew":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, IOContext.READONCE);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, pendingDeletes.getLiveDocs(), info.info.maxDoc() - info.getDelCount() - pendingDeletes.numPendingDeletes());\n      boolean success2 = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        pendingDeletes.onNewReader(reader, info);\n        success2 = true;\n      } finally {\n        if (success2 == false) {\n          newReader.decRef();\n        }\n      }\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","sourceOld":"  public synchronized boolean writeFieldUpdates(Directory dir, long maxDelGen, InfoStream infoStream) throws IOException {\n\n    long startTimeNS = System.nanoTime();\n    \n    assert Thread.holdsLock(writer);\n\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n\n    boolean any = false;\n    int count = 0;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, (a, b) -> Long.compare(a.delGen, b.delGen));\n      count += updates.size();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, writer.segmentInfos.getIndexCreatedVersionMajor(), IOContext.READONCE);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - info.getDelCount() - pendingDeleteCount);\n      boolean success2 = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        success2 = true;\n      } finally {\n        if (success2 == false) {\n          newReader.decRef();\n        }\n      }\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    // wrote new files, should checkpoint()\n    writer.checkpointNoSIS();\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n\n    return true;\n  }\n\n","bugFix":null,"bugIntro":["11b5a40d323ce34cc4159e1b8a44aeea352e0222","11b5a40d323ce34cc4159e1b8a44aeea352e0222"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"aa5e39259dfd4a68287c824d3b7e1bc9097dc895","date":1522505041,"type":5,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,long,InfoStream).mjava","sourceNew":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, IOContext.READONCE);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, pendingDeletes.getLiveDocs(), info.info.maxDoc() - info.getDelCount() - pendingDeletes.numPendingDeletes());\n      boolean success2 = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        pendingDeletes.onNewReader(reader, info);\n        success2 = true;\n      } finally {\n        if (success2 == false) {\n          newReader.decRef();\n        }\n      }\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","sourceOld":"  public synchronized boolean writeFieldUpdates(Directory dir, long maxDelGen, InfoStream infoStream) throws IOException {\n\n    long startTimeNS = System.nanoTime();\n    \n    assert Thread.holdsLock(writer);\n\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n\n    boolean any = false;\n    int count = 0;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, (a, b) -> Long.compare(a.delGen, b.delGen));\n      count += updates.size();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, writer.segmentInfos.getIndexCreatedVersionMajor(), IOContext.READONCE);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - info.getDelCount() - pendingDeleteCount);\n      boolean success2 = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        success2 = true;\n      } finally {\n        if (success2 == false) {\n          newReader.decRef();\n        }\n      }\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    // wrote new files, should checkpoint()\n    writer.checkpointNoSIS();\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"aa5e39259dfd4a68287c824d3b7e1bc9097dc895":["28288370235ed02234a64753cdbf0c6ec096304a","d60c1bb96a28a26d197c36299f7b6c9c5da617a1"],"d60c1bb96a28a26d197c36299f7b6c9c5da617a1":["28288370235ed02234a64753cdbf0c6ec096304a"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"28288370235ed02234a64753cdbf0c6ec096304a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["aa5e39259dfd4a68287c824d3b7e1bc9097dc895"]},"commit2Childs":{"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"aa5e39259dfd4a68287c824d3b7e1bc9097dc895":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d60c1bb96a28a26d197c36299f7b6c9c5da617a1":["aa5e39259dfd4a68287c824d3b7e1bc9097dc895"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":[],"28288370235ed02234a64753cdbf0c6ec096304a":["aa5e39259dfd4a68287c824d3b7e1bc9097dc895","d60c1bb96a28a26d197c36299f7b6c9c5da617a1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}