{"path":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","commits":[{"id":"3f767f8c99eaedb984df754fe61f21c5de260f94","date":1344105153,"type":1,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.CloudStateUpdater#removeCore(CloudState,ZkNodeProps).mjava","sourceNew":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.get(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.get(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.get(ZkStateReader.COLLECTION_PROP);\n\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: clusterState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = clusterState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getShards().containsKey(coreNodeName)) {\n                LinkedHashMap<String, ZkNodeProps> newShards = new LinkedHashMap<String, ZkNodeProps>();\n                newShards.putAll(slice.getShards());\n                newShards.remove(coreNodeName);\n                \n                Slice newSlice = new Slice(slice.getName(), newShards);\n                newSlices.put(slice.getName(), newSlice);\n\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            int cnt = 0;\n            for (Slice slice : newSlices.values()) {\n              cnt+=slice.getShards().size();\n            }\n            // TODO: if no nodes are left after this unload\n            // remove from zk - do we have a race where Overseer\n            // see's registered nodes and publishes though?\n            if (cnt > 0) {\n              newStates.put(collectionName, newSlices);\n            } else {\n              // TODO: it might be better logically to have this in ZkController\n              // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n              // ZkController out of the Overseer.\n              try {\n                zkClient.clean(\"/collections/\" + collectionName);\n              } catch (InterruptedException e) {\n                SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collectionName, e);\n                Thread.currentThread().interrupt();\n              } catch (KeeperException e) {\n                SolrException.log(log, \"Problem cleaning up collection in zk:\" + collectionName, e);\n              }\n            }\n          } else {\n            newStates.put(collectionName, clusterState.getSlices(collectionName));\n          }\n        }\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","sourceOld":"      /*\n       * Remove core from cloudstate\n       */\n      private CloudState removeCore(final CloudState cloudState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.get(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.get(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.get(ZkStateReader.COLLECTION_PROP);\n\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: cloudState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = cloudState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getShards().containsKey(coreNodeName)) {\n                LinkedHashMap<String, ZkNodeProps> newShards = new LinkedHashMap<String, ZkNodeProps>();\n                newShards.putAll(slice.getShards());\n                newShards.remove(coreNodeName);\n                \n                Slice newSlice = new Slice(slice.getName(), newShards);\n                newSlices.put(slice.getName(), newSlice);\n\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            int cnt = 0;\n            for (Slice slice : newSlices.values()) {\n              cnt+=slice.getShards().size();\n            }\n            // TODO: if no nodes are left after this unload\n            // remove from zk - do we have a race where Overseer\n            // see's registered nodes and publishes though?\n            if (cnt > 0) {\n              newStates.put(collectionName, newSlices);\n            } else {\n              // TODO: it might be better logically to have this in ZkController\n              // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n              // ZkController out of the Overseer.\n              try {\n                zkClient.clean(\"/collections/\" + collectionName);\n              } catch (InterruptedException e) {\n                SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collectionName, e);\n                Thread.currentThread().interrupt();\n              } catch (KeeperException e) {\n                SolrException.log(log, \"Problem cleaning up collection in zk:\" + collectionName, e);\n              }\n            }\n          } else {\n            newStates.put(collectionName, cloudState.getSlices(collectionName));\n          }\n        }\n        CloudState newState = new CloudState(cloudState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8fd5be977c105554c6a7b68afcdbc511439723ab","date":1344115570,"type":1,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.CloudStateUpdater#removeCore(CloudState,ZkNodeProps).mjava","sourceNew":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.get(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.get(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.get(ZkStateReader.COLLECTION_PROP);\n\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: clusterState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = clusterState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getShards().containsKey(coreNodeName)) {\n                LinkedHashMap<String, ZkNodeProps> newShards = new LinkedHashMap<String, ZkNodeProps>();\n                newShards.putAll(slice.getShards());\n                newShards.remove(coreNodeName);\n                \n                Slice newSlice = new Slice(slice.getName(), newShards);\n                newSlices.put(slice.getName(), newSlice);\n\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            int cnt = 0;\n            for (Slice slice : newSlices.values()) {\n              cnt+=slice.getShards().size();\n            }\n            // TODO: if no nodes are left after this unload\n            // remove from zk - do we have a race where Overseer\n            // see's registered nodes and publishes though?\n            if (cnt > 0) {\n              newStates.put(collectionName, newSlices);\n            } else {\n              // TODO: it might be better logically to have this in ZkController\n              // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n              // ZkController out of the Overseer.\n              try {\n                zkClient.clean(\"/collections/\" + collectionName);\n              } catch (InterruptedException e) {\n                SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collectionName, e);\n                Thread.currentThread().interrupt();\n              } catch (KeeperException e) {\n                SolrException.log(log, \"Problem cleaning up collection in zk:\" + collectionName, e);\n              }\n            }\n          } else {\n            newStates.put(collectionName, clusterState.getSlices(collectionName));\n          }\n        }\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","sourceOld":"      /*\n       * Remove core from cloudstate\n       */\n      private CloudState removeCore(final CloudState cloudState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.get(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.get(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.get(ZkStateReader.COLLECTION_PROP);\n\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: cloudState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = cloudState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getShards().containsKey(coreNodeName)) {\n                LinkedHashMap<String, ZkNodeProps> newShards = new LinkedHashMap<String, ZkNodeProps>();\n                newShards.putAll(slice.getShards());\n                newShards.remove(coreNodeName);\n                \n                Slice newSlice = new Slice(slice.getName(), newShards);\n                newSlices.put(slice.getName(), newSlice);\n\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            int cnt = 0;\n            for (Slice slice : newSlices.values()) {\n              cnt+=slice.getShards().size();\n            }\n            // TODO: if no nodes are left after this unload\n            // remove from zk - do we have a race where Overseer\n            // see's registered nodes and publishes though?\n            if (cnt > 0) {\n              newStates.put(collectionName, newSlices);\n            } else {\n              // TODO: it might be better logically to have this in ZkController\n              // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n              // ZkController out of the Overseer.\n              try {\n                zkClient.clean(\"/collections/\" + collectionName);\n              } catch (InterruptedException e) {\n                SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collectionName, e);\n                Thread.currentThread().interrupt();\n              } catch (KeeperException e) {\n                SolrException.log(log, \"Problem cleaning up collection in zk:\" + collectionName, e);\n              }\n            }\n          } else {\n            newStates.put(collectionName, cloudState.getSlices(collectionName));\n          }\n        }\n        CloudState newState = new CloudState(cloudState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":0,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"/dev/null","sourceNew":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.get(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.get(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.get(ZkStateReader.COLLECTION_PROP);\n\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: clusterState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = clusterState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getShards().containsKey(coreNodeName)) {\n                LinkedHashMap<String, ZkNodeProps> newShards = new LinkedHashMap<String, ZkNodeProps>();\n                newShards.putAll(slice.getShards());\n                newShards.remove(coreNodeName);\n                \n                Slice newSlice = new Slice(slice.getName(), newShards);\n                newSlices.put(slice.getName(), newSlice);\n\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            int cnt = 0;\n            for (Slice slice : newSlices.values()) {\n              cnt+=slice.getShards().size();\n            }\n            // TODO: if no nodes are left after this unload\n            // remove from zk - do we have a race where Overseer\n            // see's registered nodes and publishes though?\n            if (cnt > 0) {\n              newStates.put(collectionName, newSlices);\n            } else {\n              // TODO: it might be better logically to have this in ZkController\n              // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n              // ZkController out of the Overseer.\n              try {\n                zkClient.clean(\"/collections/\" + collectionName);\n              } catch (InterruptedException e) {\n                SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collectionName, e);\n                Thread.currentThread().interrupt();\n              } catch (KeeperException e) {\n                SolrException.log(log, \"Problem cleaning up collection in zk:\" + collectionName, e);\n              }\n            }\n          } else {\n            newStates.put(collectionName, clusterState.getSlices(collectionName));\n          }\n        }\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fa64435b5902ce266c23755a4a00691a3285dab8","date":1347243290,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.getStr(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.getStr(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: clusterState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = clusterState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getReplicasMap().containsKey(coreNodeName)) {\n                LinkedHashMap<String, Replica> newShards = new LinkedHashMap<String, Replica>();\n                newShards.putAll(slice.getReplicasMap());\n                newShards.remove(coreNodeName);\n                \n                Slice newSlice = new Slice(slice.getName(), newShards);\n                newSlices.put(slice.getName(), newSlice);\n\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            int cnt = 0;\n            for (Slice slice : newSlices.values()) {\n              cnt+=slice.getReplicasMap().size();\n            }\n            // TODO: if no nodes are left after this unload\n            // remove from zk - do we have a race where Overseer\n            // see's registered nodes and publishes though?\n            if (cnt > 0) {\n              newStates.put(collectionName, newSlices);\n            } else {\n              // TODO: it might be better logically to have this in ZkController\n              // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n              // ZkController out of the Overseer.\n              try {\n                zkClient.clean(\"/collections/\" + collectionName);\n              } catch (InterruptedException e) {\n                SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collectionName, e);\n                Thread.currentThread().interrupt();\n              } catch (KeeperException e) {\n                SolrException.log(log, \"Problem cleaning up collection in zk:\" + collectionName, e);\n              }\n            }\n          } else {\n            newStates.put(collectionName, clusterState.getSlices(collectionName));\n          }\n        }\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","sourceOld":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.get(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.get(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.get(ZkStateReader.COLLECTION_PROP);\n\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: clusterState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = clusterState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getShards().containsKey(coreNodeName)) {\n                LinkedHashMap<String, ZkNodeProps> newShards = new LinkedHashMap<String, ZkNodeProps>();\n                newShards.putAll(slice.getShards());\n                newShards.remove(coreNodeName);\n                \n                Slice newSlice = new Slice(slice.getName(), newShards);\n                newSlices.put(slice.getName(), newSlice);\n\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            int cnt = 0;\n            for (Slice slice : newSlices.values()) {\n              cnt+=slice.getShards().size();\n            }\n            // TODO: if no nodes are left after this unload\n            // remove from zk - do we have a race where Overseer\n            // see's registered nodes and publishes though?\n            if (cnt > 0) {\n              newStates.put(collectionName, newSlices);\n            } else {\n              // TODO: it might be better logically to have this in ZkController\n              // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n              // ZkController out of the Overseer.\n              try {\n                zkClient.clean(\"/collections/\" + collectionName);\n              } catch (InterruptedException e) {\n                SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collectionName, e);\n                Thread.currentThread().interrupt();\n              } catch (KeeperException e) {\n                SolrException.log(log, \"Problem cleaning up collection in zk:\" + collectionName, e);\n              }\n            }\n          } else {\n            newStates.put(collectionName, clusterState.getSlices(collectionName));\n          }\n        }\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b456de5d7facbce08ccaf1f4fac0b71a5bf39add","date":1348087324,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.getStr(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.getStr(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: clusterState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = clusterState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getReplicasMap().containsKey(coreNodeName)) {\n                Map<String, Replica> newReplicas = slice.getReplicasCopy();\n                newReplicas.remove(coreNodeName);\n                Slice newSlice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n                newSlices.put(slice.getName(), newSlice);\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            int cnt = 0;\n            for (Slice slice : newSlices.values()) {\n              cnt+=slice.getReplicasMap().size();\n            }\n            // TODO: if no nodes are left after this unload\n            // remove from zk - do we have a race where Overseer\n            // see's registered nodes and publishes though?\n            if (cnt > 0) {\n              newStates.put(collectionName, newSlices);\n            } else {\n              // TODO: it might be better logically to have this in ZkController\n              // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n              // ZkController out of the Overseer.\n              try {\n                zkClient.clean(\"/collections/\" + collectionName);\n              } catch (InterruptedException e) {\n                SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collectionName, e);\n                Thread.currentThread().interrupt();\n              } catch (KeeperException e) {\n                SolrException.log(log, \"Problem cleaning up collection in zk:\" + collectionName, e);\n              }\n            }\n          } else {\n            newStates.put(collectionName, clusterState.getSlices(collectionName));\n          }\n        }\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","sourceOld":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.getStr(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.getStr(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: clusterState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = clusterState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getReplicasMap().containsKey(coreNodeName)) {\n                LinkedHashMap<String, Replica> newShards = new LinkedHashMap<String, Replica>();\n                newShards.putAll(slice.getReplicasMap());\n                newShards.remove(coreNodeName);\n                \n                Slice newSlice = new Slice(slice.getName(), newShards);\n                newSlices.put(slice.getName(), newSlice);\n\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            int cnt = 0;\n            for (Slice slice : newSlices.values()) {\n              cnt+=slice.getReplicasMap().size();\n            }\n            // TODO: if no nodes are left after this unload\n            // remove from zk - do we have a race where Overseer\n            // see's registered nodes and publishes though?\n            if (cnt > 0) {\n              newStates.put(collectionName, newSlices);\n            } else {\n              // TODO: it might be better logically to have this in ZkController\n              // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n              // ZkController out of the Overseer.\n              try {\n                zkClient.clean(\"/collections/\" + collectionName);\n              } catch (InterruptedException e) {\n                SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collectionName, e);\n                Thread.currentThread().interrupt();\n              } catch (KeeperException e) {\n                SolrException.log(log, \"Problem cleaning up collection in zk:\" + collectionName, e);\n              }\n            }\n          } else {\n            newStates.put(collectionName, clusterState.getSlices(collectionName));\n          }\n        }\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","bugFix":null,"bugIntro":["8c7cbf6b69f2a4acc536536fe1a152a8ad572d05"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8c7cbf6b69f2a4acc536536fe1a152a8ad572d05","date":1353362776,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.getStr(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.getStr(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: clusterState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = clusterState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getReplicasMap().containsKey(coreNodeName)) {\n                Map<String, Replica> newReplicas = slice.getReplicasCopy();\n                newReplicas.remove(coreNodeName);\n                if (newReplicas.size() != 0) {\n                  Slice newSlice = new Slice(slice.getName(), newReplicas,\n                      slice.getProperties());\n                  newSlices.put(slice.getName(), newSlice);\n                }\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            int cnt = 0;\n            for (Slice slice : newSlices.values()) {\n              cnt+=slice.getReplicasMap().size();\n            }\n            // TODO: if no nodes are left after this unload\n            // remove from zk - do we have a race where Overseer\n            // see's registered nodes and publishes though?\n            if (cnt > 0) {\n              newStates.put(collectionName, newSlices);\n            } else {\n              // TODO: it might be better logically to have this in ZkController\n              // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n              // ZkController out of the Overseer.\n              try {\n                zkClient.clean(\"/collections/\" + collectionName);\n              } catch (InterruptedException e) {\n                SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collectionName, e);\n                Thread.currentThread().interrupt();\n              } catch (KeeperException e) {\n                SolrException.log(log, \"Problem cleaning up collection in zk:\" + collectionName, e);\n              }\n            }\n          } else {\n            newStates.put(collectionName, clusterState.getSlices(collectionName));\n          }\n        }\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","sourceOld":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.getStr(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.getStr(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: clusterState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = clusterState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getReplicasMap().containsKey(coreNodeName)) {\n                Map<String, Replica> newReplicas = slice.getReplicasCopy();\n                newReplicas.remove(coreNodeName);\n                Slice newSlice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n                newSlices.put(slice.getName(), newSlice);\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            int cnt = 0;\n            for (Slice slice : newSlices.values()) {\n              cnt+=slice.getReplicasMap().size();\n            }\n            // TODO: if no nodes are left after this unload\n            // remove from zk - do we have a race where Overseer\n            // see's registered nodes and publishes though?\n            if (cnt > 0) {\n              newStates.put(collectionName, newSlices);\n            } else {\n              // TODO: it might be better logically to have this in ZkController\n              // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n              // ZkController out of the Overseer.\n              try {\n                zkClient.clean(\"/collections/\" + collectionName);\n              } catch (InterruptedException e) {\n                SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collectionName, e);\n                Thread.currentThread().interrupt();\n              } catch (KeeperException e) {\n                SolrException.log(log, \"Problem cleaning up collection in zk:\" + collectionName, e);\n              }\n            }\n          } else {\n            newStates.put(collectionName, clusterState.getSlices(collectionName));\n          }\n        }\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","bugFix":["b456de5d7facbce08ccaf1f4fac0b71a5bf39add","c51a2dcb8b4e1820a44f35f11961110201e06cdb"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c5a558d54519c651068ddb202f03befefb1514a7","date":1354382006,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.getStr(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.getStr(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n        DocCollection coll = newCollections.get(collection);\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(coreNodeName);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(coreNodeName);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n        return newState;\n     }\n\n","sourceOld":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.getStr(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.getStr(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: clusterState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = clusterState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getReplicasMap().containsKey(coreNodeName)) {\n                Map<String, Replica> newReplicas = slice.getReplicasCopy();\n                newReplicas.remove(coreNodeName);\n                if (newReplicas.size() != 0) {\n                  Slice newSlice = new Slice(slice.getName(), newReplicas,\n                      slice.getProperties());\n                  newSlices.put(slice.getName(), newSlice);\n                }\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            int cnt = 0;\n            for (Slice slice : newSlices.values()) {\n              cnt+=slice.getReplicasMap().size();\n            }\n            // TODO: if no nodes are left after this unload\n            // remove from zk - do we have a race where Overseer\n            // see's registered nodes and publishes though?\n            if (cnt > 0) {\n              newStates.put(collectionName, newSlices);\n            } else {\n              // TODO: it might be better logically to have this in ZkController\n              // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n              // ZkController out of the Overseer.\n              try {\n                zkClient.clean(\"/collections/\" + collectionName);\n              } catch (InterruptedException e) {\n                SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collectionName, e);\n                Thread.currentThread().interrupt();\n              } catch (KeeperException e) {\n                SolrException.log(log, \"Problem cleaning up collection in zk:\" + collectionName, e);\n              }\n            }\n          } else {\n            newStates.put(collectionName, clusterState.getSlices(collectionName));\n          }\n        }\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"407687e67faf6e1f02a211ca078d8e3eed631027","date":1355157407,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.getStr(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.getStr(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n        DocCollection coll = newCollections.get(collection);\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(coreNodeName);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(coreNodeName);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n        return newState;\n     }\n\n","sourceOld":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.getStr(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.getStr(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: clusterState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = clusterState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getReplicasMap().containsKey(coreNodeName)) {\n                Map<String, Replica> newReplicas = slice.getReplicasCopy();\n                newReplicas.remove(coreNodeName);\n                Slice newSlice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n                newSlices.put(slice.getName(), newSlice);\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            int cnt = 0;\n            for (Slice slice : newSlices.values()) {\n              cnt+=slice.getReplicasMap().size();\n            }\n            // TODO: if no nodes are left after this unload\n            // remove from zk - do we have a race where Overseer\n            // see's registered nodes and publishes though?\n            if (cnt > 0) {\n              newStates.put(collectionName, newSlices);\n            } else {\n              // TODO: it might be better logically to have this in ZkController\n              // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n              // ZkController out of the Overseer.\n              try {\n                zkClient.clean(\"/collections/\" + collectionName);\n              } catch (InterruptedException e) {\n                SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collectionName, e);\n                Thread.currentThread().interrupt();\n              } catch (KeeperException e) {\n                SolrException.log(log, \"Problem cleaning up collection in zk:\" + collectionName, e);\n              }\n            }\n          } else {\n            newStates.put(collectionName, clusterState.getSlices(collectionName));\n          }\n        }\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cb4a195b8dc1808cd01748bd2e0fba26ca915d4d","date":1361851792,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n        if (cnn == null) {\n          // it must be the default then\n          cnn = message.getStr(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.getStr(ZkStateReader.CORE_NAME_PROP);\n        }\n\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n        DocCollection coll = newCollections.get(collection);\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n        return newState;\n     }\n\n","sourceOld":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.getStr(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.getStr(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n        DocCollection coll = newCollections.get(collection);\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(coreNodeName);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(coreNodeName);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6e756785b6f25f3b8f7ee57c7e210c6b67fbfbbf","date":1363562282,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n        if (cnn == null) {\n          // it must be the default then\n          cnn = message.getStr(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.getStr(ZkStateReader.CORE_NAME_PROP);\n        }\n\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n        DocCollection coll = newCollections.get(collection);\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n        return newState;\n     }\n\n","sourceOld":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n        if (cnn == null) {\n          // it must be the default then\n          cnn = message.getStr(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.getStr(ZkStateReader.CORE_NAME_PROP);\n        }\n\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n        DocCollection coll = newCollections.get(collection);\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"36f2b01395ef2bc334ebf2f94f2fe44e0f2921b1","date":1369336666,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n        if (cnn == null) {\n          // it must be the default then\n          cnn = message.getStr(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.getStr(ZkStateReader.CORE_NAME_PROP);\n        }\n\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n        DocCollection coll = newCollections.get(collection);\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n        return newState;\n     }\n\n","sourceOld":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n        if (cnn == null) {\n          // it must be the default then\n          cnn = message.getStr(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.getStr(ZkStateReader.CORE_NAME_PROP);\n        }\n\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n        DocCollection coll = newCollections.get(collection);\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"849494cf2f3a96af5c8c84995108ddd8456fcd04","date":1372277913,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n        DocCollection coll = newCollections.get(collection);\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n        return newState;\n     }\n\n","sourceOld":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n        if (cnn == null) {\n          // it must be the default then\n          cnn = message.getStr(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.getStr(ZkStateReader.CORE_NAME_PROP);\n        }\n\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n        DocCollection coll = newCollections.get(collection);\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4ee5a5186e7187cd42c6f7ff64b6e7206a780325","date":1372914261,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n        DocCollection coll = newCollections.get(collection);\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n        return newState;\n     }\n\n","sourceOld":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n        DocCollection coll = newCollections.get(collection);\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n        DocCollection coll = newCollections.get(collection);\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n        return newState;\n     }\n\n","sourceOld":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n        if (cnn == null) {\n          // it must be the default then\n          cnn = message.getStr(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.getStr(ZkStateReader.CORE_NAME_PROP);\n        }\n\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n        DocCollection coll = newCollections.get(collection);\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ce7cfca1a733d2ed1f7089b339faf006bdcc7b70","date":1386334715,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n//        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n//        DocCollection coll = newCollections.get(collection);\n        DocCollection coll = clusterState.getCollectionOrNull(collection) ;\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n//          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return newState(clusterState,singletonMap(collection, (DocCollection) null));\n\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n           return newState(clusterState,singletonMap(collection,newCollection));\n//          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n//        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n//        return newState;\n     }\n\n","sourceOld":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n        DocCollection coll = newCollections.get(collection);\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"74f45af4339b0daf7a95c820ab88c1aea74fbce0","date":1387475327,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n//        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n//        DocCollection coll = newCollections.get(collection);\n        DocCollection coll = clusterState.getCollectionOrNull(collection) ;\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n//          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return newState(clusterState,singletonMap(collection, (DocCollection) null));\n\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n           return newState(clusterState,singletonMap(collection,newCollection));\n//          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n//        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n//        return newState;\n     }\n\n","sourceOld":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n        DocCollection coll = newCollections.get(collection);\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29f5eaf296600e1665151e7929d42a3cbe22e481","date":1393983215,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n        checkCollection(message, collection);\n\n//        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n//        DocCollection coll = newCollections.get(collection);\n        DocCollection coll = clusterState.getCollectionOrNull(collection) ;\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n//          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return newState(clusterState,singletonMap(collection, (DocCollection) null));\n\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n           return newState(clusterState,singletonMap(collection,newCollection));\n//          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n//        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n//        return newState;\n     }\n\n","sourceOld":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n//        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n//        DocCollection coll = newCollections.get(collection);\n        DocCollection coll = clusterState.getCollectionOrNull(collection) ;\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n//          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return newState(clusterState,singletonMap(collection, (DocCollection) null));\n\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n           return newState(clusterState,singletonMap(collection,newCollection));\n//          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n//        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n//        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6df412542f3e2161f4bc2b13357b4a973195bfb7","date":1394040511,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        final String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n        if (!checkCollectionKeyExistence(message)) return clusterState;\n\n//        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n//        DocCollection coll = newCollections.get(collection);\n        DocCollection coll = clusterState.getCollectionOrNull(collection) ;\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n//          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return newState(clusterState,singletonMap(collection, (DocCollection) null));\n\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n           return newState(clusterState,singletonMap(collection,newCollection));\n//          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n//        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n//        return newState;\n     }\n\n","sourceOld":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n        checkCollection(message, collection);\n\n//        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n//        DocCollection coll = newCollections.get(collection);\n        DocCollection coll = clusterState.getCollectionOrNull(collection) ;\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n//          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return newState(clusterState,singletonMap(collection, (DocCollection) null));\n\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n           return newState(clusterState,singletonMap(collection,newCollection));\n//          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n//        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n//        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"96ea64d994d340044e0d57aeb6a5871539d10ca5","date":1394225445,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        final String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n        if (!checkCollectionKeyExistence(message)) return clusterState;\n\n//        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n//        DocCollection coll = newCollections.get(collection);\n        DocCollection coll = clusterState.getCollectionOrNull(collection) ;\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n//          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return newState(clusterState,singletonMap(collection, (DocCollection) null));\n\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n           return newState(clusterState,singletonMap(collection,newCollection));\n//          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n//        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n//        return newState;\n     }\n\n","sourceOld":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n//        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n//        DocCollection coll = newCollections.get(collection);\n        DocCollection coll = clusterState.getCollectionOrNull(collection) ;\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n//          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return newState(clusterState,singletonMap(collection, (DocCollection) null));\n\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n           return newState(clusterState,singletonMap(collection,newCollection));\n//          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n//        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n//        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        final String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n        if (!checkCollectionKeyExistence(message)) return clusterState;\n\n//        final Map<String, DocCollection> newCollections = new LinkedHashMap<>(clusterState.getCollectionStates()); // shallow copy\n//        DocCollection coll = newCollections.get(collection);\n        DocCollection coll = clusterState.getCollectionOrNull(collection) ;\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n//          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return newState(clusterState,singletonMap(collection, (DocCollection) null));\n\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n           return newState(clusterState,singletonMap(collection,newCollection));\n//          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n//        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n//        return newState;\n     }\n\n","sourceOld":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        final String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n        if (!checkCollectionKeyExistence(message)) return clusterState;\n\n//        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy\n//        DocCollection coll = newCollections.get(collection);\n        DocCollection coll = clusterState.getCollectionOrNull(collection) ;\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n//          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return newState(clusterState,singletonMap(collection, (DocCollection) null));\n\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n           return newState(clusterState,singletonMap(collection,newCollection));\n//          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n//        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n//        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0622fbd990643ae4cacb693db6a0c82cf8916ae2","date":1397637446,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        final String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n        if (!checkCollectionKeyExistence(message)) return clusterState;\n\n//        final Map<String, DocCollection> newCollections = new LinkedHashMap<>(clusterState.getCollectionStates()); // shallow copy\n//        DocCollection coll = newCollections.get(collection);\n        DocCollection coll = clusterState.getCollectionOrNull(collection) ;\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return newState(clusterState,singletonMap(collection, (DocCollection) null));\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n           return newState(clusterState,singletonMap(collection,newCollection));\n        }\n\n     }\n\n","sourceOld":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        final String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n        if (!checkCollectionKeyExistence(message)) return clusterState;\n\n//        final Map<String, DocCollection> newCollections = new LinkedHashMap<>(clusterState.getCollectionStates()); // shallow copy\n//        DocCollection coll = newCollections.get(collection);\n        DocCollection coll = clusterState.getCollectionOrNull(collection) ;\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n//          newCollections.remove(coll.getName());\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return newState(clusterState,singletonMap(collection, (DocCollection) null));\n\n\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n           return newState(clusterState,singletonMap(collection,newCollection));\n//          newCollections.put(newCollection.getName(), newCollection);\n        }\n\n//        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newCollections);\n//        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a6f693ed86f289b2e42b46684409b3997f2c264a","date":1404319832,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        final String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n        if (!checkCollectionKeyExistence(message)) return clusterState;\n\n        DocCollection coll = clusterState.getCollectionOrNull(collection) ;\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n\n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.delete(\"/collections/\" + collection,-1,true);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return newState(clusterState,singletonMap(collection, (DocCollection) null));\n\n        } else {\n          DocCollection newCollection = coll.copyWith(newSlices);\n          return newState(clusterState,singletonMap(collection,newCollection));\n        }\n\n     }\n\n","sourceOld":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        final String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n        if (!checkCollectionKeyExistence(message)) return clusterState;\n\n//        final Map<String, DocCollection> newCollections = new LinkedHashMap<>(clusterState.getCollectionStates()); // shallow copy\n//        DocCollection coll = newCollections.get(collection);\n        DocCollection coll = clusterState.getCollectionOrNull(collection) ;\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return newState(clusterState,singletonMap(collection, (DocCollection) null));\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n           return newState(clusterState,singletonMap(collection,newCollection));\n        }\n\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f4c07fa58a256dccf8b95364855fd5e9ad4d1401","date":1404386015,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        final String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n        if (!checkCollectionKeyExistence(message)) return clusterState;\n\n//        final Map<String, DocCollection> newCollections = new LinkedHashMap<>(clusterState.getCollectionStates()); // shallow copy\n//        DocCollection coll = newCollections.get(collection);\n        DocCollection coll = clusterState.getCollectionOrNull(collection) ;\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return newState(clusterState,singletonMap(collection, (DocCollection) null));\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n           return newState(clusterState,singletonMap(collection,newCollection));\n        }\n\n     }\n\n","sourceOld":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        final String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n        if (!checkCollectionKeyExistence(message)) return clusterState;\n\n        DocCollection coll = clusterState.getCollectionOrNull(collection) ;\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n\n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.delete(\"/collections/\" + collection,-1,true);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return newState(clusterState,singletonMap(collection, (DocCollection) null));\n\n        } else {\n          DocCollection newCollection = coll.copyWith(newSlices);\n          return newState(clusterState,singletonMap(collection,newCollection));\n        }\n\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9279b175e5e66258442d2123a50f052219a9cc1b","date":1410531077,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        final String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n        if (!checkCollectionKeyExistence(message)) return clusterState;\n\n        DocCollection coll = clusterState.getCollectionOrNull(collection) ;\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n\n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.delete(\"/collections/\" + collection, -1, true);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return newState(clusterState,singletonMap(collection, (DocCollection) null));\n\n        } else {\n          DocCollection newCollection = coll.copyWithSlices(newSlices);\n          return newState(clusterState,singletonMap(collection,newCollection));\n        }\n\n     }\n\n","sourceOld":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        final String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n        if (!checkCollectionKeyExistence(message)) return clusterState;\n\n//        final Map<String, DocCollection> newCollections = new LinkedHashMap<>(clusterState.getCollectionStates()); // shallow copy\n//        DocCollection coll = newCollections.get(collection);\n        DocCollection coll = clusterState.getCollectionOrNull(collection) ;\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n        \n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return newState(clusterState,singletonMap(collection, (DocCollection) null));\n\n        } else {\n          DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());\n           return newState(clusterState,singletonMap(collection,newCollection));\n        }\n\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"50dad58cdcb7f989ab06a004f1dfdb47d986d9b2","date":1411108217,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        final String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n        if (!checkCollectionKeyExistence(message)) return clusterState;\n\n        DocCollection coll = clusterState.getCollectionOrNull(collection) ;\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n\n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return newState(clusterState,singletonMap(collection, (DocCollection) null));\n\n        } else {\n          DocCollection newCollection = coll.copyWithSlices(newSlices);\n          return newState(clusterState,singletonMap(collection,newCollection));\n        }\n\n     }\n\n","sourceOld":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        final String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n        if (!checkCollectionKeyExistence(message)) return clusterState;\n\n        DocCollection coll = clusterState.getCollectionOrNull(collection) ;\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n\n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.delete(\"/collections/\" + collection, -1, true);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return newState(clusterState,singletonMap(collection, (DocCollection) null));\n\n        } else {\n          DocCollection newCollection = coll.copyWithSlices(newSlices);\n          return newState(clusterState,singletonMap(collection,newCollection));\n        }\n\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"24a5da2a0d397ff29f3de8f6cf451d3412c2509a","date":1417276391,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":"    /*\n       * Remove core from cloudstate\n       */\n      private ZkWriteCommand removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        return new SliceMutator(getZkStateReader()).removeReplica(clusterState, message);\n     }\n\n","sourceOld":"    /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        final String cnn = message.getStr(ZkStateReader.CORE_NODE_NAME_PROP);\n        final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n        if (!checkCollectionKeyExistence(message)) return clusterState;\n\n        DocCollection coll = clusterState.getCollectionOrNull(collection) ;\n        if (coll == null) {\n          // TODO: log/error that we didn't find it?\n          // just in case, remove the zk collection node\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return clusterState;\n        }\n\n        Map<String, Slice> newSlices = new LinkedHashMap<>();\n        boolean lastSlice = false;\n        for (Slice slice : coll.getSlices()) {\n          Replica replica = slice.getReplica(cnn);\n          if (replica != null) {\n            Map<String, Replica> newReplicas = slice.getReplicasCopy();\n            newReplicas.remove(cnn);\n            // TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n            // if (newReplicas.size() == 0 && slice.getRange() == null) {\n            // if there are no replicas left for the slice remove it\n            if (newReplicas.size() == 0) {\n              slice = null;\n              lastSlice = true;\n            } else {\n              slice = new Slice(slice.getName(), newReplicas, slice.getProperties());\n            }\n          }\n\n          if (slice != null) {\n            newSlices.put(slice.getName(), slice);\n          }\n        }\n\n        if (lastSlice) {\n          // remove all empty pre allocated slices\n          for (Slice slice : coll.getSlices()) {\n            if (slice.getReplicas().size() == 0) {\n              newSlices.remove(slice.getName());\n            }\n          }\n        }\n\n        // if there are no slices left in the collection, remove it?\n        if (newSlices.size() == 0) {\n\n          // TODO: it might be better logically to have this in ZkController\n          // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n          // ZkController out of the Overseer.\n          try {\n            zkClient.clean(\"/collections/\" + collection);\n          } catch (InterruptedException e) {\n            SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collection, e);\n            Thread.currentThread().interrupt();\n          } catch (KeeperException e) {\n            SolrException.log(log, \"Problem cleaning up collection in zk:\" + collection, e);\n          }\n          return newState(clusterState,singletonMap(collection, (DocCollection) null));\n\n        } else {\n          DocCollection newCollection = coll.copyWithSlices(newSlices);\n          return newState(clusterState,singletonMap(collection,newCollection));\n        }\n\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"03f6fef27e0d27edc875c720e6ce5db17480a562","date":1417277285,"type":4,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","sourceNew":null,"sourceOld":"    /*\n       * Remove core from cloudstate\n       */\n      private ZkWriteCommand removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        return new SliceMutator(getZkStateReader()).removeReplica(clusterState, message);\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"96ea64d994d340044e0d57aeb6a5871539d10ca5":["ce7cfca1a733d2ed1f7089b339faf006bdcc7b70","6df412542f3e2161f4bc2b13357b4a973195bfb7"],"ce7cfca1a733d2ed1f7089b339faf006bdcc7b70":["4ee5a5186e7187cd42c6f7ff64b6e7206a780325"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["36f2b01395ef2bc334ebf2f94f2fe44e0f2921b1","4ee5a5186e7187cd42c6f7ff64b6e7206a780325"],"36f2b01395ef2bc334ebf2f94f2fe44e0f2921b1":["6e756785b6f25f3b8f7ee57c7e210c6b67fbfbbf"],"29f5eaf296600e1665151e7929d42a3cbe22e481":["ce7cfca1a733d2ed1f7089b339faf006bdcc7b70"],"24a5da2a0d397ff29f3de8f6cf451d3412c2509a":["50dad58cdcb7f989ab06a004f1dfdb47d986d9b2"],"6e756785b6f25f3b8f7ee57c7e210c6b67fbfbbf":["cb4a195b8dc1808cd01748bd2e0fba26ca915d4d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b456de5d7facbce08ccaf1f4fac0b71a5bf39add":["fa64435b5902ce266c23755a4a00691a3285dab8"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["6df412542f3e2161f4bc2b13357b4a973195bfb7"],"fa64435b5902ce266c23755a4a00691a3285dab8":["3f767f8c99eaedb984df754fe61f21c5de260f94"],"f4c07fa58a256dccf8b95364855fd5e9ad4d1401":["a6f693ed86f289b2e42b46684409b3997f2c264a"],"407687e67faf6e1f02a211ca078d8e3eed631027":["b456de5d7facbce08ccaf1f4fac0b71a5bf39add","c5a558d54519c651068ddb202f03befefb1514a7"],"8fd5be977c105554c6a7b68afcdbc511439723ab":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3f767f8c99eaedb984df754fe61f21c5de260f94"],"50dad58cdcb7f989ab06a004f1dfdb47d986d9b2":["9279b175e5e66258442d2123a50f052219a9cc1b"],"9279b175e5e66258442d2123a50f052219a9cc1b":["f4c07fa58a256dccf8b95364855fd5e9ad4d1401"],"849494cf2f3a96af5c8c84995108ddd8456fcd04":["36f2b01395ef2bc334ebf2f94f2fe44e0f2921b1"],"3f767f8c99eaedb984df754fe61f21c5de260f94":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a6f693ed86f289b2e42b46684409b3997f2c264a":["0622fbd990643ae4cacb693db6a0c82cf8916ae2"],"03f6fef27e0d27edc875c720e6ce5db17480a562":["24a5da2a0d397ff29f3de8f6cf451d3412c2509a"],"c5a558d54519c651068ddb202f03befefb1514a7":["8c7cbf6b69f2a4acc536536fe1a152a8ad572d05"],"8c7cbf6b69f2a4acc536536fe1a152a8ad572d05":["b456de5d7facbce08ccaf1f4fac0b71a5bf39add"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3f767f8c99eaedb984df754fe61f21c5de260f94"],"6df412542f3e2161f4bc2b13357b4a973195bfb7":["29f5eaf296600e1665151e7929d42a3cbe22e481"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":["4ee5a5186e7187cd42c6f7ff64b6e7206a780325","ce7cfca1a733d2ed1f7089b339faf006bdcc7b70"],"cb4a195b8dc1808cd01748bd2e0fba26ca915d4d":["c5a558d54519c651068ddb202f03befefb1514a7"],"0622fbd990643ae4cacb693db6a0c82cf8916ae2":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"4ee5a5186e7187cd42c6f7ff64b6e7206a780325":["849494cf2f3a96af5c8c84995108ddd8456fcd04"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["03f6fef27e0d27edc875c720e6ce5db17480a562"]},"commit2Childs":{"96ea64d994d340044e0d57aeb6a5871539d10ca5":[],"ce7cfca1a733d2ed1f7089b339faf006bdcc7b70":["96ea64d994d340044e0d57aeb6a5871539d10ca5","29f5eaf296600e1665151e7929d42a3cbe22e481","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"36f2b01395ef2bc334ebf2f94f2fe44e0f2921b1":["37a0f60745e53927c4c876cfe5b5a58170f0646c","849494cf2f3a96af5c8c84995108ddd8456fcd04"],"29f5eaf296600e1665151e7929d42a3cbe22e481":["6df412542f3e2161f4bc2b13357b4a973195bfb7"],"24a5da2a0d397ff29f3de8f6cf451d3412c2509a":["03f6fef27e0d27edc875c720e6ce5db17480a562"],"6e756785b6f25f3b8f7ee57c7e210c6b67fbfbbf":["36f2b01395ef2bc334ebf2f94f2fe44e0f2921b1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["8fd5be977c105554c6a7b68afcdbc511439723ab","3f767f8c99eaedb984df754fe61f21c5de260f94","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"b456de5d7facbce08ccaf1f4fac0b71a5bf39add":["407687e67faf6e1f02a211ca078d8e3eed631027","8c7cbf6b69f2a4acc536536fe1a152a8ad572d05"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["0622fbd990643ae4cacb693db6a0c82cf8916ae2"],"fa64435b5902ce266c23755a4a00691a3285dab8":["b456de5d7facbce08ccaf1f4fac0b71a5bf39add"],"f4c07fa58a256dccf8b95364855fd5e9ad4d1401":["9279b175e5e66258442d2123a50f052219a9cc1b"],"407687e67faf6e1f02a211ca078d8e3eed631027":[],"8fd5be977c105554c6a7b68afcdbc511439723ab":[],"50dad58cdcb7f989ab06a004f1dfdb47d986d9b2":["24a5da2a0d397ff29f3de8f6cf451d3412c2509a"],"9279b175e5e66258442d2123a50f052219a9cc1b":["50dad58cdcb7f989ab06a004f1dfdb47d986d9b2"],"3f767f8c99eaedb984df754fe61f21c5de260f94":["fa64435b5902ce266c23755a4a00691a3285dab8","8fd5be977c105554c6a7b68afcdbc511439723ab","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"849494cf2f3a96af5c8c84995108ddd8456fcd04":["4ee5a5186e7187cd42c6f7ff64b6e7206a780325"],"a6f693ed86f289b2e42b46684409b3997f2c264a":["f4c07fa58a256dccf8b95364855fd5e9ad4d1401"],"03f6fef27e0d27edc875c720e6ce5db17480a562":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c5a558d54519c651068ddb202f03befefb1514a7":["407687e67faf6e1f02a211ca078d8e3eed631027","cb4a195b8dc1808cd01748bd2e0fba26ca915d4d"],"8c7cbf6b69f2a4acc536536fe1a152a8ad572d05":["c5a558d54519c651068ddb202f03befefb1514a7"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":[],"6df412542f3e2161f4bc2b13357b4a973195bfb7":["96ea64d994d340044e0d57aeb6a5871539d10ca5","634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":[],"cb4a195b8dc1808cd01748bd2e0fba26ca915d4d":["6e756785b6f25f3b8f7ee57c7e210c6b67fbfbbf"],"4ee5a5186e7187cd42c6f7ff64b6e7206a780325":["ce7cfca1a733d2ed1f7089b339faf006bdcc7b70","37a0f60745e53927c4c876cfe5b5a58170f0646c","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"0622fbd990643ae4cacb693db6a0c82cf8916ae2":["a6f693ed86f289b2e42b46684409b3997f2c264a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["96ea64d994d340044e0d57aeb6a5871539d10ca5","37a0f60745e53927c4c876cfe5b5a58170f0646c","407687e67faf6e1f02a211ca078d8e3eed631027","8fd5be977c105554c6a7b68afcdbc511439723ab","d6f074e73200c07d54f242d3880a8da5a35ff97b","74f45af4339b0daf7a95c820ab88c1aea74fbce0","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}