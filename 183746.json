{"path":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","pathOld":"src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","sourceNew":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream. After successful merge, the source files\n     *  are deleted.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\n                \"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\n                \"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = null;\n        try {\n            os = directory.createOutput(fileName);\n\n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(fe.file);\n                totalSize += directory.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            byte buffer[] = new byte[16384];\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os, buffer);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n\n        } finally {\n            if (os != null) try { os.close(); } catch (IOException e) { }\n        }\n    }\n\n","sourceOld":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream. After successful merge, the source files\n     *  are deleted.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\n                \"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\n                \"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = null;\n        try {\n            os = directory.createOutput(fileName);\n\n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(fe.file);\n                totalSize += directory.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            byte buffer[] = new byte[16384];\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os, buffer);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n\n        } finally {\n            if (os != null) try { os.close(); } catch (IOException e) { }\n        }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fb10b6bcde550b87d8f10e5f010bd8f3021023b6","date":1274974592,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","sourceNew":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream. After successful merge, the source files\n     *  are deleted.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\n                \"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\n                \"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = null;\n        try {\n            os = directory.createOutput(fileName);\n\n            // Write the Version info - must be a VInt because CFR reads a VInt\n            // in older versions!\n            os.writeVInt(FORMAT_CURRENT);\n            \n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(IndexFileNames.stripSegmentName(fe.file));\n                totalSize += directory.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            byte buffer[] = new byte[16384];\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os, buffer);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n\n        } finally {\n            if (os != null) try { os.close(); } catch (IOException e) { }\n        }\n    }\n\n","sourceOld":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream. After successful merge, the source files\n     *  are deleted.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\n                \"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\n                \"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = null;\n        try {\n            os = directory.createOutput(fileName);\n\n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(fe.file);\n                totalSize += directory.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            byte buffer[] = new byte[16384];\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os, buffer);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n\n        } finally {\n            if (os != null) try { os.close(); } catch (IOException e) { }\n        }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"462dfb1d8690f192817503773f5b8b94a702246a","date":1280128992,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","sourceNew":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream. After successful merge, the source files\n     *  are deleted.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = directory.createOutput(fileName);\n        IOException priorException = null;\n        try {\n            // Write the Version info - must be a VInt because CFR reads a VInt\n            // in older versions!\n            os.writeVInt(FORMAT_CURRENT);\n            \n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(IndexFileNames.stripSegmentName(fe.file));\n                totalSize += directory.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n        } catch (IOException e) {\n          priorException = e;\n        } finally {\n          IOUtils.closeSafely(priorException, os);\n        }\n    }\n\n","sourceOld":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream. After successful merge, the source files\n     *  are deleted.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\n                \"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\n                \"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = null;\n        try {\n            os = directory.createOutput(fileName);\n\n            // Write the Version info - must be a VInt because CFR reads a VInt\n            // in older versions!\n            os.writeVInt(FORMAT_CURRENT);\n            \n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(IndexFileNames.stripSegmentName(fe.file));\n                totalSize += directory.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            byte buffer[] = new byte[16384];\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os, buffer);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n\n        } finally {\n            if (os != null) try { os.close(); } catch (IOException e) { }\n        }\n    }\n\n","bugFix":["dda77265180d41bf85c84c995e25eda7b8e1b74d","8fb95844e4ba5160067c64c5eb1cd8a09f7a94f1","f6dba7919de4ff4ed6ff17f90619203772722f08","a67f534c1db9eb255bc5a5137c7bf362ff90c276"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3242a09f703274d3b9283f2064a1a33064b53a1b","date":1280263474,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","sourceNew":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream. After successful merge, the source files\n     *  are deleted.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = directory.createOutput(fileName);\n        IOException priorException = null;\n        try {\n            // Write the Version info - must be a VInt because CFR reads a VInt\n            // in older versions!\n            os.writeVInt(FORMAT_CURRENT);\n            \n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(IndexFileNames.stripSegmentName(fe.file));\n                totalSize += directory.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n        } catch (IOException e) {\n          priorException = e;\n        } finally {\n          IOUtils.closeSafely(priorException, os);\n        }\n    }\n\n","sourceOld":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream. After successful merge, the source files\n     *  are deleted.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\n                \"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\n                \"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = null;\n        try {\n            os = directory.createOutput(fileName);\n\n            // Write the Version info - must be a VInt because CFR reads a VInt\n            // in older versions!\n            os.writeVInt(FORMAT_CURRENT);\n            \n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(IndexFileNames.stripSegmentName(fe.file));\n                totalSize += directory.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            byte buffer[] = new byte[16384];\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os, buffer);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n\n        } finally {\n            if (os != null) try { os.close(); } catch (IOException e) { }\n        }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"766e2265a545239f23f615e12f17c86442ac136b","date":1304590001,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","sourceNew":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = directory.createOutput(fileName);\n        IOException priorException = null;\n        try {\n            // Write the Version info - must be a VInt because CFR reads a VInt\n            // in older versions!\n            os.writeVInt(FORMAT_CURRENT);\n            \n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(IndexFileNames.stripSegmentName(fe.file));\n                totalSize += directory.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n        } catch (IOException e) {\n          priorException = e;\n        } finally {\n          IOUtils.closeSafely(priorException, os);\n        }\n    }\n\n","sourceOld":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream. After successful merge, the source files\n     *  are deleted.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = directory.createOutput(fileName);\n        IOException priorException = null;\n        try {\n            // Write the Version info - must be a VInt because CFR reads a VInt\n            // in older versions!\n            os.writeVInt(FORMAT_CURRENT);\n            \n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(IndexFileNames.stripSegmentName(fe.file));\n                totalSize += directory.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n        } catch (IOException e) {\n          priorException = e;\n        } finally {\n          IOUtils.closeSafely(priorException, os);\n        }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c700f8d0842d3e52bb2bdfbfdc046a137e836edb","date":1305285499,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","sourceNew":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = directory.createOutput(fileName);\n        IOException priorException = null;\n        try {\n            // Write the Version info - must be a VInt because CFR reads a VInt\n            // in older versions!\n            os.writeVInt(FORMAT_CURRENT);\n            \n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(IndexFileNames.stripSegmentName(fe.file));\n                totalSize += directory.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n        } catch (IOException e) {\n          priorException = e;\n        } finally {\n          IOUtils.closeSafely(priorException, os);\n        }\n    }\n\n","sourceOld":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream. After successful merge, the source files\n     *  are deleted.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = directory.createOutput(fileName);\n        IOException priorException = null;\n        try {\n            // Write the Version info - must be a VInt because CFR reads a VInt\n            // in older versions!\n            os.writeVInt(FORMAT_CURRENT);\n            \n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(IndexFileNames.stripSegmentName(fe.file));\n                totalSize += directory.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n        } catch (IOException e) {\n          priorException = e;\n        } finally {\n          IOUtils.closeSafely(priorException, os);\n        }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","sourceNew":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = directory.createOutput(fileName);\n        IOException priorException = null;\n        try {\n            // Write the Version info - must be a VInt because CFR reads a VInt\n            // in older versions!\n            os.writeVInt(FORMAT_CURRENT);\n            \n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(IndexFileNames.stripSegmentName(fe.file));\n                totalSize += directory.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n        } catch (IOException e) {\n          priorException = e;\n        } finally {\n          IOUtils.closeSafely(priorException, os);\n        }\n    }\n\n","sourceOld":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream. After successful merge, the source files\n     *  are deleted.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = directory.createOutput(fileName);\n        IOException priorException = null;\n        try {\n            // Write the Version info - must be a VInt because CFR reads a VInt\n            // in older versions!\n            os.writeVInt(FORMAT_CURRENT);\n            \n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(IndexFileNames.stripSegmentName(fe.file));\n                totalSize += directory.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n        } catch (IOException e) {\n          priorException = e;\n        } finally {\n          IOUtils.closeSafely(priorException, os);\n        }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"32dd6d3e87d9e4f05e3e9de40bebf1ff1482771f","date":1306408552,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","sourceNew":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = directory.createOutput(fileName);\n        IOException priorException = null;\n        try {\n            // Write the Version info - must be a VInt because CFR reads a VInt\n            // in older versions!\n            os.writeVInt(FORMAT_CURRENT);\n            \n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(IndexFileNames.stripSegmentName(fe.file));\n                totalSize += fe.dir.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n        } catch (IOException e) {\n          priorException = e;\n        } finally {\n          IOUtils.closeSafely(priorException, os);\n        }\n    }\n\n","sourceOld":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = directory.createOutput(fileName);\n        IOException priorException = null;\n        try {\n            // Write the Version info - must be a VInt because CFR reads a VInt\n            // in older versions!\n            os.writeVInt(FORMAT_CURRENT);\n            \n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(IndexFileNames.stripSegmentName(fe.file));\n                totalSize += directory.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n        } catch (IOException e) {\n          priorException = e;\n        } finally {\n          IOUtils.closeSafely(priorException, os);\n        }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","date":1306767085,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","sourceNew":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = directory.createOutput(fileName);\n        IOException priorException = null;\n        try {\n            // Write the Version info - must be a VInt because CFR reads a VInt\n            // in older versions!\n            os.writeVInt(FORMAT_CURRENT);\n            \n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(IndexFileNames.stripSegmentName(fe.file));\n                totalSize += fe.dir.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n        } catch (IOException e) {\n          priorException = e;\n        } finally {\n          IOUtils.closeSafely(priorException, os);\n        }\n    }\n\n","sourceOld":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = directory.createOutput(fileName);\n        IOException priorException = null;\n        try {\n            // Write the Version info - must be a VInt because CFR reads a VInt\n            // in older versions!\n            os.writeVInt(FORMAT_CURRENT);\n            \n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(IndexFileNames.stripSegmentName(fe.file));\n                totalSize += directory.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n        } catch (IOException e) {\n          priorException = e;\n        } finally {\n          IOUtils.closeSafely(priorException, os);\n        }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e10cb22a8bdb44339e282925a29182bb2f3174d","date":1306841137,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","sourceNew":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = directory.createOutput(fileName);\n        IOException priorException = null;\n        try {\n            // Write the Version info - must be a VInt because CFR reads a VInt\n            // in older versions!\n            os.writeVInt(FORMAT_CURRENT);\n            \n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(IndexFileNames.stripSegmentName(fe.file));\n                totalSize += fe.dir.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n        } catch (IOException e) {\n          priorException = e;\n        } finally {\n          IOUtils.closeSafely(priorException, os);\n        }\n    }\n\n","sourceOld":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = directory.createOutput(fileName);\n        IOException priorException = null;\n        try {\n            // Write the Version info - must be a VInt because CFR reads a VInt\n            // in older versions!\n            os.writeVInt(FORMAT_CURRENT);\n            \n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(IndexFileNames.stripSegmentName(fe.file));\n                totalSize += directory.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n        } catch (IOException e) {\n          priorException = e;\n        } finally {\n          IOUtils.closeSafely(priorException, os);\n        }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"639c36565ce03aed5b0fce7c9e4448e53a1f7efd","date":1308580104,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","sourceNew":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = directory.createOutput(fileName, context);\n        IOException priorException = null;\n        try {\n            // Write the Version info - must be a VInt because CFR reads a VInt\n            // in older versions!\n            os.writeVInt(FORMAT_CURRENT);\n            \n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(IndexFileNames.stripSegmentName(fe.file));\n                totalSize += fe.dir.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n        } catch (IOException e) {\n          priorException = e;\n        } finally {\n          IOUtils.closeSafely(priorException, os);\n        }\n    }\n\n","sourceOld":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = directory.createOutput(fileName);\n        IOException priorException = null;\n        try {\n            // Write the Version info - must be a VInt because CFR reads a VInt\n            // in older versions!\n            os.writeVInt(FORMAT_CURRENT);\n            \n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(IndexFileNames.stripSegmentName(fe.file));\n                totalSize += fe.dir.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n        } catch (IOException e) {\n          priorException = e;\n        } finally {\n          IOUtils.closeSafely(priorException, os);\n        }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0aab6e810b4b0d3743d6a048be0602801f4b3920","date":1308671625,"type":4,"author":"Simon Willnauer","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","sourceNew":null,"sourceOld":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = directory.createOutput(fileName);\n        IOException priorException = null;\n        try {\n            // Write the Version info - must be a VInt because CFR reads a VInt\n            // in older versions!\n            os.writeVInt(FORMAT_CURRENT);\n            \n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(IndexFileNames.stripSegmentName(fe.file));\n                totalSize += fe.dir.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n        } catch (IOException e) {\n          priorException = e;\n        } finally {\n          IOUtils.closeSafely(priorException, os);\n        }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2553b00f699380c64959ccb27991289aae87be2e","date":1309290151,"type":4,"author":"Steven Rowe","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","sourceNew":null,"sourceOld":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = directory.createOutput(fileName);\n        IOException priorException = null;\n        try {\n            // Write the Version info - must be a VInt because CFR reads a VInt\n            // in older versions!\n            os.writeVInt(FORMAT_CURRENT);\n            \n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(IndexFileNames.stripSegmentName(fe.file));\n                totalSize += fe.dir.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n        } catch (IOException e) {\n          priorException = e;\n        } finally {\n          IOUtils.closeSafely(priorException, os);\n        }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":4,"author":"Simon Willnauer","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","sourceNew":null,"sourceOld":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = directory.createOutput(fileName, context);\n        IOException priorException = null;\n        try {\n            // Write the Version info - must be a VInt because CFR reads a VInt\n            // in older versions!\n            os.writeVInt(FORMAT_CURRENT);\n            \n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(IndexFileNames.stripSegmentName(fe.file));\n                totalSize += fe.dir.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n        } catch (IOException e) {\n          priorException = e;\n        } finally {\n          IOUtils.closeSafely(priorException, os);\n        }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"32dd6d3e87d9e4f05e3e9de40bebf1ff1482771f":["766e2265a545239f23f615e12f17c86442ac136b"],"3242a09f703274d3b9283f2064a1a33064b53a1b":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6","462dfb1d8690f192817503773f5b8b94a702246a"],"0aab6e810b4b0d3743d6a048be0602801f4b3920":["32dd6d3e87d9e4f05e3e9de40bebf1ff1482771f"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["32dd6d3e87d9e4f05e3e9de40bebf1ff1482771f"],"2553b00f699380c64959ccb27991289aae87be2e":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","0aab6e810b4b0d3743d6a048be0602801f4b3920"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["639c36565ce03aed5b0fce7c9e4448e53a1f7efd","0aab6e810b4b0d3743d6a048be0602801f4b3920"],"fb10b6bcde550b87d8f10e5f010bd8f3021023b6":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"462dfb1d8690f192817503773f5b8b94a702246a":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"766e2265a545239f23f615e12f17c86442ac136b":["462dfb1d8690f192817503773f5b8b94a702246a"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["a3776dccca01c11e7046323cfad46a3b4a471233","32dd6d3e87d9e4f05e3e9de40bebf1ff1482771f"],"a3776dccca01c11e7046323cfad46a3b4a471233":["462dfb1d8690f192817503773f5b8b94a702246a","766e2265a545239f23f615e12f17c86442ac136b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["462dfb1d8690f192817503773f5b8b94a702246a","766e2265a545239f23f615e12f17c86442ac136b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0aab6e810b4b0d3743d6a048be0602801f4b3920"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"2e10cb22a8bdb44339e282925a29182bb2f3174d":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb","32dd6d3e87d9e4f05e3e9de40bebf1ff1482771f"]},"commit2Childs":{"32dd6d3e87d9e4f05e3e9de40bebf1ff1482771f":["0aab6e810b4b0d3743d6a048be0602801f4b3920","639c36565ce03aed5b0fce7c9e4448e53a1f7efd","5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"3242a09f703274d3b9283f2064a1a33064b53a1b":[],"0aab6e810b4b0d3743d6a048be0602801f4b3920":["2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["d083e83f225b11e5fdd900e83d26ddb385b6955c"],"2553b00f699380c64959ccb27991289aae87be2e":[],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"fb10b6bcde550b87d8f10e5f010bd8f3021023b6":["3242a09f703274d3b9283f2064a1a33064b53a1b","462dfb1d8690f192817503773f5b8b94a702246a"],"462dfb1d8690f192817503773f5b8b94a702246a":["3242a09f703274d3b9283f2064a1a33064b53a1b","766e2265a545239f23f615e12f17c86442ac136b","a3776dccca01c11e7046323cfad46a3b4a471233","c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"766e2265a545239f23f615e12f17c86442ac136b":["32dd6d3e87d9e4f05e3e9de40bebf1ff1482771f","a3776dccca01c11e7046323cfad46a3b4a471233","c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["2553b00f699380c64959ccb27991289aae87be2e"],"a3776dccca01c11e7046323cfad46a3b4a471233":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["2e10cb22a8bdb44339e282925a29182bb2f3174d"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"2e10cb22a8bdb44339e282925a29182bb2f3174d":[]},"heads":["3242a09f703274d3b9283f2064a1a33064b53a1b","2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c","cd5edd1f2b162a5cfa08efd17851a07373a96817","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}