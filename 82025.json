{"path":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","commits":[{"id":"2713584a660051cd646423be682771e3bbd99985","date":1425046322,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n      @Override\n      Scorer scorer(LeafReaderContext context, Bits acceptDocs, float score) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return null;\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(context.reader().maxDoc());\n        PostingsEnum docs = null;\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n          builder.or(docs);\n        }\n        final BitDocIdSet set = builder.build();\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new Scorer(this) {\n\n          @Override\n          public float score() throws IOException {\n            return score;\n          }\n\n          @Override\n          public int freq() throws IOException {\n            return 1;\n          }\n\n          @Override\n          public int docID() {\n            return disi.docID();\n          }\n\n          @Override\n          public int nextDoc() throws IOException {\n            return disi.nextDoc();\n          }\n\n          @Override\n          public int advance(int target) throws IOException {\n            return disi.advance(target);\n          }\n\n          @Override\n          public long cost() {\n            return disi.cost();\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":0,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n      @Override\n      Scorer scorer(LeafReaderContext context, Bits acceptDocs, float score) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return null;\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(context.reader().maxDoc());\n        PostingsEnum docs = null;\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n          builder.or(docs);\n        }\n        final BitDocIdSet set = builder.build();\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new Scorer(this) {\n\n          @Override\n          public float score() throws IOException {\n            return score;\n          }\n\n          @Override\n          public int freq() throws IOException {\n            return 1;\n          }\n\n          @Override\n          public int docID() {\n            return disi.docID();\n          }\n\n          @Override\n          public int nextDoc() throws IOException {\n            return disi.nextDoc();\n          }\n\n          @Override\n          public int advance(int target) throws IOException {\n            return disi.advance(target);\n          }\n\n          @Override\n          public long cost() {\n            return disi.cost();\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29efba95465cc25f76d9f92aec35c9f71b1a55ca","date":1428692677,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n      @Override\n      protected Scorer scorer(LeafReaderContext context, Bits acceptDocs, float score) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return null;\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(context.reader().maxDoc());\n        PostingsEnum docs = null;\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n          builder.or(docs);\n        }\n        final BitDocIdSet set = builder.build();\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new Scorer(this) {\n\n          @Override\n          public float score() throws IOException {\n            return score;\n          }\n\n          @Override\n          public int freq() throws IOException {\n            return 1;\n          }\n\n          @Override\n          public int docID() {\n            return disi.docID();\n          }\n\n          @Override\n          public int nextDoc() throws IOException {\n            return disi.nextDoc();\n          }\n\n          @Override\n          public int advance(int target) throws IOException {\n            return disi.advance(target);\n          }\n\n          @Override\n          public long cost() {\n            return disi.cost();\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n      @Override\n      Scorer scorer(LeafReaderContext context, Bits acceptDocs, float score) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return null;\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(context.reader().maxDoc());\n        PostingsEnum docs = null;\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n          builder.or(docs);\n        }\n        final BitDocIdSet set = builder.build();\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new Scorer(this) {\n\n          @Override\n          public float score() throws IOException {\n            return score;\n          }\n\n          @Override\n          public int freq() throws IOException {\n            return 1;\n          }\n\n          @Override\n          public int docID() {\n            return disi.docID();\n          }\n\n          @Override\n          public int nextDoc() throws IOException {\n            return disi.nextDoc();\n          }\n\n          @Override\n          public int advance(int target) throws IOException {\n            return disi.advance(target);\n          }\n\n          @Override\n          public long cost() {\n            return disi.cost();\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a7f94ff172f40ff68a926d112e25b96bc38e5a27","date":1431002360,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return null;\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(context.reader().maxDoc());\n        PostingsEnum docs = null;\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n          builder.or(docs);\n        }\n        final BitDocIdSet set = builder.build();\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n      @Override\n      protected Scorer scorer(LeafReaderContext context, Bits acceptDocs, float score) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return null;\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(context.reader().maxDoc());\n        PostingsEnum docs = null;\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n          builder.or(docs);\n        }\n        final BitDocIdSet set = builder.build();\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new Scorer(this) {\n\n          @Override\n          public float score() throws IOException {\n            return score;\n          }\n\n          @Override\n          public int freq() throws IOException {\n            return 1;\n          }\n\n          @Override\n          public int docID() {\n            return disi.docID();\n          }\n\n          @Override\n          public int nextDoc() throws IOException {\n            return disi.nextDoc();\n          }\n\n          @Override\n          public int advance(int target) throws IOException {\n            return disi.advance(target);\n          }\n\n          @Override\n          public long cost() {\n            return disi.cost();\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"721230395a78975958098de53098f4520729619d","date":1432106709,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), termsEnum.termState(), termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return null;\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(context.reader().maxDoc());\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery bq = new BooleanQuery();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq);\n          q.setBoost(score());\n          return searcher.rewrite(q).createWeight(searcher, needsScores).scorer(context, acceptDocs);\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(acceptDocs, docs, PostingsEnum.NONE);\n            builder.or(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n          builder.or(docs);\n        } while (termsEnum.next() != null);\n\n        final BitDocIdSet set = builder.build();\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return null;\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(context.reader().maxDoc());\n        PostingsEnum docs = null;\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n          builder.or(docs);\n        }\n        final BitDocIdSet set = builder.build();\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":["17a677e88529303e630fb314d1506ea0cdb40f00"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"17a677e88529303e630fb314d1506ea0cdb40f00","date":1432211402,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          if (state.isRealTerm() == false) {\n            // TermQuery does not accept fake terms for now\n            return false;\n          }\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return null;\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(context.reader().maxDoc());\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery bq = new BooleanQuery();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq);\n          q.setBoost(score());\n          return searcher.rewrite(q).createWeight(searcher, needsScores).scorer(context, acceptDocs);\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(acceptDocs, docs, PostingsEnum.NONE);\n            builder.or(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n          builder.or(docs);\n        } while (termsEnum.next() != null);\n\n        final BitDocIdSet set = builder.build();\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), termsEnum.termState(), termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return null;\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(context.reader().maxDoc());\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery bq = new BooleanQuery();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq);\n          q.setBoost(score());\n          return searcher.rewrite(q).createWeight(searcher, needsScores).scorer(context, acceptDocs);\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(acceptDocs, docs, PostingsEnum.NONE);\n            builder.or(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n          builder.or(docs);\n        } while (termsEnum.next() != null);\n\n        final BitDocIdSet set = builder.build();\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n    };\n  }\n\n","bugFix":["721230395a78975958098de53098f4520729619d"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a7ac28a6733b2a658eea5b10afe8b071c5fbd0a","date":1432219909,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          if (state.isRealTerm() == false) {\n            // TermQuery does not accept fake terms for now\n            return false;\n          }\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few terms, or build a bitset containing matching docs.\n       */\n      private WeightOrBitSet rewrite(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return new WeightOrBitSet((BitDocIdSet) null);\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery bq = new BooleanQuery();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq);\n          q.setBoost(score());\n          return new WeightOrBitSet(searcher.rewrite(q).createWeight(searcher, needsScores));\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(context.reader().maxDoc());\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(acceptDocs, docs, PostingsEnum.NONE);\n            builder.or(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n          builder.or(docs);\n        } while (termsEnum.next() != null);\n\n        return new WeightOrBitSet(builder.build());\n      }\n\n      private Scorer scorer(BitDocIdSet set) {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context, acceptDocs);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context, acceptDocs);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.bitset);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context, acceptDocs);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context, acceptDocs);\n        } else {\n          return scorer(weightOrBitSet.bitset);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          if (state.isRealTerm() == false) {\n            // TermQuery does not accept fake terms for now\n            return false;\n          }\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return null;\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(context.reader().maxDoc());\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery bq = new BooleanQuery();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq);\n          q.setBoost(score());\n          return searcher.rewrite(q).createWeight(searcher, needsScores).scorer(context, acceptDocs);\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(acceptDocs, docs, PostingsEnum.NONE);\n            builder.or(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n          builder.or(docs);\n        } while (termsEnum.next() != null);\n\n        final BitDocIdSet set = builder.build();\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3f68d01cf19df971dcdcb05e30247f4ad7ec9747","date":1434611645,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          if (state.isRealTerm() == false) {\n            // TermQuery does not accept fake terms for now\n            return false;\n          }\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few terms, or build a bitset containing matching docs.\n       */\n      private WeightOrBitSet rewrite(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return new WeightOrBitSet((BitDocIdSet) null);\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          q.setBoost(score());\n          return new WeightOrBitSet(searcher.rewrite(q).createWeight(searcher, needsScores));\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(context.reader().maxDoc());\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(acceptDocs, docs, PostingsEnum.NONE);\n            builder.or(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n          builder.or(docs);\n        } while (termsEnum.next() != null);\n\n        return new WeightOrBitSet(builder.build());\n      }\n\n      private Scorer scorer(BitDocIdSet set) {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context, acceptDocs);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context, acceptDocs);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.bitset);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context, acceptDocs);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context, acceptDocs);\n        } else {\n          return scorer(weightOrBitSet.bitset);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          if (state.isRealTerm() == false) {\n            // TermQuery does not accept fake terms for now\n            return false;\n          }\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few terms, or build a bitset containing matching docs.\n       */\n      private WeightOrBitSet rewrite(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return new WeightOrBitSet((BitDocIdSet) null);\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery bq = new BooleanQuery();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq);\n          q.setBoost(score());\n          return new WeightOrBitSet(searcher.rewrite(q).createWeight(searcher, needsScores));\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(context.reader().maxDoc());\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(acceptDocs, docs, PostingsEnum.NONE);\n            builder.or(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n          builder.or(docs);\n        } while (termsEnum.next() != null);\n\n        return new WeightOrBitSet(builder.build());\n      }\n\n      private Scorer scorer(BitDocIdSet set) {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context, acceptDocs);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context, acceptDocs);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.bitset);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context, acceptDocs);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context, acceptDocs);\n        } else {\n          return scorer(weightOrBitSet.bitset);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          if (state.isRealTerm() == false) {\n            // TermQuery does not accept fake terms for now\n            return false;\n          }\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few terms, or build a bitset containing matching docs.\n       */\n      private WeightOrBitSet rewrite(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return new WeightOrBitSet((BitDocIdSet) null);\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          q.setBoost(score());\n          return new WeightOrBitSet(searcher.rewrite(q).createWeight(searcher, needsScores));\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(context.reader().maxDoc());\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(docs, PostingsEnum.NONE);\n            builder.or(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          builder.or(docs);\n        } while (termsEnum.next() != null);\n\n        return new WeightOrBitSet(builder.build());\n      }\n\n      private Scorer scorer(BitDocIdSet set) {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.bitset);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.bitset);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          if (state.isRealTerm() == false) {\n            // TermQuery does not accept fake terms for now\n            return false;\n          }\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few terms, or build a bitset containing matching docs.\n       */\n      private WeightOrBitSet rewrite(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return new WeightOrBitSet((BitDocIdSet) null);\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          q.setBoost(score());\n          return new WeightOrBitSet(searcher.rewrite(q).createWeight(searcher, needsScores));\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(context.reader().maxDoc());\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(acceptDocs, docs, PostingsEnum.NONE);\n            builder.or(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n          builder.or(docs);\n        } while (termsEnum.next() != null);\n\n        return new WeightOrBitSet(builder.build());\n      }\n\n      private Scorer scorer(BitDocIdSet set) {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context, acceptDocs);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context, acceptDocs);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.bitset);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context, acceptDocs);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context, acceptDocs);\n        } else {\n          return scorer(weightOrBitSet.bitset);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0e91a2d9ed80172872da0f517870da6756289554","date":1436431140,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          if (state.isRealTerm() == false) {\n            // TermQuery does not accept fake terms for now\n            return false;\n          }\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return new WeightOrDocIdSet((DocIdSet) null);\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          q.setBoost(score());\n          return new WeightOrDocIdSet(searcher.rewrite(q).createWeight(searcher, needsScores));\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        DocIdSetBuilder builder = new DocIdSetBuilder(context.reader().maxDoc());\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(docs, PostingsEnum.NONE);\n            builder.add(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          builder.add(docs);\n        } while (termsEnum.next() != null);\n\n        return new WeightOrDocIdSet(builder.build());\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          if (state.isRealTerm() == false) {\n            // TermQuery does not accept fake terms for now\n            return false;\n          }\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few terms, or build a bitset containing matching docs.\n       */\n      private WeightOrBitSet rewrite(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return new WeightOrBitSet((BitDocIdSet) null);\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          q.setBoost(score());\n          return new WeightOrBitSet(searcher.rewrite(q).createWeight(searcher, needsScores));\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(context.reader().maxDoc());\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(docs, PostingsEnum.NONE);\n            builder.or(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          builder.or(docs);\n        } while (termsEnum.next() != null);\n\n        return new WeightOrBitSet(builder.build());\n      }\n\n      private Scorer scorer(BitDocIdSet set) {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.bitset);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.bitset);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2dfdf766e55e943d942055d7de53c7ad6bc45283","date":1441632886,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          if (state.isRealTerm() == false) {\n            // TermQuery does not accept fake terms for now\n            return false;\n          }\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return new WeightOrDocIdSet((DocIdSet) null);\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        DocIdSetBuilder builder = new DocIdSetBuilder(context.reader().maxDoc());\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(docs, PostingsEnum.NONE);\n            builder.add(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          builder.add(docs);\n        } while (termsEnum.next() != null);\n\n        return new WeightOrDocIdSet(builder.build());\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          if (state.isRealTerm() == false) {\n            // TermQuery does not accept fake terms for now\n            return false;\n          }\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return new WeightOrDocIdSet((DocIdSet) null);\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          q.setBoost(score());\n          return new WeightOrDocIdSet(searcher.rewrite(q).createWeight(searcher, needsScores));\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        DocIdSetBuilder builder = new DocIdSetBuilder(context.reader().maxDoc());\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(docs, PostingsEnum.NONE);\n            builder.add(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          builder.add(docs);\n        } while (termsEnum.next() != null);\n\n        return new WeightOrDocIdSet(builder.build());\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9ee4c03e3ee986704eeeb45c571d001905a6430","date":1462194267,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          if (state.isRealTerm() == false) {\n            // TermQuery does not accept fake terms for now\n            return false;\n          }\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return new WeightOrDocIdSet((DocIdSet) null);\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        DocIdSetBuilder builder = new DocIdSetBuilder(context.reader().maxDoc(), terms);\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(docs, PostingsEnum.NONE);\n            builder.add(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          builder.add(docs);\n        } while (termsEnum.next() != null);\n\n        return new WeightOrDocIdSet(builder.build());\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          if (state.isRealTerm() == false) {\n            // TermQuery does not accept fake terms for now\n            return false;\n          }\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return new WeightOrDocIdSet((DocIdSet) null);\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        DocIdSetBuilder builder = new DocIdSetBuilder(context.reader().maxDoc());\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(docs, PostingsEnum.NONE);\n            builder.add(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          builder.add(docs);\n        } while (termsEnum.next() != null);\n\n        return new WeightOrDocIdSet(builder.build());\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30540ec27130887a9372c159e8fe971200f37727","date":1462223109,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          if (state.isRealTerm() == false) {\n            // TermQuery does not accept fake terms for now\n            return false;\n          }\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return new WeightOrDocIdSet((DocIdSet) null);\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        DocIdSetBuilder builder = new DocIdSetBuilder(context.reader().maxDoc(), terms);\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(docs, PostingsEnum.NONE);\n            builder.add(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          builder.add(docs);\n        } while (termsEnum.next() != null);\n\n        return new WeightOrDocIdSet(builder.build());\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          if (state.isRealTerm() == false) {\n            // TermQuery does not accept fake terms for now\n            return false;\n          }\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return new WeightOrDocIdSet((DocIdSet) null);\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        DocIdSetBuilder builder = new DocIdSetBuilder(context.reader().maxDoc());\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(docs, PostingsEnum.NONE);\n            builder.add(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          builder.add(docs);\n        } while (termsEnum.next() != null);\n\n        return new WeightOrDocIdSet(builder.build());\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55b50463286869f584cf849d1587a0fcd54d1dfa","date":1462378517,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          if (state.isRealTerm() == false) {\n            // TermQuery does not accept fake terms for now\n            return false;\n          }\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return new WeightOrDocIdSet((DocIdSet) null);\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        DocIdSetBuilder builder = new DocIdSetBuilder(context.reader().maxDoc(), terms);\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(docs, PostingsEnum.NONE);\n            builder.add(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          builder.add(docs);\n        } while (termsEnum.next() != null);\n\n        return new WeightOrDocIdSet(builder.build());\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          if (state.isRealTerm() == false) {\n            // TermQuery does not accept fake terms for now\n            return false;\n          }\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return new WeightOrDocIdSet((DocIdSet) null);\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        DocIdSetBuilder builder = new DocIdSetBuilder(context.reader().maxDoc());\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(docs, PostingsEnum.NONE);\n            builder.add(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          builder.add(docs);\n        } while (termsEnum.next() != null);\n\n        return new WeightOrDocIdSet(builder.build());\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"25b8a027ba57951e63075a2ae9647c5c4a8c5c5f","date":1466407389,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return new WeightOrDocIdSet((DocIdSet) null);\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        DocIdSetBuilder builder = new DocIdSetBuilder(context.reader().maxDoc(), terms);\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(docs, PostingsEnum.NONE);\n            builder.add(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          builder.add(docs);\n        } while (termsEnum.next() != null);\n\n        return new WeightOrDocIdSet(builder.build());\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          if (state.isRealTerm() == false) {\n            // TermQuery does not accept fake terms for now\n            return false;\n          }\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return new WeightOrDocIdSet((DocIdSet) null);\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        DocIdSetBuilder builder = new DocIdSetBuilder(context.reader().maxDoc(), terms);\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(docs, PostingsEnum.NONE);\n            builder.add(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          builder.add(docs);\n        } while (termsEnum.next() != null);\n\n        return new WeightOrDocIdSet(builder.build());\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6aaba221b22442bdf0ef28770c25fe259dfb3f55","date":1466496193,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return new WeightOrDocIdSet((DocIdSet) null);\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        DocIdSetBuilder builder = new DocIdSetBuilder(context.reader().maxDoc(), terms);\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(docs, PostingsEnum.NONE);\n            builder.add(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          builder.add(docs);\n        } while (termsEnum.next() != null);\n\n        return new WeightOrDocIdSet(builder.build());\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          if (state.isRealTerm() == false) {\n            // TermQuery does not accept fake terms for now\n            return false;\n          }\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return new WeightOrDocIdSet((DocIdSet) null);\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        DocIdSetBuilder builder = new DocIdSetBuilder(context.reader().maxDoc(), terms);\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(docs, PostingsEnum.NONE);\n            builder.add(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          builder.add(docs);\n        } while (termsEnum.next() != null);\n\n        return new WeightOrDocIdSet(builder.build());\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"02e175abd2c4c1611c5a9647486ae8ba249a94c1","date":1468327116,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return new WeightOrDocIdSet((DocIdSet) null);\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        DocIdSetBuilder builder = new DocIdSetBuilder(context.reader().maxDoc(), terms);\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(docs, PostingsEnum.NONE);\n            builder.add(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          builder.add(docs);\n        } while (termsEnum.next() != null);\n\n        return new WeightOrDocIdSet(builder.build());\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return new WeightOrDocIdSet((DocIdSet) null);\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        DocIdSetBuilder builder = new DocIdSetBuilder(context.reader().maxDoc(), terms);\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(docs, PostingsEnum.NONE);\n            builder.add(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          builder.add(docs);\n        } while (termsEnum.next() != null);\n\n        return new WeightOrDocIdSet(builder.build());\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/search/MultiTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":null,"sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      /** Try to collect terms from the given terms enum and return true iff all\n       *  terms could be collected. If {@code false} is returned, the enum is\n       *  left positioned on the next term. */\n      private boolean collectTerms(LeafReaderContext context, TermsEnum termsEnum, List<TermAndState> terms) throws IOException {\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        for (int i = 0; i < threshold; ++i) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            return true;\n          }\n          TermState state = termsEnum.termState();\n          if (state.isRealTerm() == false) {\n            // TermQuery does not accept fake terms for now\n            return false;\n          }\n          terms.add(new TermAndState(BytesRef.deepCopyOf(term), state, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n        }\n        return termsEnum.next() == null;\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          // field does not exist\n          return new WeightOrDocIdSet((DocIdSet) null);\n        }\n\n        final TermsEnum termsEnum = query.getTermsEnum(terms);\n        assert termsEnum != null;\n\n        PostingsEnum docs = null;\n\n        final List<TermAndState> collectedTerms = new ArrayList<>();\n        if (collectTerms(context, termsEnum, collectedTerms)) {\n          // build a boolean query\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : collectedTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(query.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        }\n\n        // Too many terms: go back to the terms we already collected and start building the bit set\n        DocIdSetBuilder builder = new DocIdSetBuilder(context.reader().maxDoc(), terms);\n        if (collectedTerms.isEmpty() == false) {\n          TermsEnum termsEnum2 = terms.iterator();\n          for (TermAndState t : collectedTerms) {\n            termsEnum2.seekExact(t.term, t.state);\n            docs = termsEnum2.postings(docs, PostingsEnum.NONE);\n            builder.add(docs);\n          }\n        }\n\n        // Then keep filling the bit set with remaining terms\n        do {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          builder.add(docs);\n        } while (termsEnum.next() != null);\n\n        return new WeightOrDocIdSet(builder.build());\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a7f94ff172f40ff68a926d112e25b96bc38e5a27":["29efba95465cc25f76d9f92aec35c9f71b1a55ca"],"2713584a660051cd646423be682771e3bbd99985":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"2dfdf766e55e943d942055d7de53c7ad6bc45283":["0e91a2d9ed80172872da0f517870da6756289554"],"30540ec27130887a9372c159e8fe971200f37727":["2dfdf766e55e943d942055d7de53c7ad6bc45283","c9ee4c03e3ee986704eeeb45c571d001905a6430"],"0a7ac28a6733b2a658eea5b10afe8b071c5fbd0a":["17a677e88529303e630fb314d1506ea0cdb40f00"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["55b50463286869f584cf849d1587a0fcd54d1dfa","02e175abd2c4c1611c5a9647486ae8ba249a94c1"],"55b50463286869f584cf849d1587a0fcd54d1dfa":["2dfdf766e55e943d942055d7de53c7ad6bc45283","30540ec27130887a9372c159e8fe971200f37727"],"3f68d01cf19df971dcdcb05e30247f4ad7ec9747":["0a7ac28a6733b2a658eea5b10afe8b071c5fbd0a"],"29efba95465cc25f76d9f92aec35c9f71b1a55ca":["2713584a660051cd646423be682771e3bbd99985"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","2713584a660051cd646423be682771e3bbd99985"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["3f68d01cf19df971dcdcb05e30247f4ad7ec9747"],"c9ee4c03e3ee986704eeeb45c571d001905a6430":["2dfdf766e55e943d942055d7de53c7ad6bc45283"],"6aaba221b22442bdf0ef28770c25fe259dfb3f55":["30540ec27130887a9372c159e8fe971200f37727","25b8a027ba57951e63075a2ae9647c5c4a8c5c5f"],"25b8a027ba57951e63075a2ae9647c5c4a8c5c5f":["30540ec27130887a9372c159e8fe971200f37727"],"0e91a2d9ed80172872da0f517870da6756289554":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"02e175abd2c4c1611c5a9647486ae8ba249a94c1":["6aaba221b22442bdf0ef28770c25fe259dfb3f55"],"17a677e88529303e630fb314d1506ea0cdb40f00":["721230395a78975958098de53098f4520729619d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["02e175abd2c4c1611c5a9647486ae8ba249a94c1"],"721230395a78975958098de53098f4520729619d":["a7f94ff172f40ff68a926d112e25b96bc38e5a27"]},"commit2Childs":{"a7f94ff172f40ff68a926d112e25b96bc38e5a27":["721230395a78975958098de53098f4520729619d"],"2713584a660051cd646423be682771e3bbd99985":["29efba95465cc25f76d9f92aec35c9f71b1a55ca","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"2dfdf766e55e943d942055d7de53c7ad6bc45283":["30540ec27130887a9372c159e8fe971200f37727","55b50463286869f584cf849d1587a0fcd54d1dfa","c9ee4c03e3ee986704eeeb45c571d001905a6430"],"30540ec27130887a9372c159e8fe971200f37727":["55b50463286869f584cf849d1587a0fcd54d1dfa","6aaba221b22442bdf0ef28770c25fe259dfb3f55","25b8a027ba57951e63075a2ae9647c5c4a8c5c5f"],"0a7ac28a6733b2a658eea5b10afe8b071c5fbd0a":["3f68d01cf19df971dcdcb05e30247f4ad7ec9747"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"55b50463286869f584cf849d1587a0fcd54d1dfa":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"29efba95465cc25f76d9f92aec35c9f71b1a55ca":["a7f94ff172f40ff68a926d112e25b96bc38e5a27"],"3f68d01cf19df971dcdcb05e30247f4ad7ec9747":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"0f4464508ee83288c8c4585b533f9faaa93aa314":["0e91a2d9ed80172872da0f517870da6756289554"],"c9ee4c03e3ee986704eeeb45c571d001905a6430":["30540ec27130887a9372c159e8fe971200f37727"],"6aaba221b22442bdf0ef28770c25fe259dfb3f55":["02e175abd2c4c1611c5a9647486ae8ba249a94c1"],"25b8a027ba57951e63075a2ae9647c5c4a8c5c5f":["6aaba221b22442bdf0ef28770c25fe259dfb3f55"],"0e91a2d9ed80172872da0f517870da6756289554":["2dfdf766e55e943d942055d7de53c7ad6bc45283"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["2713584a660051cd646423be682771e3bbd99985","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"02e175abd2c4c1611c5a9647486ae8ba249a94c1":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"17a677e88529303e630fb314d1506ea0cdb40f00":["0a7ac28a6733b2a658eea5b10afe8b071c5fbd0a"],"721230395a78975958098de53098f4520729619d":["17a677e88529303e630fb314d1506ea0cdb40f00"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}