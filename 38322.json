{"path":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsWithDeletions(MergeState,Lucene40TermVectorsReader,MergeState.IndexReaderAndLiveDocs,int[],int[]).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsWithDeletions(MergeState,Lucene40TermVectorsReader,MergeState.IndexReaderAndLiveDocs,int[],int[]).mjava","pathOld":"lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsWithDeletions(MergeState,Lucene40TermVectorsReader,MergeState.IndexReaderAndLiveDocs,int[],int[]).mjava","sourceNew":"  private int copyVectorsWithDeletions(MergeState mergeState,\n                                        final Lucene40TermVectorsReader matchingVectorsReader,\n                                        final MergeState.IndexReaderAndLiveDocs reader,\n                                        int rawDocLengths[],\n                                        int rawDocLengths2[])\n          throws IOException, MergeAbortedException {\n    final int maxDoc = reader.reader.maxDoc();\n    final Bits liveDocs = reader.liveDocs;\n    int totalNumDocs = 0;\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int docNum = 0; docNum < maxDoc;) {\n        if (!liveDocs.get(docNum)) {\n          // skip deleted docs\n          ++docNum;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = docNum, numDocs = 0;\n        do {\n          docNum++;\n          numDocs++;\n          if (docNum >= maxDoc) break;\n          if (!liveDocs.get(docNum)) {\n            docNum++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n        totalNumDocs += numDocs;\n        mergeState.checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        if (!liveDocs.get(docNum)) {\n          // skip deleted docs\n          continue;\n        }\n        \n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState.fieldInfos);\n        totalNumDocs++;\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return totalNumDocs;\n  }\n\n","sourceOld":"  private int copyVectorsWithDeletions(MergeState mergeState,\n                                        final Lucene40TermVectorsReader matchingVectorsReader,\n                                        final MergeState.IndexReaderAndLiveDocs reader,\n                                        int rawDocLengths[],\n                                        int rawDocLengths2[])\n          throws IOException, MergeAbortedException {\n    final int maxDoc = reader.reader.maxDoc();\n    final Bits liveDocs = reader.liveDocs;\n    int totalNumDocs = 0;\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int docNum = 0; docNum < maxDoc;) {\n        if (!liveDocs.get(docNum)) {\n          // skip deleted docs\n          ++docNum;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = docNum, numDocs = 0;\n        do {\n          docNum++;\n          numDocs++;\n          if (docNum >= maxDoc) break;\n          if (!liveDocs.get(docNum)) {\n            docNum++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n        totalNumDocs += numDocs;\n        mergeState.checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        if (!liveDocs.get(docNum)) {\n          // skip deleted docs\n          continue;\n        }\n        \n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState.fieldInfos);\n        totalNumDocs++;\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return totalNumDocs;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d08eba3d52b63561ebf936481ce73e6b6a14aa03","date":1333879759,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsWithDeletions(MergeState,Lucene40TermVectorsReader,MergeState.IndexReaderAndLiveDocs,int[],int[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsWithDeletions(MergeState,Lucene40TermVectorsReader,MergeState.IndexReaderAndLiveDocs,int[],int[]).mjava","sourceNew":"  private int copyVectorsWithDeletions(MergeState mergeState,\n                                        final Lucene40TermVectorsReader matchingVectorsReader,\n                                        final MergeState.IndexReaderAndLiveDocs reader,\n                                        int rawDocLengths[],\n                                        int rawDocLengths2[])\n          throws IOException, MergeAbortedException {\n    final int maxDoc = reader.reader.maxDoc();\n    final Bits liveDocs = reader.liveDocs;\n    int totalNumDocs = 0;\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int docNum = 0; docNum < maxDoc;) {\n        if (!liveDocs.get(docNum)) {\n          // skip deleted docs\n          ++docNum;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = docNum, numDocs = 0;\n        do {\n          docNum++;\n          numDocs++;\n          if (docNum >= maxDoc) break;\n          if (!liveDocs.get(docNum)) {\n            docNum++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n        totalNumDocs += numDocs;\n        mergeState.checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        if (!liveDocs.get(docNum)) {\n          // skip deleted docs\n          continue;\n        }\n        \n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        InvertedFields vectors = reader.reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState.fieldInfos);\n        totalNumDocs++;\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return totalNumDocs;\n  }\n\n","sourceOld":"  private int copyVectorsWithDeletions(MergeState mergeState,\n                                        final Lucene40TermVectorsReader matchingVectorsReader,\n                                        final MergeState.IndexReaderAndLiveDocs reader,\n                                        int rawDocLengths[],\n                                        int rawDocLengths2[])\n          throws IOException, MergeAbortedException {\n    final int maxDoc = reader.reader.maxDoc();\n    final Bits liveDocs = reader.liveDocs;\n    int totalNumDocs = 0;\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int docNum = 0; docNum < maxDoc;) {\n        if (!liveDocs.get(docNum)) {\n          // skip deleted docs\n          ++docNum;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = docNum, numDocs = 0;\n        do {\n          docNum++;\n          numDocs++;\n          if (docNum >= maxDoc) break;\n          if (!liveDocs.get(docNum)) {\n            docNum++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n        totalNumDocs += numDocs;\n        mergeState.checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        if (!liveDocs.get(docNum)) {\n          // skip deleted docs\n          continue;\n        }\n        \n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState.fieldInfos);\n        totalNumDocs++;\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return totalNumDocs;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf","date":1333892281,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsWithDeletions(MergeState,Lucene40TermVectorsReader,MergeState.IndexReaderAndLiveDocs,int[],int[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsWithDeletions(MergeState,Lucene40TermVectorsReader,MergeState.IndexReaderAndLiveDocs,int[],int[]).mjava","sourceNew":"  private int copyVectorsWithDeletions(MergeState mergeState,\n                                        final Lucene40TermVectorsReader matchingVectorsReader,\n                                        final MergeState.IndexReaderAndLiveDocs reader,\n                                        int rawDocLengths[],\n                                        int rawDocLengths2[])\n          throws IOException, MergeAbortedException {\n    final int maxDoc = reader.reader.maxDoc();\n    final Bits liveDocs = reader.liveDocs;\n    int totalNumDocs = 0;\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int docNum = 0; docNum < maxDoc;) {\n        if (!liveDocs.get(docNum)) {\n          // skip deleted docs\n          ++docNum;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = docNum, numDocs = 0;\n        do {\n          docNum++;\n          numDocs++;\n          if (docNum >= maxDoc) break;\n          if (!liveDocs.get(docNum)) {\n            docNum++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n        totalNumDocs += numDocs;\n        mergeState.checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        if (!liveDocs.get(docNum)) {\n          // skip deleted docs\n          continue;\n        }\n        \n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState.fieldInfos);\n        totalNumDocs++;\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return totalNumDocs;\n  }\n\n","sourceOld":"  private int copyVectorsWithDeletions(MergeState mergeState,\n                                        final Lucene40TermVectorsReader matchingVectorsReader,\n                                        final MergeState.IndexReaderAndLiveDocs reader,\n                                        int rawDocLengths[],\n                                        int rawDocLengths2[])\n          throws IOException, MergeAbortedException {\n    final int maxDoc = reader.reader.maxDoc();\n    final Bits liveDocs = reader.liveDocs;\n    int totalNumDocs = 0;\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int docNum = 0; docNum < maxDoc;) {\n        if (!liveDocs.get(docNum)) {\n          // skip deleted docs\n          ++docNum;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = docNum, numDocs = 0;\n        do {\n          docNum++;\n          numDocs++;\n          if (docNum >= maxDoc) break;\n          if (!liveDocs.get(docNum)) {\n            docNum++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n        totalNumDocs += numDocs;\n        mergeState.checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        if (!liveDocs.get(docNum)) {\n          // skip deleted docs\n          continue;\n        }\n        \n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        InvertedFields vectors = reader.reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState.fieldInfos);\n        totalNumDocs++;\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return totalNumDocs;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4d3e8520fd031bab31fd0e4d480e55958bc45efe","date":1340901565,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsWithDeletions(MergeState,Lucene40TermVectorsReader,MergeState.IndexReaderAndLiveDocs,int[],int[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsWithDeletions(MergeState,Lucene40TermVectorsReader,MergeState.IndexReaderAndLiveDocs,int[],int[]).mjava","sourceNew":"  private int copyVectorsWithDeletions(MergeState mergeState,\n                                        final Lucene40TermVectorsReader matchingVectorsReader,\n                                        final MergeState.IndexReaderAndLiveDocs reader,\n                                        int rawDocLengths[],\n                                        int rawDocLengths2[])\n          throws IOException {\n    final int maxDoc = reader.reader.maxDoc();\n    final Bits liveDocs = reader.liveDocs;\n    int totalNumDocs = 0;\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int docNum = 0; docNum < maxDoc;) {\n        if (!liveDocs.get(docNum)) {\n          // skip deleted docs\n          ++docNum;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = docNum, numDocs = 0;\n        do {\n          docNum++;\n          numDocs++;\n          if (docNum >= maxDoc) break;\n          if (!liveDocs.get(docNum)) {\n            docNum++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n        totalNumDocs += numDocs;\n        mergeState.checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        if (!liveDocs.get(docNum)) {\n          // skip deleted docs\n          continue;\n        }\n        \n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState.fieldInfos);\n        totalNumDocs++;\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return totalNumDocs;\n  }\n\n","sourceOld":"  private int copyVectorsWithDeletions(MergeState mergeState,\n                                        final Lucene40TermVectorsReader matchingVectorsReader,\n                                        final MergeState.IndexReaderAndLiveDocs reader,\n                                        int rawDocLengths[],\n                                        int rawDocLengths2[])\n          throws IOException, MergeAbortedException {\n    final int maxDoc = reader.reader.maxDoc();\n    final Bits liveDocs = reader.liveDocs;\n    int totalNumDocs = 0;\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int docNum = 0; docNum < maxDoc;) {\n        if (!liveDocs.get(docNum)) {\n          // skip deleted docs\n          ++docNum;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = docNum, numDocs = 0;\n        do {\n          docNum++;\n          numDocs++;\n          if (docNum >= maxDoc) break;\n          if (!liveDocs.get(docNum)) {\n            docNum++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n        totalNumDocs += numDocs;\n        mergeState.checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        if (!liveDocs.get(docNum)) {\n          // skip deleted docs\n          continue;\n        }\n        \n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState.fieldInfos);\n        totalNumDocs++;\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return totalNumDocs;\n  }\n\n","bugFix":["3cc749c053615f5871f3b95715fe292f34e70a53"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c95a819869502635864dac0a788f874787e3395b","date":1341394787,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsWithDeletions(MergeState,Lucene40TermVectorsReader,AtomicReader,int[],int[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsWithDeletions(MergeState,Lucene40TermVectorsReader,MergeState.IndexReaderAndLiveDocs,int[],int[]).mjava","sourceNew":"  private int copyVectorsWithDeletions(MergeState mergeState,\n                                        final Lucene40TermVectorsReader matchingVectorsReader,\n                                        final AtomicReader reader,\n                                        int rawDocLengths[],\n                                        int rawDocLengths2[])\n          throws IOException {\n    final int maxDoc = reader.maxDoc();\n    final Bits liveDocs = reader.getLiveDocs();\n    int totalNumDocs = 0;\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int docNum = 0; docNum < maxDoc;) {\n        if (!liveDocs.get(docNum)) {\n          // skip deleted docs\n          ++docNum;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = docNum, numDocs = 0;\n        do {\n          docNum++;\n          numDocs++;\n          if (docNum >= maxDoc) break;\n          if (!liveDocs.get(docNum)) {\n            docNum++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n        totalNumDocs += numDocs;\n        mergeState.checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        if (!liveDocs.get(docNum)) {\n          // skip deleted docs\n          continue;\n        }\n        \n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState.fieldInfos);\n        totalNumDocs++;\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return totalNumDocs;\n  }\n\n","sourceOld":"  private int copyVectorsWithDeletions(MergeState mergeState,\n                                        final Lucene40TermVectorsReader matchingVectorsReader,\n                                        final MergeState.IndexReaderAndLiveDocs reader,\n                                        int rawDocLengths[],\n                                        int rawDocLengths2[])\n          throws IOException {\n    final int maxDoc = reader.reader.maxDoc();\n    final Bits liveDocs = reader.liveDocs;\n    int totalNumDocs = 0;\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int docNum = 0; docNum < maxDoc;) {\n        if (!liveDocs.get(docNum)) {\n          // skip deleted docs\n          ++docNum;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = docNum, numDocs = 0;\n        do {\n          docNum++;\n          numDocs++;\n          if (docNum >= maxDoc) break;\n          if (!liveDocs.get(docNum)) {\n            docNum++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n        totalNumDocs += numDocs;\n        mergeState.checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        if (!liveDocs.get(docNum)) {\n          // skip deleted docs\n          continue;\n        }\n        \n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState.fieldInfos);\n        totalNumDocs++;\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return totalNumDocs;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe33227f6805edab2036cbb80645cc4e2d1fa424","date":1342713534,"type":4,"author":"Michael McCandless","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsWithDeletions(MergeState,Lucene40TermVectorsReader,MergeState.IndexReaderAndLiveDocs,int[],int[]).mjava","sourceNew":null,"sourceOld":"  private int copyVectorsWithDeletions(MergeState mergeState,\n                                        final Lucene40TermVectorsReader matchingVectorsReader,\n                                        final MergeState.IndexReaderAndLiveDocs reader,\n                                        int rawDocLengths[],\n                                        int rawDocLengths2[])\n          throws IOException, MergeAbortedException {\n    final int maxDoc = reader.reader.maxDoc();\n    final Bits liveDocs = reader.liveDocs;\n    int totalNumDocs = 0;\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int docNum = 0; docNum < maxDoc;) {\n        if (!liveDocs.get(docNum)) {\n          // skip deleted docs\n          ++docNum;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = docNum, numDocs = 0;\n        do {\n          docNum++;\n          numDocs++;\n          if (docNum >= maxDoc) break;\n          if (!liveDocs.get(docNum)) {\n            docNum++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n        totalNumDocs += numDocs;\n        mergeState.checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        if (!liveDocs.get(docNum)) {\n          // skip deleted docs\n          continue;\n        }\n        \n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState.fieldInfos);\n        totalNumDocs++;\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return totalNumDocs;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c95a819869502635864dac0a788f874787e3395b":["4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf","c95a819869502635864dac0a788f874787e3395b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf":["d08eba3d52b63561ebf936481ce73e6b6a14aa03"],"d08eba3d52b63561ebf936481ce73e6b6a14aa03":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"4d3e8520fd031bab31fd0e4d480e55958bc45efe":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c95a819869502635864dac0a788f874787e3395b"]},"commit2Childs":{"c95a819869502635864dac0a788f874787e3395b":["fe33227f6805edab2036cbb80645cc4e2d1fa424","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["d08eba3d52b63561ebf936481ce73e6b6a14aa03"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf":["fe33227f6805edab2036cbb80645cc4e2d1fa424","4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"d08eba3d52b63561ebf936481ce73e6b6a14aa03":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf"],"4d3e8520fd031bab31fd0e4d480e55958bc45efe":["c95a819869502635864dac0a788f874787e3395b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["fe33227f6805edab2036cbb80645cc4e2d1fa424","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}