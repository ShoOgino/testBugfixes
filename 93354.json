{"path":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(Iterable[Term],ReadersAndUpdates,SegmentReader).mjava","commits":[{"id":"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0","date":1383367127,"type":1,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(Iterable[Term],ReadersAndUpdates,SegmentReader).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","sourceNew":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndUpdates rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(termsEnum);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes())) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, DocsEnum.FLAG_NONE);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(termsEnum);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes())) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, DocsEnum.FLAG_NONE);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8028ab7a24273833d53d35eb160dba5b57283cf5","date":1416767720,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(Iterable[Term],ReadersAndUpdates,SegmentReader).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(Iterable[Term],ReadersAndUpdates,SegmentReader).mjava","sourceNew":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndUpdates rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(termsEnum);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes())) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, DocsEnum.FLAG_NONE);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndUpdates rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(termsEnum);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes())) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, DocsEnum.FLAG_NONE);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":["69a6d2d525aeab53c867ed26934185e5bb627d0e"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5faf65b6692f15cca0f87bf8666c87899afc619f","date":1420468108,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(Iterable[Term],ReadersAndUpdates,SegmentReader).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(Iterable[Term],ReadersAndUpdates,SegmentReader).mjava","sourceNew":"  // Delete by Term\n  private synchronized TermDeleteCounts applyTermDeletes(Iterable<Term> termsIter, ReadersAndUpdates rld, SegmentReader reader) throws IOException {\n    int delCount = 0;\n    long termVisitedCount = 0;\n    Fields fields = reader.fields();\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    long ns = System.nanoTime();\n\n    for (Term term : termsIter) {\n      termVisitedCount++;\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(termsEnum);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes())) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, DocsEnum.FLAG_NONE);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return new TermDeleteCounts(delCount, termVisitedCount);\n  }\n\n","sourceOld":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndUpdates rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(termsEnum);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes())) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, DocsEnum.FLAG_NONE);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2636148e70acf2722b6fc6179449e118bb1a0cea","date":1420480398,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(Iterable[Term],ReadersAndUpdates,SegmentReader).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(Iterable[Term],ReadersAndUpdates,SegmentReader).mjava","sourceNew":"  // Delete by Term\n  private synchronized TermDeleteCounts applyTermDeletes(Iterable<Term> termsIter, ReadersAndUpdates rld, SegmentReader reader) throws IOException {\n    int delCount = 0;\n    long termVisitedCount = 0;\n    Fields fields = reader.fields();\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docsEnum = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    long ns = System.nanoTime();\n\n    for (Term term : termsIter) {\n      termVisitedCount++;\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(termsEnum);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        // no terms in this field\n        continue;\n      }\n\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes())) {\n        // we don't need term frequencies for this\n        docsEnum = termsEnum.docs(rld.getLiveDocs(), docsEnum, DocsEnum.FLAG_NONE);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        assert docsEnum != null;\n\n        while (true) {\n          final int docID = docsEnum.nextDoc();\n          //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }   \n          if (!any) {\n            rld.initWritableLiveDocs();\n            any = true;\n          }\n          // NOTE: there is no limit check on the docID\n          // when deleting by Term (unlike by Query)\n          // because on flush we apply all Term deletes to\n          // each segment.  So all Term deleting here is\n          // against prior segments:\n          if (rld.delete(docID)) {\n            delCount++;\n          }\n        }\n      }\n    }\n\n    return new TermDeleteCounts(delCount, termVisitedCount);\n  }\n\n","sourceOld":"  // Delete by Term\n  private synchronized TermDeleteCounts applyTermDeletes(Iterable<Term> termsIter, ReadersAndUpdates rld, SegmentReader reader) throws IOException {\n    int delCount = 0;\n    long termVisitedCount = 0;\n    Fields fields = reader.fields();\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    long ns = System.nanoTime();\n\n    for (Term term : termsIter) {\n      termVisitedCount++;\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(termsEnum);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes())) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, DocsEnum.FLAG_NONE);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return new TermDeleteCounts(delCount, termVisitedCount);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e4c214a1f904dde76f5611b56d4081533055b3b","date":1421938451,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(Iterable[Term],ReadersAndUpdates,SegmentReader).mjava","sourceNew":null,"sourceOld":"  // Delete by Term\n  private synchronized TermDeleteCounts applyTermDeletes(Iterable<Term> termsIter, ReadersAndUpdates rld, SegmentReader reader) throws IOException {\n    int delCount = 0;\n    long termVisitedCount = 0;\n    Fields fields = reader.fields();\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docsEnum = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    long ns = System.nanoTime();\n\n    for (Term term : termsIter) {\n      termVisitedCount++;\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(termsEnum);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        // no terms in this field\n        continue;\n      }\n\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes())) {\n        // we don't need term frequencies for this\n        docsEnum = termsEnum.docs(rld.getLiveDocs(), docsEnum, DocsEnum.FLAG_NONE);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        assert docsEnum != null;\n\n        while (true) {\n          final int docID = docsEnum.nextDoc();\n          //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }   \n          if (!any) {\n            rld.initWritableLiveDocs();\n            any = true;\n          }\n          // NOTE: there is no limit check on the docID\n          // when deleting by Term (unlike by Query)\n          // because on flush we apply all Term deletes to\n          // each segment.  So all Term deleting here is\n          // against prior segments:\n          if (rld.delete(docID)) {\n            delCount++;\n          }\n        }\n      }\n    }\n\n    return new TermDeleteCounts(delCount, termVisitedCount);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"8028ab7a24273833d53d35eb160dba5b57283cf5":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"7e4c214a1f904dde76f5611b56d4081533055b3b":["2636148e70acf2722b6fc6179449e118bb1a0cea"],"5faf65b6692f15cca0f87bf8666c87899afc619f":["8028ab7a24273833d53d35eb160dba5b57283cf5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["7e4c214a1f904dde76f5611b56d4081533055b3b"],"2636148e70acf2722b6fc6179449e118bb1a0cea":["5faf65b6692f15cca0f87bf8666c87899afc619f"]},"commit2Childs":{"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["8028ab7a24273833d53d35eb160dba5b57283cf5"],"8028ab7a24273833d53d35eb160dba5b57283cf5":["5faf65b6692f15cca0f87bf8666c87899afc619f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"7e4c214a1f904dde76f5611b56d4081533055b3b":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5faf65b6692f15cca0f87bf8666c87899afc619f":["2636148e70acf2722b6fc6179449e118bb1a0cea"],"2636148e70acf2722b6fc6179449e118bb1a0cea":["7e4c214a1f904dde76f5611b56d4081533055b3b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}