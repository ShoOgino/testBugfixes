{"path":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#init().mjava","commits":[{"id":"714aa8d007eef87d7203cfc6e0fe4dab8dd8a497","date":1417181893,"type":1,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#init().mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector#init().mjava","sourceNew":"  //We initialize in reset() because we can see which attributes the consumer wants, particularly payloads\n  private void init() throws IOException {\n    if (vector.hasOffsets()) {\n      offsetAttribute = addAttribute(OffsetAttribute.class);\n    }\n    if (vector.hasPayloads() && hasAttribute(PayloadAttribute.class)) {\n      payloadAttribute = getAttribute(PayloadAttribute.class);\n    }\n\n    // Step 1: iterate termsEnum and create a token, placing into an array of tokens by position\n\n    TokenLL[] positionedTokens = initTokensArray();\n\n    int lastPosition = -1;\n\n    final TermsEnum termsEnum = vector.iterator(null);\n    BytesRef termBytesRef;\n    DocsAndPositionsEnum dpEnum = null;\n    //int sumFreq = 0;\n    while ((termBytesRef = termsEnum.next()) != null) {\n      //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n      // note: if term vectors supported seek by ord then we might just keep an int and seek by ord on-demand\n      final char[] termChars = new char[termBytesRef.length];\n      final int termCharsLen = UnicodeUtil.UTF8toUTF16(termBytesRef, termChars);\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      assert dpEnum != null; // presumably checked by TokenSources.hasPositions earlier\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      //sumFreq += freq;\n      for (int j = 0; j < freq; j++) {\n        int pos = dpEnum.nextPosition();\n        TokenLL token = new TokenLL();\n        token.termChars = termChars;\n        token.termCharsLen = termCharsLen;\n        if (offsetAttribute != null) {\n          token.startOffset = dpEnum.startOffset();\n          token.endOffset = dpEnum.endOffset();\n          if (pos == -1) {\n            pos = token.startOffset >> 3;//divide by 8\n          }\n        }\n\n        if (payloadAttribute != null) {\n          // Must make a deep copy of the returned payload,\n          // since D&PEnum API is allowed to re-use on every\n          // call:\n          final BytesRef payload = dpEnum.getPayload();\n          if (payload != null) {\n            token.payload = BytesRef.deepCopyOf(payload);//TODO share a ByteBlockPool & re-use BytesRef\n          }\n        }\n\n        //Add token to an array indexed by position\n        if (positionedTokens.length <= pos) {\n          //grow, but not 2x since we think our original length estimate is close\n          TokenLL[] newPositionedTokens = new TokenLL[(int)((pos + 1) * 1.5f)];\n          System.arraycopy(positionedTokens, 0, newPositionedTokens, 0, lastPosition + 1);\n          positionedTokens = newPositionedTokens;\n        }\n        positionedTokens[pos] = token.insertIntoSortedLinkedList(positionedTokens[pos]);\n\n        lastPosition = Math.max(lastPosition, pos);\n      }\n    }\n\n//    System.out.println(String.format(\n//        \"SumFreq: %5d Size: %4d SumFreq/size: %3.3f MaxPos: %4d MaxPos/SumFreq: %3.3f WastePct: %3.3f\",\n//        sumFreq, vector.size(), (sumFreq / (float)vector.size()), lastPosition, ((float)lastPosition)/sumFreq,\n//        (originalPositionEstimate/(lastPosition + 1.0f))));\n\n    // Step 2:  Link all Tokens into a linked-list and set position increments as we go\n\n    int prevTokenPos = -1;\n    TokenLL prevToken = null;\n    for (int pos = 0; pos <= lastPosition; pos++) {\n      TokenLL token = positionedTokens[pos];\n      if (token == null) {\n        continue;\n      }\n      //link\n      if (prevToken != null) {\n        assert prevToken.next == null;\n        prevToken.next = token; //concatenate linked-list\n      } else {\n        assert firstToken == null;\n        firstToken = token;\n      }\n      //set increments\n      if (vector.hasPositions()) {\n        token.positionIncrement = pos - prevTokenPos;\n        while (token.next != null) {\n          token = token.next;\n          token.positionIncrement = 0;\n        }\n      } else {\n        token.positionIncrement = 1;\n        while (token.next != null) {\n          prevToken = token;\n          token = token.next;\n          if (prevToken.startOffset == token.startOffset) {\n            token.positionIncrement = 0;\n          } else {\n            token.positionIncrement = 1;\n          }\n        }\n      }\n      prevTokenPos = pos;\n      prevToken = token;\n    }\n  }\n\n","sourceOld":"  //We initialize in reset() because we can see which attributes the consumer wants, particularly payloads\n  private void init() throws IOException {\n    if (vector.hasOffsets()) {\n      offsetAttribute = addAttribute(OffsetAttribute.class);\n    }\n    if (vector.hasPayloads() && hasAttribute(PayloadAttribute.class)) {\n      payloadAttribute = getAttribute(PayloadAttribute.class);\n    }\n\n    // Step 1: iterate termsEnum and create a token, placing into an array of tokens by position\n\n    TokenLL[] positionedTokens = initTokensArray();\n\n    int lastPosition = -1;\n\n    final TermsEnum termsEnum = vector.iterator(null);\n    BytesRef termBytesRef;\n    DocsAndPositionsEnum dpEnum = null;\n    //int sumFreq = 0;\n    while ((termBytesRef = termsEnum.next()) != null) {\n      //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n      // note: if term vectors supported seek by ord then we might just keep an int and seek by ord on-demand\n      final char[] termChars = new char[termBytesRef.length];\n      final int termCharsLen = UnicodeUtil.UTF8toUTF16(termBytesRef, termChars);\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      assert dpEnum != null; // presumably checked by TokenSources.hasPositions earlier\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      //sumFreq += freq;\n      for (int j = 0; j < freq; j++) {\n        int pos = dpEnum.nextPosition();\n        TokenLL token = new TokenLL();\n        token.termChars = termChars;\n        token.termCharsLen = termCharsLen;\n        if (offsetAttribute != null) {\n          token.startOffset = dpEnum.startOffset();\n          token.endOffset = dpEnum.endOffset();\n          if (pos == -1) {\n            pos = token.startOffset >> 3;//divide by 8\n          }\n        }\n\n        if (payloadAttribute != null) {\n          // Must make a deep copy of the returned payload,\n          // since D&PEnum API is allowed to re-use on every\n          // call:\n          final BytesRef payload = dpEnum.getPayload();\n          if (payload != null) {\n            token.payload = BytesRef.deepCopyOf(payload);//TODO share a ByteBlockPool & re-use BytesRef\n          }\n        }\n\n        //Add token to an array indexed by position\n        if (positionedTokens.length <= pos) {\n          //grow, but not 2x since we think our original length estimate is close\n          TokenLL[] newPositionedTokens = new TokenLL[(int)((pos + 1) * 1.5f)];\n          System.arraycopy(positionedTokens, 0, newPositionedTokens, 0, lastPosition + 1);\n          positionedTokens = newPositionedTokens;\n        }\n        positionedTokens[pos] = token.insertIntoSortedLinkedList(positionedTokens[pos]);\n\n        lastPosition = Math.max(lastPosition, pos);\n      }\n    }\n\n//    System.out.println(String.format(\n//        \"SumFreq: %5d Size: %4d SumFreq/size: %3.3f MaxPos: %4d MaxPos/SumFreq: %3.3f WastePct: %3.3f\",\n//        sumFreq, vector.size(), (sumFreq / (float)vector.size()), lastPosition, ((float)lastPosition)/sumFreq,\n//        (originalPositionEstimate/(lastPosition + 1.0f))));\n\n    // Step 2:  Link all Tokens into a linked-list and set position increments as we go\n\n    int prevTokenPos = -1;\n    TokenLL prevToken = null;\n    for (int pos = 0; pos <= lastPosition; pos++) {\n      TokenLL token = positionedTokens[pos];\n      if (token == null) {\n        continue;\n      }\n      //link\n      if (prevToken != null) {\n        assert prevToken.next == null;\n        prevToken.next = token; //concatenate linked-list\n      } else {\n        assert firstToken == null;\n        firstToken = token;\n      }\n      //set increments\n      if (vector.hasPositions()) {\n        token.positionIncrement = pos - prevTokenPos;\n        while (token.next != null) {\n          token = token.next;\n          token.positionIncrement = 0;\n        }\n      } else {\n        token.positionIncrement = 1;\n        while (token.next != null) {\n          prevToken = token;\n          token = token.next;\n          if (prevToken.startOffset == token.startOffset) {\n            token.positionIncrement = 0;\n          } else {\n            token.positionIncrement = 1;\n          }\n        }\n      }\n      prevTokenPos = pos;\n      prevToken = token;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4dc0de75e60c6f90667efd23879e863f5b1ca46e","date":1419308055,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#init().mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#init().mjava","sourceNew":"  //We delay initialization because we can see which attributes the consumer wants, particularly payloads\n  private void init() throws IOException {\n    assert !initialized;\n    if (vector.hasOffsets()) {\n      offsetAttribute = addAttribute(OffsetAttribute.class);\n    }\n    if (vector.hasPayloads() && hasAttribute(PayloadAttribute.class)) {\n      payloadAttribute = getAttribute(PayloadAttribute.class);\n      payloadsBytesRefArray = new BytesRefArray(Counter.newCounter());\n      spareBytesRefBuilder = new BytesRefBuilder();\n    }\n\n    // Step 1: iterate termsEnum and create a token, placing into an array of tokens by position\n\n    TokenLL[] positionedTokens = initTokensArray();\n\n    int lastPosition = -1;\n\n    final TermsEnum termsEnum = vector.iterator(null);\n    BytesRef termBytesRef;\n    DocsAndPositionsEnum dpEnum = null;\n    //int sumFreq = 0;\n    while ((termBytesRef = termsEnum.next()) != null) {\n      //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n      // note: if term vectors supported seek by ord then we might just keep an int and seek by ord on-demand\n      final char[] termChars = new char[termBytesRef.length];\n      final int termCharsLen = UnicodeUtil.UTF8toUTF16(termBytesRef, termChars);\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      assert dpEnum != null; // presumably checked by TokenSources.hasPositions earlier\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      //sumFreq += freq;\n      for (int j = 0; j < freq; j++) {\n        int pos = dpEnum.nextPosition();\n        TokenLL token = new TokenLL();\n        token.termChars = termChars;\n        token.termCharsLen = termCharsLen;\n        if (offsetAttribute != null) {\n          token.startOffset = dpEnum.startOffset();\n          token.endOffset = dpEnum.endOffset();\n          if (pos == -1) {\n            pos = token.startOffset >> 3;//divide by 8\n          }\n        }\n\n        if (payloadAttribute != null) {\n          final BytesRef payload = dpEnum.getPayload();\n          token.payloadIndex = payload == null ? -1 : payloadsBytesRefArray.append(payload);\n        }\n\n        //Add token to an array indexed by position\n        if (positionedTokens.length <= pos) {\n          //grow, but not 2x since we think our original length estimate is close\n          TokenLL[] newPositionedTokens = new TokenLL[(int)((pos + 1) * 1.5f)];\n          System.arraycopy(positionedTokens, 0, newPositionedTokens, 0, lastPosition + 1);\n          positionedTokens = newPositionedTokens;\n        }\n        positionedTokens[pos] = token.insertIntoSortedLinkedList(positionedTokens[pos]);\n\n        lastPosition = Math.max(lastPosition, pos);\n      }\n    }\n\n//    System.out.println(String.format(\n//        \"SumFreq: %5d Size: %4d SumFreq/size: %3.3f MaxPos: %4d MaxPos/SumFreq: %3.3f WastePct: %3.3f\",\n//        sumFreq, vector.size(), (sumFreq / (float)vector.size()), lastPosition, ((float)lastPosition)/sumFreq,\n//        (originalPositionEstimate/(lastPosition + 1.0f))));\n\n    // Step 2:  Link all Tokens into a linked-list and set position increments as we go\n\n    int prevTokenPos = -1;\n    TokenLL prevToken = null;\n    for (int pos = 0; pos <= lastPosition; pos++) {\n      TokenLL token = positionedTokens[pos];\n      if (token == null) {\n        continue;\n      }\n      //link\n      if (prevToken != null) {\n        assert prevToken.next == null;\n        prevToken.next = token; //concatenate linked-list\n      } else {\n        assert firstToken == null;\n        firstToken = token;\n      }\n      //set increments\n      if (vector.hasPositions()) {\n        token.positionIncrement = pos - prevTokenPos;\n        while (token.next != null) {\n          token = token.next;\n          token.positionIncrement = 0;\n        }\n      } else {\n        token.positionIncrement = 1;\n        while (token.next != null) {\n          prevToken = token;\n          token = token.next;\n          if (prevToken.startOffset == token.startOffset) {\n            token.positionIncrement = 0;\n          } else {\n            token.positionIncrement = 1;\n          }\n        }\n      }\n      prevTokenPos = pos;\n      prevToken = token;\n    }\n\n    initialized = true;\n  }\n\n","sourceOld":"  //We initialize in reset() because we can see which attributes the consumer wants, particularly payloads\n  private void init() throws IOException {\n    if (vector.hasOffsets()) {\n      offsetAttribute = addAttribute(OffsetAttribute.class);\n    }\n    if (vector.hasPayloads() && hasAttribute(PayloadAttribute.class)) {\n      payloadAttribute = getAttribute(PayloadAttribute.class);\n    }\n\n    // Step 1: iterate termsEnum and create a token, placing into an array of tokens by position\n\n    TokenLL[] positionedTokens = initTokensArray();\n\n    int lastPosition = -1;\n\n    final TermsEnum termsEnum = vector.iterator(null);\n    BytesRef termBytesRef;\n    DocsAndPositionsEnum dpEnum = null;\n    //int sumFreq = 0;\n    while ((termBytesRef = termsEnum.next()) != null) {\n      //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n      // note: if term vectors supported seek by ord then we might just keep an int and seek by ord on-demand\n      final char[] termChars = new char[termBytesRef.length];\n      final int termCharsLen = UnicodeUtil.UTF8toUTF16(termBytesRef, termChars);\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      assert dpEnum != null; // presumably checked by TokenSources.hasPositions earlier\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      //sumFreq += freq;\n      for (int j = 0; j < freq; j++) {\n        int pos = dpEnum.nextPosition();\n        TokenLL token = new TokenLL();\n        token.termChars = termChars;\n        token.termCharsLen = termCharsLen;\n        if (offsetAttribute != null) {\n          token.startOffset = dpEnum.startOffset();\n          token.endOffset = dpEnum.endOffset();\n          if (pos == -1) {\n            pos = token.startOffset >> 3;//divide by 8\n          }\n        }\n\n        if (payloadAttribute != null) {\n          // Must make a deep copy of the returned payload,\n          // since D&PEnum API is allowed to re-use on every\n          // call:\n          final BytesRef payload = dpEnum.getPayload();\n          if (payload != null) {\n            token.payload = BytesRef.deepCopyOf(payload);//TODO share a ByteBlockPool & re-use BytesRef\n          }\n        }\n\n        //Add token to an array indexed by position\n        if (positionedTokens.length <= pos) {\n          //grow, but not 2x since we think our original length estimate is close\n          TokenLL[] newPositionedTokens = new TokenLL[(int)((pos + 1) * 1.5f)];\n          System.arraycopy(positionedTokens, 0, newPositionedTokens, 0, lastPosition + 1);\n          positionedTokens = newPositionedTokens;\n        }\n        positionedTokens[pos] = token.insertIntoSortedLinkedList(positionedTokens[pos]);\n\n        lastPosition = Math.max(lastPosition, pos);\n      }\n    }\n\n//    System.out.println(String.format(\n//        \"SumFreq: %5d Size: %4d SumFreq/size: %3.3f MaxPos: %4d MaxPos/SumFreq: %3.3f WastePct: %3.3f\",\n//        sumFreq, vector.size(), (sumFreq / (float)vector.size()), lastPosition, ((float)lastPosition)/sumFreq,\n//        (originalPositionEstimate/(lastPosition + 1.0f))));\n\n    // Step 2:  Link all Tokens into a linked-list and set position increments as we go\n\n    int prevTokenPos = -1;\n    TokenLL prevToken = null;\n    for (int pos = 0; pos <= lastPosition; pos++) {\n      TokenLL token = positionedTokens[pos];\n      if (token == null) {\n        continue;\n      }\n      //link\n      if (prevToken != null) {\n        assert prevToken.next == null;\n        prevToken.next = token; //concatenate linked-list\n      } else {\n        assert firstToken == null;\n        firstToken = token;\n      }\n      //set increments\n      if (vector.hasPositions()) {\n        token.positionIncrement = pos - prevTokenPos;\n        while (token.next != null) {\n          token = token.next;\n          token.positionIncrement = 0;\n        }\n      } else {\n        token.positionIncrement = 1;\n        while (token.next != null) {\n          prevToken = token;\n          token = token.next;\n          if (prevToken.startOffset == token.startOffset) {\n            token.positionIncrement = 0;\n          } else {\n            token.positionIncrement = 1;\n          }\n        }\n      }\n      prevTokenPos = pos;\n      prevToken = token;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#init().mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#init().mjava","sourceNew":"  //We delay initialization because we can see which attributes the consumer wants, particularly payloads\n  private void init() throws IOException {\n    assert !initialized;\n    if (vector.hasOffsets()) {\n      offsetAttribute = addAttribute(OffsetAttribute.class);\n    }\n    if (vector.hasPayloads() && hasAttribute(PayloadAttribute.class)) {\n      payloadAttribute = getAttribute(PayloadAttribute.class);\n      payloadsBytesRefArray = new BytesRefArray(Counter.newCounter());\n      spareBytesRefBuilder = new BytesRefBuilder();\n    }\n\n    // Step 1: iterate termsEnum and create a token, placing into an array of tokens by position\n\n    TokenLL[] positionedTokens = initTokensArray();\n\n    int lastPosition = -1;\n\n    final TermsEnum termsEnum = vector.iterator(null);\n    BytesRef termBytesRef;\n    PostingsEnum dpEnum = null;\n    //int sumFreq = 0;\n    while ((termBytesRef = termsEnum.next()) != null) {\n      //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n      // note: if term vectors supported seek by ord then we might just keep an int and seek by ord on-demand\n      final char[] termChars = new char[termBytesRef.length];\n      final int termCharsLen = UnicodeUtil.UTF8toUTF16(termBytesRef, termChars);\n\n      dpEnum = termsEnum.postings(null, dpEnum, PostingsEnum.FLAG_POSITIONS);\n      assert dpEnum != null; // presumably checked by TokenSources.hasPositions earlier\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      //sumFreq += freq;\n      for (int j = 0; j < freq; j++) {\n        int pos = dpEnum.nextPosition();\n        TokenLL token = new TokenLL();\n        token.termChars = termChars;\n        token.termCharsLen = termCharsLen;\n        if (offsetAttribute != null) {\n          token.startOffset = dpEnum.startOffset();\n          token.endOffset = dpEnum.endOffset();\n          if (pos == -1) {\n            pos = token.startOffset >> 3;//divide by 8\n          }\n        }\n\n        if (payloadAttribute != null) {\n          final BytesRef payload = dpEnum.getPayload();\n          token.payloadIndex = payload == null ? -1 : payloadsBytesRefArray.append(payload);\n        }\n\n        //Add token to an array indexed by position\n        if (positionedTokens.length <= pos) {\n          //grow, but not 2x since we think our original length estimate is close\n          TokenLL[] newPositionedTokens = new TokenLL[(int)((pos + 1) * 1.5f)];\n          System.arraycopy(positionedTokens, 0, newPositionedTokens, 0, lastPosition + 1);\n          positionedTokens = newPositionedTokens;\n        }\n        positionedTokens[pos] = token.insertIntoSortedLinkedList(positionedTokens[pos]);\n\n        lastPosition = Math.max(lastPosition, pos);\n      }\n    }\n\n//    System.out.println(String.format(\n//        \"SumFreq: %5d Size: %4d SumFreq/size: %3.3f MaxPos: %4d MaxPos/SumFreq: %3.3f WastePct: %3.3f\",\n//        sumFreq, vector.size(), (sumFreq / (float)vector.size()), lastPosition, ((float)lastPosition)/sumFreq,\n//        (originalPositionEstimate/(lastPosition + 1.0f))));\n\n    // Step 2:  Link all Tokens into a linked-list and set position increments as we go\n\n    int prevTokenPos = -1;\n    TokenLL prevToken = null;\n    for (int pos = 0; pos <= lastPosition; pos++) {\n      TokenLL token = positionedTokens[pos];\n      if (token == null) {\n        continue;\n      }\n      //link\n      if (prevToken != null) {\n        assert prevToken.next == null;\n        prevToken.next = token; //concatenate linked-list\n      } else {\n        assert firstToken == null;\n        firstToken = token;\n      }\n      //set increments\n      if (vector.hasPositions()) {\n        token.positionIncrement = pos - prevTokenPos;\n        while (token.next != null) {\n          token = token.next;\n          token.positionIncrement = 0;\n        }\n      } else {\n        token.positionIncrement = 1;\n        while (token.next != null) {\n          prevToken = token;\n          token = token.next;\n          if (prevToken.startOffset == token.startOffset) {\n            token.positionIncrement = 0;\n          } else {\n            token.positionIncrement = 1;\n          }\n        }\n      }\n      prevTokenPos = pos;\n      prevToken = token;\n    }\n\n    initialized = true;\n  }\n\n","sourceOld":"  //We delay initialization because we can see which attributes the consumer wants, particularly payloads\n  private void init() throws IOException {\n    assert !initialized;\n    if (vector.hasOffsets()) {\n      offsetAttribute = addAttribute(OffsetAttribute.class);\n    }\n    if (vector.hasPayloads() && hasAttribute(PayloadAttribute.class)) {\n      payloadAttribute = getAttribute(PayloadAttribute.class);\n      payloadsBytesRefArray = new BytesRefArray(Counter.newCounter());\n      spareBytesRefBuilder = new BytesRefBuilder();\n    }\n\n    // Step 1: iterate termsEnum and create a token, placing into an array of tokens by position\n\n    TokenLL[] positionedTokens = initTokensArray();\n\n    int lastPosition = -1;\n\n    final TermsEnum termsEnum = vector.iterator(null);\n    BytesRef termBytesRef;\n    DocsAndPositionsEnum dpEnum = null;\n    //int sumFreq = 0;\n    while ((termBytesRef = termsEnum.next()) != null) {\n      //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n      // note: if term vectors supported seek by ord then we might just keep an int and seek by ord on-demand\n      final char[] termChars = new char[termBytesRef.length];\n      final int termCharsLen = UnicodeUtil.UTF8toUTF16(termBytesRef, termChars);\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      assert dpEnum != null; // presumably checked by TokenSources.hasPositions earlier\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      //sumFreq += freq;\n      for (int j = 0; j < freq; j++) {\n        int pos = dpEnum.nextPosition();\n        TokenLL token = new TokenLL();\n        token.termChars = termChars;\n        token.termCharsLen = termCharsLen;\n        if (offsetAttribute != null) {\n          token.startOffset = dpEnum.startOffset();\n          token.endOffset = dpEnum.endOffset();\n          if (pos == -1) {\n            pos = token.startOffset >> 3;//divide by 8\n          }\n        }\n\n        if (payloadAttribute != null) {\n          final BytesRef payload = dpEnum.getPayload();\n          token.payloadIndex = payload == null ? -1 : payloadsBytesRefArray.append(payload);\n        }\n\n        //Add token to an array indexed by position\n        if (positionedTokens.length <= pos) {\n          //grow, but not 2x since we think our original length estimate is close\n          TokenLL[] newPositionedTokens = new TokenLL[(int)((pos + 1) * 1.5f)];\n          System.arraycopy(positionedTokens, 0, newPositionedTokens, 0, lastPosition + 1);\n          positionedTokens = newPositionedTokens;\n        }\n        positionedTokens[pos] = token.insertIntoSortedLinkedList(positionedTokens[pos]);\n\n        lastPosition = Math.max(lastPosition, pos);\n      }\n    }\n\n//    System.out.println(String.format(\n//        \"SumFreq: %5d Size: %4d SumFreq/size: %3.3f MaxPos: %4d MaxPos/SumFreq: %3.3f WastePct: %3.3f\",\n//        sumFreq, vector.size(), (sumFreq / (float)vector.size()), lastPosition, ((float)lastPosition)/sumFreq,\n//        (originalPositionEstimate/(lastPosition + 1.0f))));\n\n    // Step 2:  Link all Tokens into a linked-list and set position increments as we go\n\n    int prevTokenPos = -1;\n    TokenLL prevToken = null;\n    for (int pos = 0; pos <= lastPosition; pos++) {\n      TokenLL token = positionedTokens[pos];\n      if (token == null) {\n        continue;\n      }\n      //link\n      if (prevToken != null) {\n        assert prevToken.next == null;\n        prevToken.next = token; //concatenate linked-list\n      } else {\n        assert firstToken == null;\n        firstToken = token;\n      }\n      //set increments\n      if (vector.hasPositions()) {\n        token.positionIncrement = pos - prevTokenPos;\n        while (token.next != null) {\n          token = token.next;\n          token.positionIncrement = 0;\n        }\n      } else {\n        token.positionIncrement = 1;\n        while (token.next != null) {\n          prevToken = token;\n          token = token.next;\n          if (prevToken.startOffset == token.startOffset) {\n            token.positionIncrement = 0;\n          } else {\n            token.positionIncrement = 1;\n          }\n        }\n      }\n      prevTokenPos = pos;\n      prevToken = token;\n    }\n\n    initialized = true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e73063b92d958076ef4ae8beb5f493e8ccdcecb4","date":1424177215,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#init().mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#init().mjava","sourceNew":"  //We delay initialization because we can see which attributes the consumer wants, particularly payloads\n  private void init() throws IOException {\n    assert !initialized;\n    if (vector.hasOffsets()) {\n      offsetAttribute = addAttribute(OffsetAttribute.class);\n    }\n    if (vector.hasPayloads() && hasAttribute(PayloadAttribute.class)) {\n      payloadAttribute = getAttribute(PayloadAttribute.class);\n      payloadsBytesRefArray = new BytesRefArray(Counter.newCounter());\n      spareBytesRefBuilder = new BytesRefBuilder();\n    }\n\n    // Step 1: iterate termsEnum and create a token, placing into an array of tokens by position\n\n    TokenLL[] positionedTokens = initTokensArray();\n\n    int lastPosition = -1;\n\n    final TermsEnum termsEnum = vector.iterator(null);\n    BytesRef termBytesRef;\n    PostingsEnum dpEnum = null;\n    //int sumFreq = 0;\n    while ((termBytesRef = termsEnum.next()) != null) {\n      //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n      // note: if term vectors supported seek by ord then we might just keep an int and seek by ord on-demand\n      final char[] termChars = new char[termBytesRef.length];\n      final int termCharsLen = UnicodeUtil.UTF8toUTF16(termBytesRef, termChars);\n\n      dpEnum = termsEnum.postings(null, dpEnum, PostingsEnum.POSITIONS);\n      assert dpEnum != null; // presumably checked by TokenSources.hasPositions earlier\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      //sumFreq += freq;\n      for (int j = 0; j < freq; j++) {\n        int pos = dpEnum.nextPosition();\n        TokenLL token = new TokenLL();\n        token.termChars = termChars;\n        token.termCharsLen = termCharsLen;\n        if (offsetAttribute != null) {\n          token.startOffset = dpEnum.startOffset();\n          token.endOffset = dpEnum.endOffset();\n          if (pos == -1) {\n            pos = token.startOffset >> 3;//divide by 8\n          }\n        }\n\n        if (payloadAttribute != null) {\n          final BytesRef payload = dpEnum.getPayload();\n          token.payloadIndex = payload == null ? -1 : payloadsBytesRefArray.append(payload);\n        }\n\n        //Add token to an array indexed by position\n        if (positionedTokens.length <= pos) {\n          //grow, but not 2x since we think our original length estimate is close\n          TokenLL[] newPositionedTokens = new TokenLL[(int)((pos + 1) * 1.5f)];\n          System.arraycopy(positionedTokens, 0, newPositionedTokens, 0, lastPosition + 1);\n          positionedTokens = newPositionedTokens;\n        }\n        positionedTokens[pos] = token.insertIntoSortedLinkedList(positionedTokens[pos]);\n\n        lastPosition = Math.max(lastPosition, pos);\n      }\n    }\n\n//    System.out.println(String.format(\n//        \"SumFreq: %5d Size: %4d SumFreq/size: %3.3f MaxPos: %4d MaxPos/SumFreq: %3.3f WastePct: %3.3f\",\n//        sumFreq, vector.size(), (sumFreq / (float)vector.size()), lastPosition, ((float)lastPosition)/sumFreq,\n//        (originalPositionEstimate/(lastPosition + 1.0f))));\n\n    // Step 2:  Link all Tokens into a linked-list and set position increments as we go\n\n    int prevTokenPos = -1;\n    TokenLL prevToken = null;\n    for (int pos = 0; pos <= lastPosition; pos++) {\n      TokenLL token = positionedTokens[pos];\n      if (token == null) {\n        continue;\n      }\n      //link\n      if (prevToken != null) {\n        assert prevToken.next == null;\n        prevToken.next = token; //concatenate linked-list\n      } else {\n        assert firstToken == null;\n        firstToken = token;\n      }\n      //set increments\n      if (vector.hasPositions()) {\n        token.positionIncrement = pos - prevTokenPos;\n        while (token.next != null) {\n          token = token.next;\n          token.positionIncrement = 0;\n        }\n      } else {\n        token.positionIncrement = 1;\n        while (token.next != null) {\n          prevToken = token;\n          token = token.next;\n          if (prevToken.startOffset == token.startOffset) {\n            token.positionIncrement = 0;\n          } else {\n            token.positionIncrement = 1;\n          }\n        }\n      }\n      prevTokenPos = pos;\n      prevToken = token;\n    }\n\n    initialized = true;\n  }\n\n","sourceOld":"  //We delay initialization because we can see which attributes the consumer wants, particularly payloads\n  private void init() throws IOException {\n    assert !initialized;\n    if (vector.hasOffsets()) {\n      offsetAttribute = addAttribute(OffsetAttribute.class);\n    }\n    if (vector.hasPayloads() && hasAttribute(PayloadAttribute.class)) {\n      payloadAttribute = getAttribute(PayloadAttribute.class);\n      payloadsBytesRefArray = new BytesRefArray(Counter.newCounter());\n      spareBytesRefBuilder = new BytesRefBuilder();\n    }\n\n    // Step 1: iterate termsEnum and create a token, placing into an array of tokens by position\n\n    TokenLL[] positionedTokens = initTokensArray();\n\n    int lastPosition = -1;\n\n    final TermsEnum termsEnum = vector.iterator(null);\n    BytesRef termBytesRef;\n    PostingsEnum dpEnum = null;\n    //int sumFreq = 0;\n    while ((termBytesRef = termsEnum.next()) != null) {\n      //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n      // note: if term vectors supported seek by ord then we might just keep an int and seek by ord on-demand\n      final char[] termChars = new char[termBytesRef.length];\n      final int termCharsLen = UnicodeUtil.UTF8toUTF16(termBytesRef, termChars);\n\n      dpEnum = termsEnum.postings(null, dpEnum, PostingsEnum.FLAG_POSITIONS);\n      assert dpEnum != null; // presumably checked by TokenSources.hasPositions earlier\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      //sumFreq += freq;\n      for (int j = 0; j < freq; j++) {\n        int pos = dpEnum.nextPosition();\n        TokenLL token = new TokenLL();\n        token.termChars = termChars;\n        token.termCharsLen = termCharsLen;\n        if (offsetAttribute != null) {\n          token.startOffset = dpEnum.startOffset();\n          token.endOffset = dpEnum.endOffset();\n          if (pos == -1) {\n            pos = token.startOffset >> 3;//divide by 8\n          }\n        }\n\n        if (payloadAttribute != null) {\n          final BytesRef payload = dpEnum.getPayload();\n          token.payloadIndex = payload == null ? -1 : payloadsBytesRefArray.append(payload);\n        }\n\n        //Add token to an array indexed by position\n        if (positionedTokens.length <= pos) {\n          //grow, but not 2x since we think our original length estimate is close\n          TokenLL[] newPositionedTokens = new TokenLL[(int)((pos + 1) * 1.5f)];\n          System.arraycopy(positionedTokens, 0, newPositionedTokens, 0, lastPosition + 1);\n          positionedTokens = newPositionedTokens;\n        }\n        positionedTokens[pos] = token.insertIntoSortedLinkedList(positionedTokens[pos]);\n\n        lastPosition = Math.max(lastPosition, pos);\n      }\n    }\n\n//    System.out.println(String.format(\n//        \"SumFreq: %5d Size: %4d SumFreq/size: %3.3f MaxPos: %4d MaxPos/SumFreq: %3.3f WastePct: %3.3f\",\n//        sumFreq, vector.size(), (sumFreq / (float)vector.size()), lastPosition, ((float)lastPosition)/sumFreq,\n//        (originalPositionEstimate/(lastPosition + 1.0f))));\n\n    // Step 2:  Link all Tokens into a linked-list and set position increments as we go\n\n    int prevTokenPos = -1;\n    TokenLL prevToken = null;\n    for (int pos = 0; pos <= lastPosition; pos++) {\n      TokenLL token = positionedTokens[pos];\n      if (token == null) {\n        continue;\n      }\n      //link\n      if (prevToken != null) {\n        assert prevToken.next == null;\n        prevToken.next = token; //concatenate linked-list\n      } else {\n        assert firstToken == null;\n        firstToken = token;\n      }\n      //set increments\n      if (vector.hasPositions()) {\n        token.positionIncrement = pos - prevTokenPos;\n        while (token.next != null) {\n          token = token.next;\n          token.positionIncrement = 0;\n        }\n      } else {\n        token.positionIncrement = 1;\n        while (token.next != null) {\n          prevToken = token;\n          token = token.next;\n          if (prevToken.startOffset == token.startOffset) {\n            token.positionIncrement = 0;\n          } else {\n            token.positionIncrement = 1;\n          }\n        }\n      }\n      prevTokenPos = pos;\n      prevToken = token;\n    }\n\n    initialized = true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","date":1428522487,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#init().mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#init().mjava","sourceNew":"  //We delay initialization because we can see which attributes the consumer wants, particularly payloads\n  private void init() throws IOException {\n    assert !initialized;\n    if (vector.hasOffsets()) {\n      offsetAttribute = addAttribute(OffsetAttribute.class);\n    }\n    if (vector.hasPayloads() && hasAttribute(PayloadAttribute.class)) {\n      payloadAttribute = getAttribute(PayloadAttribute.class);\n      payloadsBytesRefArray = new BytesRefArray(Counter.newCounter());\n      spareBytesRefBuilder = new BytesRefBuilder();\n    }\n\n    // Step 1: iterate termsEnum and create a token, placing into an array of tokens by position\n\n    TokenLL[] positionedTokens = initTokensArray();\n\n    int lastPosition = -1;\n\n    final TermsEnum termsEnum = vector.iterator();\n    BytesRef termBytesRef;\n    PostingsEnum dpEnum = null;\n    //int sumFreq = 0;\n    while ((termBytesRef = termsEnum.next()) != null) {\n      //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n      // note: if term vectors supported seek by ord then we might just keep an int and seek by ord on-demand\n      final char[] termChars = new char[termBytesRef.length];\n      final int termCharsLen = UnicodeUtil.UTF8toUTF16(termBytesRef, termChars);\n\n      dpEnum = termsEnum.postings(null, dpEnum, PostingsEnum.POSITIONS);\n      assert dpEnum != null; // presumably checked by TokenSources.hasPositions earlier\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      //sumFreq += freq;\n      for (int j = 0; j < freq; j++) {\n        int pos = dpEnum.nextPosition();\n        TokenLL token = new TokenLL();\n        token.termChars = termChars;\n        token.termCharsLen = termCharsLen;\n        if (offsetAttribute != null) {\n          token.startOffset = dpEnum.startOffset();\n          token.endOffset = dpEnum.endOffset();\n          if (pos == -1) {\n            pos = token.startOffset >> 3;//divide by 8\n          }\n        }\n\n        if (payloadAttribute != null) {\n          final BytesRef payload = dpEnum.getPayload();\n          token.payloadIndex = payload == null ? -1 : payloadsBytesRefArray.append(payload);\n        }\n\n        //Add token to an array indexed by position\n        if (positionedTokens.length <= pos) {\n          //grow, but not 2x since we think our original length estimate is close\n          TokenLL[] newPositionedTokens = new TokenLL[(int)((pos + 1) * 1.5f)];\n          System.arraycopy(positionedTokens, 0, newPositionedTokens, 0, lastPosition + 1);\n          positionedTokens = newPositionedTokens;\n        }\n        positionedTokens[pos] = token.insertIntoSortedLinkedList(positionedTokens[pos]);\n\n        lastPosition = Math.max(lastPosition, pos);\n      }\n    }\n\n//    System.out.println(String.format(\n//        \"SumFreq: %5d Size: %4d SumFreq/size: %3.3f MaxPos: %4d MaxPos/SumFreq: %3.3f WastePct: %3.3f\",\n//        sumFreq, vector.size(), (sumFreq / (float)vector.size()), lastPosition, ((float)lastPosition)/sumFreq,\n//        (originalPositionEstimate/(lastPosition + 1.0f))));\n\n    // Step 2:  Link all Tokens into a linked-list and set position increments as we go\n\n    int prevTokenPos = -1;\n    TokenLL prevToken = null;\n    for (int pos = 0; pos <= lastPosition; pos++) {\n      TokenLL token = positionedTokens[pos];\n      if (token == null) {\n        continue;\n      }\n      //link\n      if (prevToken != null) {\n        assert prevToken.next == null;\n        prevToken.next = token; //concatenate linked-list\n      } else {\n        assert firstToken == null;\n        firstToken = token;\n      }\n      //set increments\n      if (vector.hasPositions()) {\n        token.positionIncrement = pos - prevTokenPos;\n        while (token.next != null) {\n          token = token.next;\n          token.positionIncrement = 0;\n        }\n      } else {\n        token.positionIncrement = 1;\n        while (token.next != null) {\n          prevToken = token;\n          token = token.next;\n          if (prevToken.startOffset == token.startOffset) {\n            token.positionIncrement = 0;\n          } else {\n            token.positionIncrement = 1;\n          }\n        }\n      }\n      prevTokenPos = pos;\n      prevToken = token;\n    }\n\n    initialized = true;\n  }\n\n","sourceOld":"  //We delay initialization because we can see which attributes the consumer wants, particularly payloads\n  private void init() throws IOException {\n    assert !initialized;\n    if (vector.hasOffsets()) {\n      offsetAttribute = addAttribute(OffsetAttribute.class);\n    }\n    if (vector.hasPayloads() && hasAttribute(PayloadAttribute.class)) {\n      payloadAttribute = getAttribute(PayloadAttribute.class);\n      payloadsBytesRefArray = new BytesRefArray(Counter.newCounter());\n      spareBytesRefBuilder = new BytesRefBuilder();\n    }\n\n    // Step 1: iterate termsEnum and create a token, placing into an array of tokens by position\n\n    TokenLL[] positionedTokens = initTokensArray();\n\n    int lastPosition = -1;\n\n    final TermsEnum termsEnum = vector.iterator(null);\n    BytesRef termBytesRef;\n    PostingsEnum dpEnum = null;\n    //int sumFreq = 0;\n    while ((termBytesRef = termsEnum.next()) != null) {\n      //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n      // note: if term vectors supported seek by ord then we might just keep an int and seek by ord on-demand\n      final char[] termChars = new char[termBytesRef.length];\n      final int termCharsLen = UnicodeUtil.UTF8toUTF16(termBytesRef, termChars);\n\n      dpEnum = termsEnum.postings(null, dpEnum, PostingsEnum.POSITIONS);\n      assert dpEnum != null; // presumably checked by TokenSources.hasPositions earlier\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      //sumFreq += freq;\n      for (int j = 0; j < freq; j++) {\n        int pos = dpEnum.nextPosition();\n        TokenLL token = new TokenLL();\n        token.termChars = termChars;\n        token.termCharsLen = termCharsLen;\n        if (offsetAttribute != null) {\n          token.startOffset = dpEnum.startOffset();\n          token.endOffset = dpEnum.endOffset();\n          if (pos == -1) {\n            pos = token.startOffset >> 3;//divide by 8\n          }\n        }\n\n        if (payloadAttribute != null) {\n          final BytesRef payload = dpEnum.getPayload();\n          token.payloadIndex = payload == null ? -1 : payloadsBytesRefArray.append(payload);\n        }\n\n        //Add token to an array indexed by position\n        if (positionedTokens.length <= pos) {\n          //grow, but not 2x since we think our original length estimate is close\n          TokenLL[] newPositionedTokens = new TokenLL[(int)((pos + 1) * 1.5f)];\n          System.arraycopy(positionedTokens, 0, newPositionedTokens, 0, lastPosition + 1);\n          positionedTokens = newPositionedTokens;\n        }\n        positionedTokens[pos] = token.insertIntoSortedLinkedList(positionedTokens[pos]);\n\n        lastPosition = Math.max(lastPosition, pos);\n      }\n    }\n\n//    System.out.println(String.format(\n//        \"SumFreq: %5d Size: %4d SumFreq/size: %3.3f MaxPos: %4d MaxPos/SumFreq: %3.3f WastePct: %3.3f\",\n//        sumFreq, vector.size(), (sumFreq / (float)vector.size()), lastPosition, ((float)lastPosition)/sumFreq,\n//        (originalPositionEstimate/(lastPosition + 1.0f))));\n\n    // Step 2:  Link all Tokens into a linked-list and set position increments as we go\n\n    int prevTokenPos = -1;\n    TokenLL prevToken = null;\n    for (int pos = 0; pos <= lastPosition; pos++) {\n      TokenLL token = positionedTokens[pos];\n      if (token == null) {\n        continue;\n      }\n      //link\n      if (prevToken != null) {\n        assert prevToken.next == null;\n        prevToken.next = token; //concatenate linked-list\n      } else {\n        assert firstToken == null;\n        firstToken = token;\n      }\n      //set increments\n      if (vector.hasPositions()) {\n        token.positionIncrement = pos - prevTokenPos;\n        while (token.next != null) {\n          token = token.next;\n          token.positionIncrement = 0;\n        }\n      } else {\n        token.positionIncrement = 1;\n        while (token.next != null) {\n          prevToken = token;\n          token = token.next;\n          if (prevToken.startOffset == token.startOffset) {\n            token.positionIncrement = 0;\n          } else {\n            token.positionIncrement = 1;\n          }\n        }\n      }\n      prevTokenPos = pos;\n      prevToken = token;\n    }\n\n    initialized = true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dd5cb61545fbf8394e449204e9780415b9f4c6fc","date":1429738516,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#init().mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#init().mjava","sourceNew":"  //We delay initialization because we can see which attributes the consumer wants, particularly payloads\n  private void init() throws IOException {\n    assert !initialized;\n    short dpEnumFlags = PostingsEnum.POSITIONS;\n    if (vector.hasOffsets()) {\n      dpEnumFlags |= PostingsEnum.OFFSETS;\n      offsetAttribute = addAttribute(OffsetAttribute.class);\n    }\n    if (vector.hasPayloads() && hasAttribute(PayloadAttribute.class)) {\n      dpEnumFlags |= (PostingsEnum.OFFSETS | PostingsEnum.PAYLOADS);//must ask for offsets too\n      payloadAttribute = getAttribute(PayloadAttribute.class);\n      payloadsBytesRefArray = new BytesRefArray(Counter.newCounter());\n      spareBytesRefBuilder = new BytesRefBuilder();\n    }\n\n    // We put term data here\n    termCharsBuilder = new CharsRefBuilder();\n    termCharsBuilder.grow((int) (vector.size() * 7));//7 is over-estimate of average term len\n\n    // Step 1: iterate termsEnum and create a token, placing into an array of tokens by position\n\n    TokenLL[] positionedTokens = initTokensArray();\n\n    int lastPosition = -1;\n\n    final TermsEnum termsEnum = vector.iterator();\n    BytesRef termBytesRef;\n    PostingsEnum dpEnum = null;\n    CharsRefBuilder tempCharsRefBuilder = new CharsRefBuilder();//only for UTF8->UTF16 call\n    //int sumFreq = 0;\n    while ((termBytesRef = termsEnum.next()) != null) {\n      //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n      // note: if term vectors supported seek by ord then we might just keep an int and seek by ord on-demand\n      tempCharsRefBuilder.grow(termBytesRef.length);\n      final int termCharsLen = UnicodeUtil.UTF8toUTF16(termBytesRef, tempCharsRefBuilder.chars());\n      final int termCharsOff = termCharsBuilder.length();\n      termCharsBuilder.append(tempCharsRefBuilder.chars(), 0, termCharsLen);\n\n      dpEnum = termsEnum.postings(null, dpEnum, dpEnumFlags);\n      assert dpEnum != null; // presumably checked by TokenSources.hasPositions earlier\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      //sumFreq += freq;\n      for (int j = 0; j < freq; j++) {\n        int pos = dpEnum.nextPosition();\n        TokenLL token = new TokenLL();\n        token.termCharsOff = termCharsOff;\n        token.termCharsLen = (short) Math.min(termCharsLen, Short.MAX_VALUE);\n        if (offsetAttribute != null) {\n          token.startOffset = dpEnum.startOffset();\n          if (token.startOffset > maxStartOffset) {\n            continue;//filter this token out; exceeds threshold\n          }\n          token.endOffsetInc = (short) Math.min(dpEnum.endOffset() - token.startOffset, Short.MAX_VALUE);\n          if (pos == -1) {\n            pos = token.startOffset >> 3;//divide by 8\n          }\n        }\n\n        if (payloadAttribute != null) {\n          final BytesRef payload = dpEnum.getPayload();\n          token.payloadIndex = payload == null ? -1 : payloadsBytesRefArray.append(payload);\n        }\n\n        //Add token to an array indexed by position\n        if (positionedTokens.length <= pos) {\n          //grow, but not 2x since we think our original length estimate is close\n          TokenLL[] newPositionedTokens = new TokenLL[(int)((pos + 1) * 1.5f)];\n          System.arraycopy(positionedTokens, 0, newPositionedTokens, 0, lastPosition + 1);\n          positionedTokens = newPositionedTokens;\n        }\n        positionedTokens[pos] = token.insertIntoSortedLinkedList(positionedTokens[pos]);\n\n        lastPosition = Math.max(lastPosition, pos);\n      }\n    }\n\n//    System.out.println(String.format(\n//        \"SumFreq: %5d Size: %4d SumFreq/size: %3.3f MaxPos: %4d MaxPos/SumFreq: %3.3f WastePct: %3.3f\",\n//        sumFreq, vector.size(), (sumFreq / (float)vector.size()), lastPosition, ((float)lastPosition)/sumFreq,\n//        (originalPositionEstimate/(lastPosition + 1.0f))));\n\n    // Step 2:  Link all Tokens into a linked-list and set position increments as we go\n\n    int prevTokenPos = -1;\n    TokenLL prevToken = null;\n    for (int pos = 0; pos <= lastPosition; pos++) {\n      TokenLL token = positionedTokens[pos];\n      if (token == null) {\n        continue;\n      }\n      //link\n      if (prevToken != null) {\n        assert prevToken.next == null;\n        prevToken.next = token; //concatenate linked-list\n      } else {\n        assert firstToken == null;\n        firstToken = token;\n      }\n      //set increments\n      if (vector.hasPositions()) {\n        token.positionIncrement = pos - prevTokenPos;\n        while (token.next != null) {\n          token = token.next;\n          token.positionIncrement = 0;\n        }\n      } else {\n        token.positionIncrement = 1;\n        while (token.next != null) {\n          prevToken = token;\n          token = token.next;\n          if (prevToken.startOffset == token.startOffset) {\n            token.positionIncrement = 0;\n          } else {\n            token.positionIncrement = 1;\n          }\n        }\n      }\n      prevTokenPos = pos;\n      prevToken = token;\n    }\n\n    initialized = true;\n  }\n\n","sourceOld":"  //We delay initialization because we can see which attributes the consumer wants, particularly payloads\n  private void init() throws IOException {\n    assert !initialized;\n    if (vector.hasOffsets()) {\n      offsetAttribute = addAttribute(OffsetAttribute.class);\n    }\n    if (vector.hasPayloads() && hasAttribute(PayloadAttribute.class)) {\n      payloadAttribute = getAttribute(PayloadAttribute.class);\n      payloadsBytesRefArray = new BytesRefArray(Counter.newCounter());\n      spareBytesRefBuilder = new BytesRefBuilder();\n    }\n\n    // Step 1: iterate termsEnum and create a token, placing into an array of tokens by position\n\n    TokenLL[] positionedTokens = initTokensArray();\n\n    int lastPosition = -1;\n\n    final TermsEnum termsEnum = vector.iterator();\n    BytesRef termBytesRef;\n    PostingsEnum dpEnum = null;\n    //int sumFreq = 0;\n    while ((termBytesRef = termsEnum.next()) != null) {\n      //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n      // note: if term vectors supported seek by ord then we might just keep an int and seek by ord on-demand\n      final char[] termChars = new char[termBytesRef.length];\n      final int termCharsLen = UnicodeUtil.UTF8toUTF16(termBytesRef, termChars);\n\n      dpEnum = termsEnum.postings(null, dpEnum, PostingsEnum.POSITIONS);\n      assert dpEnum != null; // presumably checked by TokenSources.hasPositions earlier\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      //sumFreq += freq;\n      for (int j = 0; j < freq; j++) {\n        int pos = dpEnum.nextPosition();\n        TokenLL token = new TokenLL();\n        token.termChars = termChars;\n        token.termCharsLen = termCharsLen;\n        if (offsetAttribute != null) {\n          token.startOffset = dpEnum.startOffset();\n          token.endOffset = dpEnum.endOffset();\n          if (pos == -1) {\n            pos = token.startOffset >> 3;//divide by 8\n          }\n        }\n\n        if (payloadAttribute != null) {\n          final BytesRef payload = dpEnum.getPayload();\n          token.payloadIndex = payload == null ? -1 : payloadsBytesRefArray.append(payload);\n        }\n\n        //Add token to an array indexed by position\n        if (positionedTokens.length <= pos) {\n          //grow, but not 2x since we think our original length estimate is close\n          TokenLL[] newPositionedTokens = new TokenLL[(int)((pos + 1) * 1.5f)];\n          System.arraycopy(positionedTokens, 0, newPositionedTokens, 0, lastPosition + 1);\n          positionedTokens = newPositionedTokens;\n        }\n        positionedTokens[pos] = token.insertIntoSortedLinkedList(positionedTokens[pos]);\n\n        lastPosition = Math.max(lastPosition, pos);\n      }\n    }\n\n//    System.out.println(String.format(\n//        \"SumFreq: %5d Size: %4d SumFreq/size: %3.3f MaxPos: %4d MaxPos/SumFreq: %3.3f WastePct: %3.3f\",\n//        sumFreq, vector.size(), (sumFreq / (float)vector.size()), lastPosition, ((float)lastPosition)/sumFreq,\n//        (originalPositionEstimate/(lastPosition + 1.0f))));\n\n    // Step 2:  Link all Tokens into a linked-list and set position increments as we go\n\n    int prevTokenPos = -1;\n    TokenLL prevToken = null;\n    for (int pos = 0; pos <= lastPosition; pos++) {\n      TokenLL token = positionedTokens[pos];\n      if (token == null) {\n        continue;\n      }\n      //link\n      if (prevToken != null) {\n        assert prevToken.next == null;\n        prevToken.next = token; //concatenate linked-list\n      } else {\n        assert firstToken == null;\n        firstToken = token;\n      }\n      //set increments\n      if (vector.hasPositions()) {\n        token.positionIncrement = pos - prevTokenPos;\n        while (token.next != null) {\n          token = token.next;\n          token.positionIncrement = 0;\n        }\n      } else {\n        token.positionIncrement = 1;\n        while (token.next != null) {\n          prevToken = token;\n          token = token.next;\n          if (prevToken.startOffset == token.startOffset) {\n            token.positionIncrement = 0;\n          } else {\n            token.positionIncrement = 1;\n          }\n        }\n      }\n      prevTokenPos = pos;\n      prevToken = token;\n    }\n\n    initialized = true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#init().mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#init().mjava","sourceNew":"  //We delay initialization because we can see which attributes the consumer wants, particularly payloads\n  private void init() throws IOException {\n    assert !initialized;\n    short dpEnumFlags = PostingsEnum.POSITIONS;\n    if (vector.hasOffsets()) {\n      dpEnumFlags |= PostingsEnum.OFFSETS;\n      offsetAttribute = addAttribute(OffsetAttribute.class);\n    }\n    if (vector.hasPayloads() && hasAttribute(PayloadAttribute.class)) {\n      dpEnumFlags |= (PostingsEnum.OFFSETS | PostingsEnum.PAYLOADS);//must ask for offsets too\n      payloadAttribute = getAttribute(PayloadAttribute.class);\n      payloadsBytesRefArray = new BytesRefArray(Counter.newCounter());\n      spareBytesRefBuilder = new BytesRefBuilder();\n    }\n\n    // We put term data here\n    termCharsBuilder = new CharsRefBuilder();\n    termCharsBuilder.grow((int) (vector.size() * 7));//7 is over-estimate of average term len\n\n    // Step 1: iterate termsEnum and create a token, placing into an array of tokens by position\n\n    TokenLL[] positionedTokens = initTokensArray();\n\n    int lastPosition = -1;\n\n    final TermsEnum termsEnum = vector.iterator();\n    BytesRef termBytesRef;\n    PostingsEnum dpEnum = null;\n    CharsRefBuilder tempCharsRefBuilder = new CharsRefBuilder();//only for UTF8->UTF16 call\n    //int sumFreq = 0;\n    while ((termBytesRef = termsEnum.next()) != null) {\n      //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n      // note: if term vectors supported seek by ord then we might just keep an int and seek by ord on-demand\n      tempCharsRefBuilder.grow(termBytesRef.length);\n      final int termCharsLen = UnicodeUtil.UTF8toUTF16(termBytesRef, tempCharsRefBuilder.chars());\n      final int termCharsOff = termCharsBuilder.length();\n      termCharsBuilder.append(tempCharsRefBuilder.chars(), 0, termCharsLen);\n\n      dpEnum = termsEnum.postings(dpEnum, dpEnumFlags);\n      assert dpEnum != null; // presumably checked by TokenSources.hasPositions earlier\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      //sumFreq += freq;\n      for (int j = 0; j < freq; j++) {\n        int pos = dpEnum.nextPosition();\n        TokenLL token = new TokenLL();\n        token.termCharsOff = termCharsOff;\n        token.termCharsLen = (short) Math.min(termCharsLen, Short.MAX_VALUE);\n        if (offsetAttribute != null) {\n          token.startOffset = dpEnum.startOffset();\n          if (token.startOffset > maxStartOffset) {\n            continue;//filter this token out; exceeds threshold\n          }\n          token.endOffsetInc = (short) Math.min(dpEnum.endOffset() - token.startOffset, Short.MAX_VALUE);\n          if (pos == -1) {\n            pos = token.startOffset >> 3;//divide by 8\n          }\n        }\n\n        if (payloadAttribute != null) {\n          final BytesRef payload = dpEnum.getPayload();\n          token.payloadIndex = payload == null ? -1 : payloadsBytesRefArray.append(payload);\n        }\n\n        //Add token to an array indexed by position\n        if (positionedTokens.length <= pos) {\n          //grow, but not 2x since we think our original length estimate is close\n          TokenLL[] newPositionedTokens = new TokenLL[(int)((pos + 1) * 1.5f)];\n          System.arraycopy(positionedTokens, 0, newPositionedTokens, 0, lastPosition + 1);\n          positionedTokens = newPositionedTokens;\n        }\n        positionedTokens[pos] = token.insertIntoSortedLinkedList(positionedTokens[pos]);\n\n        lastPosition = Math.max(lastPosition, pos);\n      }\n    }\n\n//    System.out.println(String.format(\n//        \"SumFreq: %5d Size: %4d SumFreq/size: %3.3f MaxPos: %4d MaxPos/SumFreq: %3.3f WastePct: %3.3f\",\n//        sumFreq, vector.size(), (sumFreq / (float)vector.size()), lastPosition, ((float)lastPosition)/sumFreq,\n//        (originalPositionEstimate/(lastPosition + 1.0f))));\n\n    // Step 2:  Link all Tokens into a linked-list and set position increments as we go\n\n    int prevTokenPos = -1;\n    TokenLL prevToken = null;\n    for (int pos = 0; pos <= lastPosition; pos++) {\n      TokenLL token = positionedTokens[pos];\n      if (token == null) {\n        continue;\n      }\n      //link\n      if (prevToken != null) {\n        assert prevToken.next == null;\n        prevToken.next = token; //concatenate linked-list\n      } else {\n        assert firstToken == null;\n        firstToken = token;\n      }\n      //set increments\n      if (vector.hasPositions()) {\n        token.positionIncrement = pos - prevTokenPos;\n        while (token.next != null) {\n          token = token.next;\n          token.positionIncrement = 0;\n        }\n      } else {\n        token.positionIncrement = 1;\n        while (token.next != null) {\n          prevToken = token;\n          token = token.next;\n          if (prevToken.startOffset == token.startOffset) {\n            token.positionIncrement = 0;\n          } else {\n            token.positionIncrement = 1;\n          }\n        }\n      }\n      prevTokenPos = pos;\n      prevToken = token;\n    }\n\n    initialized = true;\n  }\n\n","sourceOld":"  //We delay initialization because we can see which attributes the consumer wants, particularly payloads\n  private void init() throws IOException {\n    assert !initialized;\n    short dpEnumFlags = PostingsEnum.POSITIONS;\n    if (vector.hasOffsets()) {\n      dpEnumFlags |= PostingsEnum.OFFSETS;\n      offsetAttribute = addAttribute(OffsetAttribute.class);\n    }\n    if (vector.hasPayloads() && hasAttribute(PayloadAttribute.class)) {\n      dpEnumFlags |= (PostingsEnum.OFFSETS | PostingsEnum.PAYLOADS);//must ask for offsets too\n      payloadAttribute = getAttribute(PayloadAttribute.class);\n      payloadsBytesRefArray = new BytesRefArray(Counter.newCounter());\n      spareBytesRefBuilder = new BytesRefBuilder();\n    }\n\n    // We put term data here\n    termCharsBuilder = new CharsRefBuilder();\n    termCharsBuilder.grow((int) (vector.size() * 7));//7 is over-estimate of average term len\n\n    // Step 1: iterate termsEnum and create a token, placing into an array of tokens by position\n\n    TokenLL[] positionedTokens = initTokensArray();\n\n    int lastPosition = -1;\n\n    final TermsEnum termsEnum = vector.iterator();\n    BytesRef termBytesRef;\n    PostingsEnum dpEnum = null;\n    CharsRefBuilder tempCharsRefBuilder = new CharsRefBuilder();//only for UTF8->UTF16 call\n    //int sumFreq = 0;\n    while ((termBytesRef = termsEnum.next()) != null) {\n      //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n      // note: if term vectors supported seek by ord then we might just keep an int and seek by ord on-demand\n      tempCharsRefBuilder.grow(termBytesRef.length);\n      final int termCharsLen = UnicodeUtil.UTF8toUTF16(termBytesRef, tempCharsRefBuilder.chars());\n      final int termCharsOff = termCharsBuilder.length();\n      termCharsBuilder.append(tempCharsRefBuilder.chars(), 0, termCharsLen);\n\n      dpEnum = termsEnum.postings(null, dpEnum, dpEnumFlags);\n      assert dpEnum != null; // presumably checked by TokenSources.hasPositions earlier\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      //sumFreq += freq;\n      for (int j = 0; j < freq; j++) {\n        int pos = dpEnum.nextPosition();\n        TokenLL token = new TokenLL();\n        token.termCharsOff = termCharsOff;\n        token.termCharsLen = (short) Math.min(termCharsLen, Short.MAX_VALUE);\n        if (offsetAttribute != null) {\n          token.startOffset = dpEnum.startOffset();\n          if (token.startOffset > maxStartOffset) {\n            continue;//filter this token out; exceeds threshold\n          }\n          token.endOffsetInc = (short) Math.min(dpEnum.endOffset() - token.startOffset, Short.MAX_VALUE);\n          if (pos == -1) {\n            pos = token.startOffset >> 3;//divide by 8\n          }\n        }\n\n        if (payloadAttribute != null) {\n          final BytesRef payload = dpEnum.getPayload();\n          token.payloadIndex = payload == null ? -1 : payloadsBytesRefArray.append(payload);\n        }\n\n        //Add token to an array indexed by position\n        if (positionedTokens.length <= pos) {\n          //grow, but not 2x since we think our original length estimate is close\n          TokenLL[] newPositionedTokens = new TokenLL[(int)((pos + 1) * 1.5f)];\n          System.arraycopy(positionedTokens, 0, newPositionedTokens, 0, lastPosition + 1);\n          positionedTokens = newPositionedTokens;\n        }\n        positionedTokens[pos] = token.insertIntoSortedLinkedList(positionedTokens[pos]);\n\n        lastPosition = Math.max(lastPosition, pos);\n      }\n    }\n\n//    System.out.println(String.format(\n//        \"SumFreq: %5d Size: %4d SumFreq/size: %3.3f MaxPos: %4d MaxPos/SumFreq: %3.3f WastePct: %3.3f\",\n//        sumFreq, vector.size(), (sumFreq / (float)vector.size()), lastPosition, ((float)lastPosition)/sumFreq,\n//        (originalPositionEstimate/(lastPosition + 1.0f))));\n\n    // Step 2:  Link all Tokens into a linked-list and set position increments as we go\n\n    int prevTokenPos = -1;\n    TokenLL prevToken = null;\n    for (int pos = 0; pos <= lastPosition; pos++) {\n      TokenLL token = positionedTokens[pos];\n      if (token == null) {\n        continue;\n      }\n      //link\n      if (prevToken != null) {\n        assert prevToken.next == null;\n        prevToken.next = token; //concatenate linked-list\n      } else {\n        assert firstToken == null;\n        firstToken = token;\n      }\n      //set increments\n      if (vector.hasPositions()) {\n        token.positionIncrement = pos - prevTokenPos;\n        while (token.next != null) {\n          token = token.next;\n          token.positionIncrement = 0;\n        }\n      } else {\n        token.positionIncrement = 1;\n        while (token.next != null) {\n          prevToken = token;\n          token = token.next;\n          if (prevToken.startOffset == token.startOffset) {\n            token.positionIncrement = 0;\n          } else {\n            token.positionIncrement = 1;\n          }\n        }\n      }\n      prevTokenPos = pos;\n      prevToken = token;\n    }\n\n    initialized = true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"714aa8d007eef87d7203cfc6e0fe4dab8dd8a497":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["dd5cb61545fbf8394e449204e9780415b9f4c6fc"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"dd5cb61545fbf8394e449204e9780415b9f4c6fc":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"51f5280f31484820499077f41fcdfe92d527d9dc":["4dc0de75e60c6f90667efd23879e863f5b1ca46e"],"4dc0de75e60c6f90667efd23879e863f5b1ca46e":["714aa8d007eef87d7203cfc6e0fe4dab8dd8a497"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0f4464508ee83288c8c4585b533f9faaa93aa314"]},"commit2Childs":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["dd5cb61545fbf8394e449204e9780415b9f4c6fc"],"714aa8d007eef87d7203cfc6e0fe4dab8dd8a497":["4dc0de75e60c6f90667efd23879e863f5b1ca46e"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["714aa8d007eef87d7203cfc6e0fe4dab8dd8a497"],"dd5cb61545fbf8394e449204e9780415b9f4c6fc":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"51f5280f31484820499077f41fcdfe92d527d9dc":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"4dc0de75e60c6f90667efd23879e863f5b1ca46e":["51f5280f31484820499077f41fcdfe92d527d9dc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}