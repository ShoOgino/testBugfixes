{"path":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","commits":[{"id":"0487f900016b7da69f089f740e28192189ef3972","date":1307810819,"type":0,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, \"1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, \"2\", null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, \"3\", null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, \"4\", null, false));\n     tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, \"1/1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, \"2/2\", null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, \"2/3\", null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, \"2/3\", null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, \"3/4\", null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, \"4/5\", null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, \"1/1/1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, \"2/2/2\", null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, \"2/3/3\", null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, \"2/3/3\", null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, \"3/4/4\", null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, \"4/5/5\", null, false));\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"02dba75457528db0b73837ff68f971ecb715ab78","date":1307981000,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","pathOld":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","sourceNew":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","sourceOld":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, \"1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, \"2\", null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, \"3\", null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, \"4\", null, false));\n     tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, \"1/1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, \"2/2\", null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, \"2/3\", null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, \"2/3\", null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, \"3/4\", null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, \"4/5\", null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, \"1/1/1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, \"2/2/2\", null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, \"2/3/3\", null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, \"2/3/3\", null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, \"3/4/4\", null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, \"4/5/5\", null, false));\n  }\n\n","bugFix":null,"bugIntro":["c85fa43e6918808743daa7847ba0264373af687f","c85fa43e6918808743daa7847ba0264373af687f","c85fa43e6918808743daa7847ba0264373af687f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9ed208afa1e7aa98899ddb1dedfddedddf898253","date":1308079587,"type":0,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","pathOld":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","sourceNew":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","sourceOld":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","pathOld":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","sourceNew":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","sourceOld":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":5,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","pathOld":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","sourceNew":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","sourceOld":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["9ed208afa1e7aa98899ddb1dedfddedddf898253"],"c26f00b574427b55127e869b935845554afde1fa":["02dba75457528db0b73837ff68f971ecb715ab78","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"02dba75457528db0b73837ff68f971ecb715ab78":["0487f900016b7da69f089f740e28192189ef3972"],"0487f900016b7da69f089f740e28192189ef3972":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["02dba75457528db0b73837ff68f971ecb715ab78"],"9ed208afa1e7aa98899ddb1dedfddedddf898253":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","02dba75457528db0b73837ff68f971ecb715ab78"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c26f00b574427b55127e869b935845554afde1fa"]},"commit2Childs":{"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"c26f00b574427b55127e869b935845554afde1fa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0487f900016b7da69f089f740e28192189ef3972","9ed208afa1e7aa98899ddb1dedfddedddf898253"],"02dba75457528db0b73837ff68f971ecb715ab78":["c26f00b574427b55127e869b935845554afde1fa","a258fbb26824fd104ed795e5d9033d2d040049ee","9ed208afa1e7aa98899ddb1dedfddedddf898253"],"0487f900016b7da69f089f740e28192189ef3972":["02dba75457528db0b73837ff68f971ecb715ab78"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"9ed208afa1e7aa98899ddb1dedfddedddf898253":["c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a258fbb26824fd104ed795e5d9033d2d040049ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}