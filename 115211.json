{"path":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(AtomicReader,Directory,Directory,Directory,Analyzer,String...).mjava","commits":[{"id":"1d2ec95cdc45879e188017e6e9b00a9a23e5e8c3","date":1354172806,"type":0,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(AtomicReader,Directory,Directory,Directory,Analyzer,String...).mjava","pathOld":"/dev/null","sourceNew":"  public void split(AtomicReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // TODO : check that the passed fields are stored in the original index\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            doc.add(new Field(fieldName, originalIndex.document(scoreDoc.doc).getField(fieldName).stringValue(), ft));\n          }\n        } else {\n          for (StorableField storableField : originalIndex.document(scoreDoc.doc).getFields()) {\n            if (storableField.readerValue()!= null){\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            }\n            else if (storableField.binaryValue()!= null){\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            }\n            else if (storableField.stringValue()!= null){\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            }\n            else if (storableField.numericValue()!= null){\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n          testWriter.commit();\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n          cvWriter.commit();\n        } else {\n          trainingWriter.addDocument(doc);\n          trainingWriter.commit();\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8cb654ba691276b2a2cac5df39bd87df6b81afb2","date":1354177688,"type":3,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(AtomicReader,Directory,Directory,Directory,Analyzer,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(AtomicReader,Directory,Directory,Directory,Analyzer,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   * @param originalIndex an {@link AtomicReader} on the source index\n   * @param trainingIndex a {@link Directory} used to write the training index\n   * @param testIndex a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer {@link Analyzer} used to create the new docs\n   * @param fieldNames names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(AtomicReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // TODO : check that the passed fields are stored in the original index\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            doc.add(new Field(fieldName, originalIndex.document(scoreDoc.doc).getField(fieldName).stringValue(), ft));\n          }\n        } else {\n          for (StorableField storableField : originalIndex.document(scoreDoc.doc).getFields()) {\n            if (storableField.readerValue()!= null){\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            }\n            else if (storableField.binaryValue()!= null){\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            }\n            else if (storableField.stringValue()!= null){\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            }\n            else if (storableField.numericValue()!= null){\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n          testWriter.commit();\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n          cvWriter.commit();\n        } else {\n          trainingWriter.addDocument(doc);\n          trainingWriter.commit();\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","sourceOld":"  public void split(AtomicReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // TODO : check that the passed fields are stored in the original index\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            doc.add(new Field(fieldName, originalIndex.document(scoreDoc.doc).getField(fieldName).stringValue(), ft));\n          }\n        } else {\n          for (StorableField storableField : originalIndex.document(scoreDoc.doc).getFields()) {\n            if (storableField.readerValue()!= null){\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            }\n            else if (storableField.binaryValue()!= null){\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            }\n            else if (storableField.stringValue()!= null){\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            }\n            else if (storableField.numericValue()!= null){\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n          testWriter.commit();\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n          cvWriter.commit();\n        } else {\n          trainingWriter.addDocument(doc);\n          trainingWriter.commit();\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ed7ede3102ac24a0b96a88dc092416f448696921","date":1355127603,"type":3,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(AtomicReader,Directory,Directory,Directory,Analyzer,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(AtomicReader,Directory,Directory,Directory,Analyzer,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link AtomicReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(AtomicReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // TODO : check that the passed fields are stored in the original index\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            doc.add(new Field(fieldName, originalIndex.document(scoreDoc.doc).getField(fieldName).stringValue(), ft));\n          }\n        } else {\n          for (StorableField storableField : originalIndex.document(scoreDoc.doc).getFields()) {\n            if (storableField.readerValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            } else if (storableField.binaryValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            } else if (storableField.stringValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            } else if (storableField.numericValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   * @param originalIndex an {@link AtomicReader} on the source index\n   * @param trainingIndex a {@link Directory} used to write the training index\n   * @param testIndex a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer {@link Analyzer} used to create the new docs\n   * @param fieldNames names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(AtomicReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // TODO : check that the passed fields are stored in the original index\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            doc.add(new Field(fieldName, originalIndex.document(scoreDoc.doc).getField(fieldName).stringValue(), ft));\n          }\n        } else {\n          for (StorableField storableField : originalIndex.document(scoreDoc.doc).getFields()) {\n            if (storableField.readerValue()!= null){\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            }\n            else if (storableField.binaryValue()!= null){\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            }\n            else if (storableField.stringValue()!= null){\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            }\n            else if (storableField.numericValue()!= null){\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n          testWriter.commit();\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n          cvWriter.commit();\n        } else {\n          trainingWriter.addDocument(doc);\n          trainingWriter.commit();\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"407687e67faf6e1f02a211ca078d8e3eed631027","date":1355157407,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(AtomicReader,Directory,Directory,Directory,Analyzer,String...).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link AtomicReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(AtomicReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // TODO : check that the passed fields are stored in the original index\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            doc.add(new Field(fieldName, originalIndex.document(scoreDoc.doc).getField(fieldName).stringValue(), ft));\n          }\n        } else {\n          for (StorableField storableField : originalIndex.document(scoreDoc.doc).getFields()) {\n            if (storableField.readerValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            } else if (storableField.binaryValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            } else if (storableField.stringValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            } else if (storableField.numericValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a1f508b269e97eeeb33e0d21c851eceb57bfd1eb","date":1383406909,"type":3,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(AtomicReader,Directory,Directory,Directory,Analyzer,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(AtomicReader,Directory,Directory,Directory,Analyzer,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link AtomicReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(AtomicReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            doc.add(new Field(fieldName, originalIndex.document(scoreDoc.doc).getField(fieldName).stringValue(), ft));\n          }\n        } else {\n          for (StorableField storableField : originalIndex.document(scoreDoc.doc).getFields()) {\n            if (storableField.readerValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            } else if (storableField.binaryValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            } else if (storableField.stringValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            } else if (storableField.numericValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link AtomicReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(AtomicReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // TODO : check that the passed fields are stored in the original index\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            doc.add(new Field(fieldName, originalIndex.document(scoreDoc.doc).getField(fieldName).stringValue(), ft));\n          }\n        } else {\n          for (StorableField storableField : originalIndex.document(scoreDoc.doc).getFields()) {\n            if (storableField.readerValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            } else if (storableField.binaryValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            } else if (storableField.stringValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            } else if (storableField.numericValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0bf41419d452997826ec5f17684993377be77f49","date":1386629618,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(AtomicReader,Directory,Directory,Directory,Analyzer,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(AtomicReader,Directory,Directory,Directory,Analyzer,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link AtomicReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(AtomicReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    // :Post-Release-Update-Version.LUCENE_XY:\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            doc.add(new Field(fieldName, originalIndex.document(scoreDoc.doc).getField(fieldName).stringValue(), ft));\n          }\n        } else {\n          for (StorableField storableField : originalIndex.document(scoreDoc.doc).getFields()) {\n            if (storableField.readerValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            } else if (storableField.binaryValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            } else if (storableField.stringValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            } else if (storableField.numericValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link AtomicReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(AtomicReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            doc.add(new Field(fieldName, originalIndex.document(scoreDoc.doc).getField(fieldName).stringValue(), ft));\n          }\n        } else {\n          for (StorableField storableField : originalIndex.document(scoreDoc.doc).getFields()) {\n            if (storableField.readerValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            } else if (storableField.binaryValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            } else if (storableField.stringValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            } else if (storableField.numericValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"74f45af4339b0daf7a95c820ab88c1aea74fbce0","date":1387475327,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(AtomicReader,Directory,Directory,Directory,Analyzer,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(AtomicReader,Directory,Directory,Directory,Analyzer,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link AtomicReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(AtomicReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    // :Post-Release-Update-Version.LUCENE_XY:\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            doc.add(new Field(fieldName, originalIndex.document(scoreDoc.doc).getField(fieldName).stringValue(), ft));\n          }\n        } else {\n          for (StorableField storableField : originalIndex.document(scoreDoc.doc).getFields()) {\n            if (storableField.readerValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            } else if (storableField.binaryValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            } else if (storableField.stringValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            } else if (storableField.numericValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link AtomicReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(AtomicReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            doc.add(new Field(fieldName, originalIndex.document(scoreDoc.doc).getField(fieldName).stringValue(), ft));\n          }\n        } else {\n          for (StorableField storableField : originalIndex.document(scoreDoc.doc).getFields()) {\n            if (storableField.readerValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            } else if (storableField.binaryValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            } else if (storableField.stringValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            } else if (storableField.numericValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(AtomicReader,Directory,Directory,Directory,Analyzer,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(AtomicReader,Directory,Directory,Directory,Analyzer,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link AtomicReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(AtomicReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    // :Post-Release-Update-Version.LUCENE_XY:\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            doc.add(new Field(fieldName, originalIndex.document(scoreDoc.doc).getField(fieldName).stringValue(), ft));\n          }\n        } else {\n          for (StorableField storableField : originalIndex.document(scoreDoc.doc).getFields()) {\n            if (storableField.readerValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            } else if (storableField.binaryValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            } else if (storableField.stringValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            } else if (storableField.numericValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n      // close IWs\n      testWriter.shutdown();\n      cvWriter.shutdown();\n      trainingWriter.shutdown();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link AtomicReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(AtomicReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    // :Post-Release-Update-Version.LUCENE_XY:\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            doc.add(new Field(fieldName, originalIndex.document(scoreDoc.doc).getField(fieldName).stringValue(), ft));\n          }\n        } else {\n          for (StorableField storableField : originalIndex.document(scoreDoc.doc).getFields()) {\n            if (storableField.readerValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            } else if (storableField.binaryValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            } else if (storableField.stringValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            } else if (storableField.numericValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dd330c9d05eacbd6e952fe0dea852e7ae037eb50","date":1398873035,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(AtomicReader,Directory,Directory,Directory,Analyzer,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(AtomicReader,Directory,Directory,Directory,Analyzer,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link AtomicReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(AtomicReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    // :Post-Release-Update-Version.LUCENE_XY:\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(Version.LUCENE_5_0, analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(Version.LUCENE_5_0, analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(Version.LUCENE_5_0, analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            doc.add(new Field(fieldName, originalIndex.document(scoreDoc.doc).getField(fieldName).stringValue(), ft));\n          }\n        } else {\n          for (StorableField storableField : originalIndex.document(scoreDoc.doc).getFields()) {\n            if (storableField.readerValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            } else if (storableField.binaryValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            } else if (storableField.stringValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            } else if (storableField.numericValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n      // close IWs\n      testWriter.shutdown();\n      cvWriter.shutdown();\n      trainingWriter.shutdown();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link AtomicReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(AtomicReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    // :Post-Release-Update-Version.LUCENE_XY:\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(Version.LUCENE_50, analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            doc.add(new Field(fieldName, originalIndex.document(scoreDoc.doc).getField(fieldName).stringValue(), ft));\n          }\n        } else {\n          for (StorableField storableField : originalIndex.document(scoreDoc.doc).getFields()) {\n            if (storableField.readerValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            } else if (storableField.binaryValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            } else if (storableField.stringValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            } else if (storableField.numericValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n      // close IWs\n      testWriter.shutdown();\n      cvWriter.shutdown();\n      trainingWriter.shutdown();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(AtomicReader,Directory,Directory,Directory,Analyzer,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(AtomicReader,Directory,Directory,Directory,Analyzer,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link AtomicReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(AtomicReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    // :Post-Release-Update-Version.LUCENE_XY:\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            doc.add(new Field(fieldName, originalIndex.document(scoreDoc.doc).getField(fieldName).stringValue(), ft));\n          }\n        } else {\n          for (StorableField storableField : originalIndex.document(scoreDoc.doc).getFields()) {\n            if (storableField.readerValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            } else if (storableField.binaryValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            } else if (storableField.stringValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            } else if (storableField.numericValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link AtomicReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(AtomicReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    // :Post-Release-Update-Version.LUCENE_XY:\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(Version.LUCENE_5_0, analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(Version.LUCENE_5_0, analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(Version.LUCENE_5_0, analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            doc.add(new Field(fieldName, originalIndex.document(scoreDoc.doc).getField(fieldName).stringValue(), ft));\n          }\n        } else {\n          for (StorableField storableField : originalIndex.document(scoreDoc.doc).getFields()) {\n            if (storableField.readerValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            } else if (storableField.binaryValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            } else if (storableField.stringValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            } else if (storableField.numericValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n      // close IWs\n      testWriter.shutdown();\n      cvWriter.shutdown();\n      trainingWriter.shutdown();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"19e497fe4da591a79332da97681b8017d9c61165","date":1409030374,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(AtomicReader,Directory,Directory,Directory,Analyzer,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(AtomicReader,Directory,Directory,Directory,Analyzer,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link AtomicReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(AtomicReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            doc.add(new Field(fieldName, originalIndex.document(scoreDoc.doc).getField(fieldName).stringValue(), ft));\n          }\n        } else {\n          for (StorableField storableField : originalIndex.document(scoreDoc.doc).getFields()) {\n            if (storableField.readerValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            } else if (storableField.binaryValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            } else if (storableField.stringValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            } else if (storableField.numericValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link AtomicReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(AtomicReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    // :Post-Release-Update-Version.LUCENE_XY:\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            doc.add(new Field(fieldName, originalIndex.document(scoreDoc.doc).getField(fieldName).stringValue(), ft));\n          }\n        } else {\n          for (StorableField storableField : originalIndex.document(scoreDoc.doc).getFields()) {\n            if (storableField.readerValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            } else if (storableField.binaryValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            } else if (storableField.stringValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            } else if (storableField.numericValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":4,"author":"Ryan Ernst","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(AtomicReader,Directory,Directory,Directory,Analyzer,String...).mjava","sourceNew":null,"sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link AtomicReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(AtomicReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            doc.add(new Field(fieldName, originalIndex.document(scoreDoc.doc).getField(fieldName).stringValue(), ft));\n          }\n        } else {\n          for (StorableField storableField : originalIndex.document(scoreDoc.doc).getFields()) {\n            if (storableField.readerValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            } else if (storableField.binaryValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            } else if (storableField.stringValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            } else if (storableField.numericValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"407687e67faf6e1f02a211ca078d8e3eed631027":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","ed7ede3102ac24a0b96a88dc092416f448696921"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["19e497fe4da591a79332da97681b8017d9c61165"],"1d2ec95cdc45879e188017e6e9b00a9a23e5e8c3":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"19e497fe4da591a79332da97681b8017d9c61165":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"0bf41419d452997826ec5f17684993377be77f49":["a1f508b269e97eeeb33e0d21c851eceb57bfd1eb"],"8cb654ba691276b2a2cac5df39bd87df6b81afb2":["1d2ec95cdc45879e188017e6e9b00a9a23e5e8c3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["dd330c9d05eacbd6e952fe0dea852e7ae037eb50"],"ed7ede3102ac24a0b96a88dc092416f448696921":["8cb654ba691276b2a2cac5df39bd87df6b81afb2"],"dd330c9d05eacbd6e952fe0dea852e7ae037eb50":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":["a1f508b269e97eeeb33e0d21c851eceb57bfd1eb","0bf41419d452997826ec5f17684993377be77f49"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["0bf41419d452997826ec5f17684993377be77f49"],"a1f508b269e97eeeb33e0d21c851eceb57bfd1eb":["ed7ede3102ac24a0b96a88dc092416f448696921"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c9fb5f46e264daf5ba3860defe623a89d202dd87"]},"commit2Childs":{"407687e67faf6e1f02a211ca078d8e3eed631027":[],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"1d2ec95cdc45879e188017e6e9b00a9a23e5e8c3":["8cb654ba691276b2a2cac5df39bd87df6b81afb2"],"19e497fe4da591a79332da97681b8017d9c61165":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"0bf41419d452997826ec5f17684993377be77f49":["74f45af4339b0daf7a95c820ab88c1aea74fbce0","ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"8cb654ba691276b2a2cac5df39bd87df6b81afb2":["ed7ede3102ac24a0b96a88dc092416f448696921"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["407687e67faf6e1f02a211ca078d8e3eed631027","1d2ec95cdc45879e188017e6e9b00a9a23e5e8c3"],"ed7ede3102ac24a0b96a88dc092416f448696921":["407687e67faf6e1f02a211ca078d8e3eed631027","a1f508b269e97eeeb33e0d21c851eceb57bfd1eb"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["19e497fe4da591a79332da97681b8017d9c61165"],"dd330c9d05eacbd6e952fe0dea852e7ae037eb50":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":[],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["dd330c9d05eacbd6e952fe0dea852e7ae037eb50"],"a1f508b269e97eeeb33e0d21c851eceb57bfd1eb":["0bf41419d452997826ec5f17684993377be77f49","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["407687e67faf6e1f02a211ca078d8e3eed631027","74f45af4339b0daf7a95c820ab88c1aea74fbce0","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}