{"path":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","commits":[{"id":"9fc47cb7b4346802411bb432f501ed0673d7119e","date":1512640179,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n\n        PointValues values = reader.getPointValues(field);\n        if (values == null) {\n          // No docs in this segment/field indexed any points\n          return null;\n        }\n\n        if (values.getNumDimensions() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + values.getNumDimensions() + \" but this query has numDims=\" + numDims);\n        }\n        if (values.getBytesPerDimension() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + values.getBytesPerDimension() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        return true;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n\n        PointValues values = reader.getPointValues(field);\n        if (values == null) {\n          // No docs in this segment/field indexed any points\n          return null;\n        }\n\n        if (values.getNumDimensions() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + values.getNumDimensions() + \" but this query has numDims=\" + numDims);\n        }\n        if (values.getBytesPerDimension() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + values.getBytesPerDimension() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        return true;\n      }\n\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"417142ff08fda9cf0b72d5133e63097a166c6458","date":1512729693,"type":1,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n\n        PointValues values = reader.getPointValues(field);\n        if (values == null) {\n          // No docs in this segment/field indexed any points\n          return null;\n        }\n\n        if (values.getNumDimensions() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + values.getNumDimensions() + \" but this query has numDims=\" + numDims);\n        }\n        if (values.getBytesPerDimension() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + values.getBytesPerDimension() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        return true;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n\n        PointValues values = reader.getPointValues(field);\n        if (values == null) {\n          // No docs in this segment/field indexed any points\n          return null;\n        }\n\n        if (values.getNumDimensions() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + values.getNumDimensions() + \" but this query has numDims=\" + numDims);\n        }\n        if (values.getBytesPerDimension() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + values.getBytesPerDimension() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        return true;\n      }\n\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f6652c943595e92c187ee904c382863013eae28f","date":1539042663,"type":3,"author":"Nicholas Knize","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","sourceNew":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n\n        PointValues values = reader.getPointValues(field);\n        if (values == null) {\n          // No docs in this segment/field indexed any points\n          return null;\n        }\n\n        if (values.getNumIndexDimensions() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numIndexDims=\" + values.getNumIndexDimensions() + \" but this query has numIndexDims=\" + numDims);\n        }\n        if (values.getBytesPerDimension() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + values.getBytesPerDimension() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        return true;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n\n        PointValues values = reader.getPointValues(field);\n        if (values == null) {\n          // No docs in this segment/field indexed any points\n          return null;\n        }\n\n        if (values.getNumDimensions() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + values.getNumDimensions() + \" but this query has numDims=\" + numDims);\n        }\n        if (values.getBytesPerDimension() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + values.getBytesPerDimension() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        return true;\n      }\n\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9c226b0eeb8b028f572020f459851a663a2c064e","date":1542377651,"type":3,"author":"Christophe Bismuth","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","sourceNew":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n\n        PointValues values = reader.getPointValues(field);\n        if (values == null) {\n          // No docs in this segment/field indexed any points\n          return null;\n        }\n\n        if (values.getNumIndexDimensions() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numIndexDims=\" + values.getNumIndexDimensions() + \" but this query has numIndexDims=\" + numDims);\n        }\n        if (values.getBytesPerDimension() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + values.getBytesPerDimension() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), scoreMode, result.build().iterator());\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        return true;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n\n        PointValues values = reader.getPointValues(field);\n        if (values == null) {\n          // No docs in this segment/field indexed any points\n          return null;\n        }\n\n        if (values.getNumIndexDimensions() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numIndexDims=\" + values.getNumIndexDimensions() + \" but this query has numIndexDims=\" + numDims);\n        }\n        if (values.getBytesPerDimension() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + values.getBytesPerDimension() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        return true;\n      }\n\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"f6652c943595e92c187ee904c382863013eae28f":["417142ff08fda9cf0b72d5133e63097a166c6458"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9fc47cb7b4346802411bb432f501ed0673d7119e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"9c226b0eeb8b028f572020f459851a663a2c064e":["f6652c943595e92c187ee904c382863013eae28f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9c226b0eeb8b028f572020f459851a663a2c064e"],"417142ff08fda9cf0b72d5133e63097a166c6458":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","9fc47cb7b4346802411bb432f501ed0673d7119e"]},"commit2Childs":{"f6652c943595e92c187ee904c382863013eae28f":["9c226b0eeb8b028f572020f459851a663a2c064e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9fc47cb7b4346802411bb432f501ed0673d7119e","417142ff08fda9cf0b72d5133e63097a166c6458"],"9fc47cb7b4346802411bb432f501ed0673d7119e":["417142ff08fda9cf0b72d5133e63097a166c6458"],"9c226b0eeb8b028f572020f459851a663a2c064e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"417142ff08fda9cf0b72d5133e63097a166c6458":["f6652c943595e92c187ee904c382863013eae28f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}