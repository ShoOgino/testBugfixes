{"path":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findForcedMergesMaxNumSegments(SegmentInfos,int,int).mjava","commits":[{"id":"d14e8d18c0e3970c20354dbeeb49da11bd587fbd","date":1321041051,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findForcedMergesMaxNumSegments(SegmentInfos,int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","sourceNew":"  /**\n   * Returns the merges necessary to forceMerge the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findForcedMergesMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    final List<SegmentInfo> segments = infos.asList();\n\n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(new OneMerge(segments.subList(last - mergeFactor, last)));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must merge down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isMerged(infos.info(0))) {\n          spec.add(new OneMerge(segments.subList(0, last)));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // forceMerge was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(new OneMerge(segments.subList(bestStart, bestStart + finalMergeSize)));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","sourceOld":"  /**\n   * Returns the merges necessary to optimize the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findMergesForOptimizeMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    final List<SegmentInfo> segments = infos.asList();\n\n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(new OneMerge(segments.subList(last - mergeFactor, last)));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must optimize down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isOptimized(infos.info(0))) {\n          spec.add(new OneMerge(segments.subList(0, last)));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // optimize was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(new OneMerge(segments.subList(bestStart, bestStart + finalMergeSize)));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/LogMergePolicy#findForcedMergesMaxNumSegments(SegmentInfos,int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findForcedMergesMaxNumSegments(SegmentInfos,int,int).mjava","sourceNew":"  /**\n   * Returns the merges necessary to forceMerge the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findForcedMergesMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    final List<SegmentInfo> segments = infos.asList();\n\n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(new OneMerge(segments.subList(last - mergeFactor, last)));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must merge down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isMerged(infos.info(0))) {\n          spec.add(new OneMerge(segments.subList(0, last)));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // forceMerge was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(new OneMerge(segments.subList(bestStart, bestStart + finalMergeSize)));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","sourceOld":"  /**\n   * Returns the merges necessary to forceMerge the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findForcedMergesMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    final List<SegmentInfo> segments = infos.asList();\n\n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(new OneMerge(segments.subList(last - mergeFactor, last)));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must merge down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isMerged(infos.info(0))) {\n          spec.add(new OneMerge(segments.subList(0, last)));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // forceMerge was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(new OneMerge(segments.subList(bestStart, bestStart + finalMergeSize)));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}