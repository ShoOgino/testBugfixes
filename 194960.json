{"path":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader#refresh().mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader#refresh().mjava","pathOld":"modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader#refresh().mjava","sourceNew":"  // Note that refresh() is synchronized (it is the only synchronized\n  // method in this class) to ensure that it never gets called concurrently\n  // with itself.\n  public synchronized boolean refresh() throws IOException, InconsistentTaxonomyException {\n    ensureOpen();\n    /*\n     * Since refresh() can be a lengthy operation, it is very important that we\n     * avoid locking out all readers for its duration. This is why we don't hold\n     * the indexReaderLock write lock for the entire duration of this method. In\n     * fact, it is enough to hold it only during a single assignment! Other\n     * comments in this method will explain this.\n     */\n\n    // note that the lengthy operation indexReader.reopen() does not\n    // modify the reader, so we can do it without holding a lock. We can\n    // safely read indexReader without holding the write lock, because\n    // no other thread can be writing at this time (this method is the\n    // only possible writer, and it is \"synchronized\" to avoid this case).\n    DirectoryReader r2 = DirectoryReader.openIfChanged(indexReader);\n    if (r2 == null) {\n    \treturn false; // no changes, nothing to do\n    } \n    \n    // validate that a refresh is valid at this point, i.e. that the taxonomy \n    // was not recreated since this reader was last opened or refresshed.\n    String t1 = indexReader.getIndexCommit().getUserData().get(DirectoryTaxonomyWriter.INDEX_CREATE_TIME);\n    String t2 = r2.getIndexCommit().getUserData().get(DirectoryTaxonomyWriter.INDEX_CREATE_TIME);\n    if (t1==null) {\n    \tif (t2!=null) {\n    \t\tr2.close();\n    \t\tthrow new InconsistentTaxonomyException(\"Taxonomy was recreated at: \"+t2);\n    \t}\n    } else if (!t1.equals(t2)) {\n    \tr2.close();\n    \tthrow new InconsistentTaxonomyException(\"Taxonomy was recreated at: \"+t2+\"  !=  \"+t1);\n    }\n    \n      IndexReader oldreader = indexReader;\n      // we can close the old searcher, but need to synchronize this\n      // so that we don't close it in the middle that another routine\n      // is reading from it.\n      indexReaderLock.writeLock().lock();\n      indexReader = r2;\n      indexReaderLock.writeLock().unlock();\n      // We can close the old reader, but need to be certain that we\n      // don't close it while another method is reading from it.\n      // Luckily, we can be certain of that even without putting the\n      // oldreader.close() in the locked section. The reason is that\n      // after lock() succeeded above, we know that all existing readers\n      // had finished (this is what a read-write lock ensures). New\n      // readers, starting after the unlock() we just did, already got\n      // the new indexReader we set above. So nobody can be possibly\n      // using the old indexReader, and we can close it:\n      oldreader.close();\n\n      // We prefetch some of the arrays to make requests much faster.\n      // Let's refresh these prefetched arrays; This refresh is much\n      // is made more efficient by assuming that it is enough to read\n      // the values for new categories (old categories could not have been\n      // changed or deleted)\n      // Note that this this done without the write lock being held,\n      // which means that it is possible that during a refresh(), a\n      // reader will have some methods (like getOrdinal and getCategory)\n      // return fresh information, while getParent()\n      // (only to be prefetched now) still return older information.\n      // We consider this to be acceptable. The important thing,\n      // however, is that refreshPrefetchArrays() itself writes to\n      // the arrays in a correct manner (see discussion there)\n      parentArray.refresh(indexReader);\n\n      // Remove any INVALID_ORDINAL values from the ordinal cache,\n      // because it is possible those are now answered by the new data!\n      Iterator<Entry<String, Integer>> i = ordinalCache.entrySet().iterator();\n      while (i.hasNext()) {\n        Entry<String, Integer> e = i.next();\n        if (e.getValue().intValue() == INVALID_ORDINAL) {\n          i.remove();\n        }\n      }\n      return true;\n    }\n\n","sourceOld":"  // Note that refresh() is synchronized (it is the only synchronized\n  // method in this class) to ensure that it never gets called concurrently\n  // with itself.\n  public synchronized boolean refresh() throws IOException, InconsistentTaxonomyException {\n    ensureOpen();\n    /*\n     * Since refresh() can be a lengthy operation, it is very important that we\n     * avoid locking out all readers for its duration. This is why we don't hold\n     * the indexReaderLock write lock for the entire duration of this method. In\n     * fact, it is enough to hold it only during a single assignment! Other\n     * comments in this method will explain this.\n     */\n\n    // note that the lengthy operation indexReader.reopen() does not\n    // modify the reader, so we can do it without holding a lock. We can\n    // safely read indexReader without holding the write lock, because\n    // no other thread can be writing at this time (this method is the\n    // only possible writer, and it is \"synchronized\" to avoid this case).\n    DirectoryReader r2 = DirectoryReader.openIfChanged(indexReader);\n    if (r2 == null) {\n    \treturn false; // no changes, nothing to do\n    } \n    \n    // validate that a refresh is valid at this point, i.e. that the taxonomy \n    // was not recreated since this reader was last opened or refresshed.\n    String t1 = indexReader.getIndexCommit().getUserData().get(DirectoryTaxonomyWriter.INDEX_CREATE_TIME);\n    String t2 = r2.getIndexCommit().getUserData().get(DirectoryTaxonomyWriter.INDEX_CREATE_TIME);\n    if (t1==null) {\n    \tif (t2!=null) {\n    \t\tr2.close();\n    \t\tthrow new InconsistentTaxonomyException(\"Taxonomy was recreated at: \"+t2);\n    \t}\n    } else if (!t1.equals(t2)) {\n    \tr2.close();\n    \tthrow new InconsistentTaxonomyException(\"Taxonomy was recreated at: \"+t2+\"  !=  \"+t1);\n    }\n    \n      IndexReader oldreader = indexReader;\n      // we can close the old searcher, but need to synchronize this\n      // so that we don't close it in the middle that another routine\n      // is reading from it.\n      indexReaderLock.writeLock().lock();\n      indexReader = r2;\n      indexReaderLock.writeLock().unlock();\n      // We can close the old reader, but need to be certain that we\n      // don't close it while another method is reading from it.\n      // Luckily, we can be certain of that even without putting the\n      // oldreader.close() in the locked section. The reason is that\n      // after lock() succeeded above, we know that all existing readers\n      // had finished (this is what a read-write lock ensures). New\n      // readers, starting after the unlock() we just did, already got\n      // the new indexReader we set above. So nobody can be possibly\n      // using the old indexReader, and we can close it:\n      oldreader.close();\n\n      // We prefetch some of the arrays to make requests much faster.\n      // Let's refresh these prefetched arrays; This refresh is much\n      // is made more efficient by assuming that it is enough to read\n      // the values for new categories (old categories could not have been\n      // changed or deleted)\n      // Note that this this done without the write lock being held,\n      // which means that it is possible that during a refresh(), a\n      // reader will have some methods (like getOrdinal and getCategory)\n      // return fresh information, while getParent()\n      // (only to be prefetched now) still return older information.\n      // We consider this to be acceptable. The important thing,\n      // however, is that refreshPrefetchArrays() itself writes to\n      // the arrays in a correct manner (see discussion there)\n      parentArray.refresh(indexReader);\n\n      // Remove any INVALID_ORDINAL values from the ordinal cache,\n      // because it is possible those are now answered by the new data!\n      Iterator<Entry<String, Integer>> i = ordinalCache.entrySet().iterator();\n      while (i.hasNext()) {\n        Entry<String, Integer> e = i.next();\n        if (e.getValue().intValue() == INVALID_ORDINAL) {\n          i.remove();\n        }\n      }\n      return true;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4093b270ba337f9c25a4c0e6cb2ae2c07f697376","date":1347897716,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader#refresh().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader#refresh().mjava","sourceNew":"  // Note that refresh() is synchronized (it is the only synchronized\n  // method in this class) to ensure that it never gets called concurrently\n  // with itself.\n  public synchronized boolean refresh() throws IOException, InconsistentTaxonomyException {\n    ensureOpen();\n    /*\n     * Since refresh() can be a lengthy operation, it is very important that we\n     * avoid locking out all readers for its duration. This is why we don't hold\n     * the indexReaderLock write lock for the entire duration of this method. In\n     * fact, it is enough to hold it only during a single assignment! Other\n     * comments in this method will explain this.\n     */\n\n    // note that the lengthy operation indexReader.reopen() does not\n    // modify the reader, so we can do it without holding a lock. We can\n    // safely read indexReader without holding the write lock, because\n    // no other thread can be writing at this time (this method is the\n    // only possible writer, and it is \"synchronized\" to avoid this case).\n    DirectoryReader r2 = DirectoryReader.openIfChanged(indexReader);\n    if (r2 == null) {\n      return false; // no changes, nothing to do\n    } \n    \n    // validate that a refresh is valid at this point, i.e. that the taxonomy \n    // was not recreated since this reader was last opened or refresshed.\n    String t1 = indexReader.getIndexCommit().getUserData().get(DirectoryTaxonomyWriter.INDEX_CREATE_TIME);\n    String t2 = r2.getIndexCommit().getUserData().get(DirectoryTaxonomyWriter.INDEX_CREATE_TIME);\n    if (t1==null) {\n      if (t2!=null) {\n        r2.close();\n        throw new InconsistentTaxonomyException(\"Taxonomy was recreated at: \"+t2);\n      }\n    } else if (!t1.equals(t2)) {\n      r2.close();\n      throw new InconsistentTaxonomyException(\"Taxonomy was recreated at: \"+t2+\"  !=  \"+t1);\n    }\n    \n      IndexReader oldreader = indexReader;\n      // we can close the old searcher, but need to synchronize this\n      // so that we don't close it in the middle that another routine\n      // is reading from it.\n      indexReaderLock.writeLock().lock();\n      indexReader = r2;\n      indexReaderLock.writeLock().unlock();\n      // We can close the old reader, but need to be certain that we\n      // don't close it while another method is reading from it.\n      // Luckily, we can be certain of that even without putting the\n      // oldreader.close() in the locked section. The reason is that\n      // after lock() succeeded above, we know that all existing readers\n      // had finished (this is what a read-write lock ensures). New\n      // readers, starting after the unlock() we just did, already got\n      // the new indexReader we set above. So nobody can be possibly\n      // using the old indexReader, and we can close it:\n      oldreader.close();\n\n      // We prefetch some of the arrays to make requests much faster.\n      // Let's refresh these prefetched arrays; This refresh is much\n      // is made more efficient by assuming that it is enough to read\n      // the values for new categories (old categories could not have been\n      // changed or deleted)\n      // Note that this this done without the write lock being held,\n      // which means that it is possible that during a refresh(), a\n      // reader will have some methods (like getOrdinal and getCategory)\n      // return fresh information, while getParent()\n      // (only to be prefetched now) still return older information.\n      // We consider this to be acceptable. The important thing,\n      // however, is that refreshPrefetchArrays() itself writes to\n      // the arrays in a correct manner (see discussion there)\n      parentArray.refresh(indexReader);\n\n      // Remove any INVALID_ORDINAL values from the ordinal cache,\n      // because it is possible those are now answered by the new data!\n      Iterator<Entry<String, Integer>> i = ordinalCache.entrySet().iterator();\n      while (i.hasNext()) {\n        Entry<String, Integer> e = i.next();\n        if (e.getValue().intValue() == INVALID_ORDINAL) {\n          i.remove();\n        }\n      }\n      return true;\n    }\n\n","sourceOld":"  // Note that refresh() is synchronized (it is the only synchronized\n  // method in this class) to ensure that it never gets called concurrently\n  // with itself.\n  public synchronized boolean refresh() throws IOException, InconsistentTaxonomyException {\n    ensureOpen();\n    /*\n     * Since refresh() can be a lengthy operation, it is very important that we\n     * avoid locking out all readers for its duration. This is why we don't hold\n     * the indexReaderLock write lock for the entire duration of this method. In\n     * fact, it is enough to hold it only during a single assignment! Other\n     * comments in this method will explain this.\n     */\n\n    // note that the lengthy operation indexReader.reopen() does not\n    // modify the reader, so we can do it without holding a lock. We can\n    // safely read indexReader without holding the write lock, because\n    // no other thread can be writing at this time (this method is the\n    // only possible writer, and it is \"synchronized\" to avoid this case).\n    DirectoryReader r2 = DirectoryReader.openIfChanged(indexReader);\n    if (r2 == null) {\n    \treturn false; // no changes, nothing to do\n    } \n    \n    // validate that a refresh is valid at this point, i.e. that the taxonomy \n    // was not recreated since this reader was last opened or refresshed.\n    String t1 = indexReader.getIndexCommit().getUserData().get(DirectoryTaxonomyWriter.INDEX_CREATE_TIME);\n    String t2 = r2.getIndexCommit().getUserData().get(DirectoryTaxonomyWriter.INDEX_CREATE_TIME);\n    if (t1==null) {\n    \tif (t2!=null) {\n    \t\tr2.close();\n    \t\tthrow new InconsistentTaxonomyException(\"Taxonomy was recreated at: \"+t2);\n    \t}\n    } else if (!t1.equals(t2)) {\n    \tr2.close();\n    \tthrow new InconsistentTaxonomyException(\"Taxonomy was recreated at: \"+t2+\"  !=  \"+t1);\n    }\n    \n      IndexReader oldreader = indexReader;\n      // we can close the old searcher, but need to synchronize this\n      // so that we don't close it in the middle that another routine\n      // is reading from it.\n      indexReaderLock.writeLock().lock();\n      indexReader = r2;\n      indexReaderLock.writeLock().unlock();\n      // We can close the old reader, but need to be certain that we\n      // don't close it while another method is reading from it.\n      // Luckily, we can be certain of that even without putting the\n      // oldreader.close() in the locked section. The reason is that\n      // after lock() succeeded above, we know that all existing readers\n      // had finished (this is what a read-write lock ensures). New\n      // readers, starting after the unlock() we just did, already got\n      // the new indexReader we set above. So nobody can be possibly\n      // using the old indexReader, and we can close it:\n      oldreader.close();\n\n      // We prefetch some of the arrays to make requests much faster.\n      // Let's refresh these prefetched arrays; This refresh is much\n      // is made more efficient by assuming that it is enough to read\n      // the values for new categories (old categories could not have been\n      // changed or deleted)\n      // Note that this this done without the write lock being held,\n      // which means that it is possible that during a refresh(), a\n      // reader will have some methods (like getOrdinal and getCategory)\n      // return fresh information, while getParent()\n      // (only to be prefetched now) still return older information.\n      // We consider this to be acceptable. The important thing,\n      // however, is that refreshPrefetchArrays() itself writes to\n      // the arrays in a correct manner (see discussion there)\n      parentArray.refresh(indexReader);\n\n      // Remove any INVALID_ORDINAL values from the ordinal cache,\n      // because it is possible those are now answered by the new data!\n      Iterator<Entry<String, Integer>> i = ordinalCache.entrySet().iterator();\n      while (i.hasNext()) {\n        Entry<String, Integer> e = i.next();\n        if (e.getValue().intValue() == INVALID_ORDINAL) {\n          i.remove();\n        }\n      }\n      return true;\n    }\n\n","bugFix":null,"bugIntro":["78e3613d9274c0d98ca67d976e415c82e9f9cf46"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"78e3613d9274c0d98ca67d976e415c82e9f9cf46","date":1352285414,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader#refresh().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader#refresh().mjava","sourceNew":"  // Note that refresh() is synchronized (it is the only synchronized\n  // method in this class) to ensure that it never gets called concurrently\n  // with itself.\n  @Override\n  public synchronized boolean refresh() throws IOException, InconsistentTaxonomyException {\n    ensureOpen();\n    /*\n     * Since refresh() can be a lengthy operation, it is very important that we\n     * avoid locking out all readers for its duration. This is why we don't hold\n     * the indexReaderLock write lock for the entire duration of this method. In\n     * fact, it is enough to hold it only during a single assignment! Other\n     * comments in this method will explain this.\n     */\n\n    // note that the lengthy operation indexReader.reopen() does not\n    // modify the reader, so we can do it without holding a lock. We can\n    // safely read indexReader without holding the write lock, because\n    // no other thread can be writing at this time (this method is the\n    // only possible writer, and it is \"synchronized\" to avoid this case).\n    DirectoryReader r2 = DirectoryReader.openIfChanged(indexReader);\n    if (r2 == null) {\n      return false; // no changes, nothing to do\n    }\n    \n    // validate that a refresh is valid at this point, i.e. that the taxonomy \n    // was not recreated since this reader was last opened or refresshed.\n    String t1 = indexReader.getIndexCommit().getUserData().get(DirectoryTaxonomyWriter.INDEX_EPOCH);\n    String t2 = r2.getIndexCommit().getUserData().get(DirectoryTaxonomyWriter.INDEX_EPOCH);\n    if (t1 == null) {\n      if (t2 != null) {\n        r2.close();\n        throw new InconsistentTaxonomyException(\"Taxonomy was recreated, epoch= \" + t2);\n      }\n    } else if (!t1.equals(t2)) {\n      // t1 != null and t2 cannot be null b/c DirTaxoWriter always puts the commit data.\n      // it's ok to use String.equals because we require the two epoch values to be the same.\n      r2.close();\n      throw new InconsistentTaxonomyException(\"Taxonomy was recreated epoch = \" + t2 + \"  !=  \" + t1);\n    }\n    \n    IndexReader oldreader = indexReader;\n    // we can close the old searcher, but need to synchronize this\n    // so that we don't close it in the middle that another routine\n    // is reading from it.\n    indexReaderLock.writeLock().lock();\n    indexReader = r2;\n    indexReaderLock.writeLock().unlock();\n    // We can close the old reader, but need to be certain that we\n    // don't close it while another method is reading from it.\n    // Luckily, we can be certain of that even without putting the\n    // oldreader.close() in the locked section. The reason is that\n    // after lock() succeeded above, we know that all existing readers\n    // had finished (this is what a read-write lock ensures). New\n    // readers, starting after the unlock() we just did, already got\n    // the new indexReader we set above. So nobody can be possibly\n    // using the old indexReader, and we can close it:\n    oldreader.close();\n    \n    // We prefetch some of the arrays to make requests much faster.\n    // Let's refresh these prefetched arrays; This refresh is much\n    // is made more efficient by assuming that it is enough to read\n    // the values for new categories (old categories could not have been\n    // changed or deleted)\n    // Note that this this done without the write lock being held,\n    // which means that it is possible that during a refresh(), a\n    // reader will have some methods (like getOrdinal and getCategory)\n    // return fresh information, while getParent()\n    // (only to be prefetched now) still return older information.\n    // We consider this to be acceptable. The important thing,\n    // however, is that refreshPrefetchArrays() itself writes to\n    // the arrays in a correct manner (see discussion there)\n    parentArray.refresh(indexReader);\n    \n    // Remove any INVALID_ORDINAL values from the ordinal cache,\n    // because it is possible those are now answered by the new data!\n    Iterator<Entry<String, Integer>> i = ordinalCache.entrySet().iterator();\n    while (i.hasNext()) {\n      Entry<String, Integer> e = i.next();\n      if (e.getValue().intValue() == INVALID_ORDINAL) {\n        i.remove();\n      }\n    }\n    return true;\n  }\n\n","sourceOld":"  // Note that refresh() is synchronized (it is the only synchronized\n  // method in this class) to ensure that it never gets called concurrently\n  // with itself.\n  public synchronized boolean refresh() throws IOException, InconsistentTaxonomyException {\n    ensureOpen();\n    /*\n     * Since refresh() can be a lengthy operation, it is very important that we\n     * avoid locking out all readers for its duration. This is why we don't hold\n     * the indexReaderLock write lock for the entire duration of this method. In\n     * fact, it is enough to hold it only during a single assignment! Other\n     * comments in this method will explain this.\n     */\n\n    // note that the lengthy operation indexReader.reopen() does not\n    // modify the reader, so we can do it without holding a lock. We can\n    // safely read indexReader without holding the write lock, because\n    // no other thread can be writing at this time (this method is the\n    // only possible writer, and it is \"synchronized\" to avoid this case).\n    DirectoryReader r2 = DirectoryReader.openIfChanged(indexReader);\n    if (r2 == null) {\n      return false; // no changes, nothing to do\n    } \n    \n    // validate that a refresh is valid at this point, i.e. that the taxonomy \n    // was not recreated since this reader was last opened or refresshed.\n    String t1 = indexReader.getIndexCommit().getUserData().get(DirectoryTaxonomyWriter.INDEX_CREATE_TIME);\n    String t2 = r2.getIndexCommit().getUserData().get(DirectoryTaxonomyWriter.INDEX_CREATE_TIME);\n    if (t1==null) {\n      if (t2!=null) {\n        r2.close();\n        throw new InconsistentTaxonomyException(\"Taxonomy was recreated at: \"+t2);\n      }\n    } else if (!t1.equals(t2)) {\n      r2.close();\n      throw new InconsistentTaxonomyException(\"Taxonomy was recreated at: \"+t2+\"  !=  \"+t1);\n    }\n    \n      IndexReader oldreader = indexReader;\n      // we can close the old searcher, but need to synchronize this\n      // so that we don't close it in the middle that another routine\n      // is reading from it.\n      indexReaderLock.writeLock().lock();\n      indexReader = r2;\n      indexReaderLock.writeLock().unlock();\n      // We can close the old reader, but need to be certain that we\n      // don't close it while another method is reading from it.\n      // Luckily, we can be certain of that even without putting the\n      // oldreader.close() in the locked section. The reason is that\n      // after lock() succeeded above, we know that all existing readers\n      // had finished (this is what a read-write lock ensures). New\n      // readers, starting after the unlock() we just did, already got\n      // the new indexReader we set above. So nobody can be possibly\n      // using the old indexReader, and we can close it:\n      oldreader.close();\n\n      // We prefetch some of the arrays to make requests much faster.\n      // Let's refresh these prefetched arrays; This refresh is much\n      // is made more efficient by assuming that it is enough to read\n      // the values for new categories (old categories could not have been\n      // changed or deleted)\n      // Note that this this done without the write lock being held,\n      // which means that it is possible that during a refresh(), a\n      // reader will have some methods (like getOrdinal and getCategory)\n      // return fresh information, while getParent()\n      // (only to be prefetched now) still return older information.\n      // We consider this to be acceptable. The important thing,\n      // however, is that refreshPrefetchArrays() itself writes to\n      // the arrays in a correct manner (see discussion there)\n      parentArray.refresh(indexReader);\n\n      // Remove any INVALID_ORDINAL values from the ordinal cache,\n      // because it is possible those are now answered by the new data!\n      Iterator<Entry<String, Integer>> i = ordinalCache.entrySet().iterator();\n      while (i.hasNext()) {\n        Entry<String, Integer> e = i.next();\n        if (e.getValue().intValue() == INVALID_ORDINAL) {\n          i.remove();\n        }\n      }\n      return true;\n    }\n\n","bugFix":["24315bf90acffc79fa6d5fb325f6c1cf9fc9642b","233afcf63b8d53faa9a7993e911cc9873b0106d1","89f15687f60bd49cd3d9de427e85c17fd9397d61","438e995b4e32916f631722aab36254146830fefb","4093b270ba337f9c25a4c0e6cb2ae2c07f697376"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"219dcddcdf2fc13f6271d9e5836bd19c53a4abf1","date":1353511594,"type":4,"author":"Shai Erera","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader#refresh().mjava","sourceNew":null,"sourceOld":"  // Note that refresh() is synchronized (it is the only synchronized\n  // method in this class) to ensure that it never gets called concurrently\n  // with itself.\n  @Override\n  public synchronized boolean refresh() throws IOException, InconsistentTaxonomyException {\n    ensureOpen();\n    /*\n     * Since refresh() can be a lengthy operation, it is very important that we\n     * avoid locking out all readers for its duration. This is why we don't hold\n     * the indexReaderLock write lock for the entire duration of this method. In\n     * fact, it is enough to hold it only during a single assignment! Other\n     * comments in this method will explain this.\n     */\n\n    // note that the lengthy operation indexReader.reopen() does not\n    // modify the reader, so we can do it without holding a lock. We can\n    // safely read indexReader without holding the write lock, because\n    // no other thread can be writing at this time (this method is the\n    // only possible writer, and it is \"synchronized\" to avoid this case).\n    DirectoryReader r2 = DirectoryReader.openIfChanged(indexReader);\n    if (r2 == null) {\n      return false; // no changes, nothing to do\n    }\n    \n    // validate that a refresh is valid at this point, i.e. that the taxonomy \n    // was not recreated since this reader was last opened or refresshed.\n    String t1 = indexReader.getIndexCommit().getUserData().get(DirectoryTaxonomyWriter.INDEX_EPOCH);\n    String t2 = r2.getIndexCommit().getUserData().get(DirectoryTaxonomyWriter.INDEX_EPOCH);\n    if (t1 == null) {\n      if (t2 != null) {\n        r2.close();\n        throw new InconsistentTaxonomyException(\"Taxonomy was recreated, epoch= \" + t2);\n      }\n    } else if (!t1.equals(t2)) {\n      // t1 != null and t2 cannot be null b/c DirTaxoWriter always puts the commit data.\n      // it's ok to use String.equals because we require the two epoch values to be the same.\n      r2.close();\n      throw new InconsistentTaxonomyException(\"Taxonomy was recreated epoch = \" + t2 + \"  !=  \" + t1);\n    }\n    \n    IndexReader oldreader = indexReader;\n    // we can close the old searcher, but need to synchronize this\n    // so that we don't close it in the middle that another routine\n    // is reading from it.\n    indexReaderLock.writeLock().lock();\n    indexReader = r2;\n    indexReaderLock.writeLock().unlock();\n    // We can close the old reader, but need to be certain that we\n    // don't close it while another method is reading from it.\n    // Luckily, we can be certain of that even without putting the\n    // oldreader.close() in the locked section. The reason is that\n    // after lock() succeeded above, we know that all existing readers\n    // had finished (this is what a read-write lock ensures). New\n    // readers, starting after the unlock() we just did, already got\n    // the new indexReader we set above. So nobody can be possibly\n    // using the old indexReader, and we can close it:\n    oldreader.close();\n    \n    // We prefetch some of the arrays to make requests much faster.\n    // Let's refresh these prefetched arrays; This refresh is much\n    // is made more efficient by assuming that it is enough to read\n    // the values for new categories (old categories could not have been\n    // changed or deleted)\n    // Note that this this done without the write lock being held,\n    // which means that it is possible that during a refresh(), a\n    // reader will have some methods (like getOrdinal and getCategory)\n    // return fresh information, while getParent()\n    // (only to be prefetched now) still return older information.\n    // We consider this to be acceptable. The important thing,\n    // however, is that refreshPrefetchArrays() itself writes to\n    // the arrays in a correct manner (see discussion there)\n    parentArray.refresh(indexReader);\n    \n    // Remove any INVALID_ORDINAL values from the ordinal cache,\n    // because it is possible those are now answered by the new data!\n    Iterator<Entry<String, Integer>> i = ordinalCache.entrySet().iterator();\n    while (i.hasNext()) {\n      Entry<String, Integer> e = i.next();\n      if (e.getValue().intValue() == INVALID_ORDINAL) {\n        i.remove();\n      }\n    }\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"407687e67faf6e1f02a211ca078d8e3eed631027","date":1355157407,"type":4,"author":"Robert Muir","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader#refresh().mjava","sourceNew":null,"sourceOld":"  // Note that refresh() is synchronized (it is the only synchronized\n  // method in this class) to ensure that it never gets called concurrently\n  // with itself.\n  @Override\n  public synchronized boolean refresh() throws IOException, InconsistentTaxonomyException {\n    ensureOpen();\n    /*\n     * Since refresh() can be a lengthy operation, it is very important that we\n     * avoid locking out all readers for its duration. This is why we don't hold\n     * the indexReaderLock write lock for the entire duration of this method. In\n     * fact, it is enough to hold it only during a single assignment! Other\n     * comments in this method will explain this.\n     */\n\n    // note that the lengthy operation indexReader.reopen() does not\n    // modify the reader, so we can do it without holding a lock. We can\n    // safely read indexReader without holding the write lock, because\n    // no other thread can be writing at this time (this method is the\n    // only possible writer, and it is \"synchronized\" to avoid this case).\n    DirectoryReader r2 = DirectoryReader.openIfChanged(indexReader);\n    if (r2 == null) {\n      return false; // no changes, nothing to do\n    }\n    \n    // validate that a refresh is valid at this point, i.e. that the taxonomy \n    // was not recreated since this reader was last opened or refresshed.\n    String t1 = indexReader.getIndexCommit().getUserData().get(DirectoryTaxonomyWriter.INDEX_EPOCH);\n    String t2 = r2.getIndexCommit().getUserData().get(DirectoryTaxonomyWriter.INDEX_EPOCH);\n    if (t1 == null) {\n      if (t2 != null) {\n        r2.close();\n        throw new InconsistentTaxonomyException(\"Taxonomy was recreated, epoch= \" + t2);\n      }\n    } else if (!t1.equals(t2)) {\n      // t1 != null and t2 cannot be null b/c DirTaxoWriter always puts the commit data.\n      // it's ok to use String.equals because we require the two epoch values to be the same.\n      r2.close();\n      throw new InconsistentTaxonomyException(\"Taxonomy was recreated epoch = \" + t2 + \"  !=  \" + t1);\n    }\n    \n    IndexReader oldreader = indexReader;\n    // we can close the old searcher, but need to synchronize this\n    // so that we don't close it in the middle that another routine\n    // is reading from it.\n    indexReaderLock.writeLock().lock();\n    indexReader = r2;\n    indexReaderLock.writeLock().unlock();\n    // We can close the old reader, but need to be certain that we\n    // don't close it while another method is reading from it.\n    // Luckily, we can be certain of that even without putting the\n    // oldreader.close() in the locked section. The reason is that\n    // after lock() succeeded above, we know that all existing readers\n    // had finished (this is what a read-write lock ensures). New\n    // readers, starting after the unlock() we just did, already got\n    // the new indexReader we set above. So nobody can be possibly\n    // using the old indexReader, and we can close it:\n    oldreader.close();\n    \n    // We prefetch some of the arrays to make requests much faster.\n    // Let's refresh these prefetched arrays; This refresh is much\n    // is made more efficient by assuming that it is enough to read\n    // the values for new categories (old categories could not have been\n    // changed or deleted)\n    // Note that this this done without the write lock being held,\n    // which means that it is possible that during a refresh(), a\n    // reader will have some methods (like getOrdinal and getCategory)\n    // return fresh information, while getParent()\n    // (only to be prefetched now) still return older information.\n    // We consider this to be acceptable. The important thing,\n    // however, is that refreshPrefetchArrays() itself writes to\n    // the arrays in a correct manner (see discussion there)\n    parentArray.refresh(indexReader);\n    \n    // Remove any INVALID_ORDINAL values from the ordinal cache,\n    // because it is possible those are now answered by the new data!\n    Iterator<Entry<String, Integer>> i = ordinalCache.entrySet().iterator();\n    while (i.hasNext()) {\n      Entry<String, Integer> e = i.next();\n      if (e.getValue().intValue() == INVALID_ORDINAL) {\n        i.remove();\n      }\n    }\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"78e3613d9274c0d98ca67d976e415c82e9f9cf46":["4093b270ba337f9c25a4c0e6cb2ae2c07f697376"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"407687e67faf6e1f02a211ca078d8e3eed631027":["78e3613d9274c0d98ca67d976e415c82e9f9cf46","219dcddcdf2fc13f6271d9e5836bd19c53a4abf1"],"219dcddcdf2fc13f6271d9e5836bd19c53a4abf1":["78e3613d9274c0d98ca67d976e415c82e9f9cf46"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["219dcddcdf2fc13f6271d9e5836bd19c53a4abf1"],"4093b270ba337f9c25a4c0e6cb2ae2c07f697376":["b89678825b68eccaf09e6ab71675fc0b0af1e099"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["4093b270ba337f9c25a4c0e6cb2ae2c07f697376"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"78e3613d9274c0d98ca67d976e415c82e9f9cf46":["407687e67faf6e1f02a211ca078d8e3eed631027","219dcddcdf2fc13f6271d9e5836bd19c53a4abf1"],"407687e67faf6e1f02a211ca078d8e3eed631027":[],"219dcddcdf2fc13f6271d9e5836bd19c53a4abf1":["407687e67faf6e1f02a211ca078d8e3eed631027","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4093b270ba337f9c25a4c0e6cb2ae2c07f697376":["78e3613d9274c0d98ca67d976e415c82e9f9cf46"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["407687e67faf6e1f02a211ca078d8e3eed631027","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}