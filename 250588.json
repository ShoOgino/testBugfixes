{"path":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#doTestSortedNumericsVsStoredFields(LongProducer,LongProducer).mjava","commits":[{"id":"d14ffaac9c4a4a2c750bf0cd956506802561e062","date":1402602036,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#doTestSortedNumericsVsStoredFields(LongProducer,LongProducer).mjava","pathOld":"/dev/null","sourceNew":"  private void doTestSortedNumericsVsStoredFields(LongProducer counts, LongProducer values) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    \n    // index some docs\n    int numDocs = atLeast(300);\n    // numDocs should be always > 256 so that in case of a codec that optimizes\n    // for numbers of values <= 256, all storage layouts are tested\n    assert numDocs > 256;\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Field.Store.NO));\n      \n      int valueCount = (int) counts.next();\n      long valueArray[] = new long[valueCount];\n      for (int j = 0; j < valueCount; j++) {\n        long value = values.next();\n        valueArray[j] = value;\n        doc.add(new SortedNumericDocValuesField(\"dv\", value));\n      }\n      Arrays.sort(valueArray);\n      for (int j = 0; j < valueCount; j++) {\n        doc.add(new StoredField(\"stored\", Long.toString(valueArray[j])));\n      }\n      writer.addDocument(doc);\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/10);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n\n    // merge some segments and ensure that at least one of them has more than\n    // 256 values\n    writer.forceMerge(numDocs / 256);\n\n    writer.shutdown();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : ir.leaves()) {\n      AtomicReader r = context.reader();\n      SortedNumericDocValues docValues = DocValues.getSortedNumeric(r, \"dv\");\n      for (int i = 0; i < r.maxDoc(); i++) {\n        String expected[] = r.document(i).getValues(\"stored\");\n        docValues.setDocument(i);\n        String actual[] = new String[docValues.count()];\n        for (int j = 0; j < actual.length; j++) {\n          actual[j] = Long.toString(docValues.valueAt(j));\n        }\n        assertArrayEquals(expected, actual);\n      }\n    }\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c6f080a2ab37c464dd98db173f6cbf10dc74f211","date":1402946779,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#doTestSortedNumericsVsStoredFields(LongProducer,LongProducer).mjava","pathOld":"/dev/null","sourceNew":"  private void doTestSortedNumericsVsStoredFields(LongProducer counts, LongProducer values) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    \n    // index some docs\n    int numDocs = atLeast(300);\n    // numDocs should be always > 256 so that in case of a codec that optimizes\n    // for numbers of values <= 256, all storage layouts are tested\n    assert numDocs > 256;\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Field.Store.NO));\n      \n      int valueCount = (int) counts.next();\n      long valueArray[] = new long[valueCount];\n      for (int j = 0; j < valueCount; j++) {\n        long value = values.next();\n        valueArray[j] = value;\n        doc.add(new SortedNumericDocValuesField(\"dv\", value));\n      }\n      Arrays.sort(valueArray);\n      for (int j = 0; j < valueCount; j++) {\n        doc.add(new StoredField(\"stored\", Long.toString(valueArray[j])));\n      }\n      writer.addDocument(doc);\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/10);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n\n    // merge some segments and ensure that at least one of them has more than\n    // 256 values\n    writer.forceMerge(numDocs / 256);\n\n    writer.shutdown();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : ir.leaves()) {\n      AtomicReader r = context.reader();\n      SortedNumericDocValues docValues = DocValues.getSortedNumeric(r, \"dv\");\n      for (int i = 0; i < r.maxDoc(); i++) {\n        String expected[] = r.document(i).getValues(\"stored\");\n        docValues.setDocument(i);\n        String actual[] = new String[docValues.count()];\n        for (int j = 0; j < actual.length; j++) {\n          actual[j] = Long.toString(docValues.valueAt(j));\n        }\n        assertArrayEquals(expected, actual);\n      }\n    }\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#doTestSortedNumericsVsStoredFields(LongProducer,LongProducer).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#doTestSortedNumericsVsStoredFields(LongProducer,LongProducer).mjava","sourceNew":"  private void doTestSortedNumericsVsStoredFields(LongProducer counts, LongProducer values) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    \n    // index some docs\n    int numDocs = atLeast(300);\n    // numDocs should be always > 256 so that in case of a codec that optimizes\n    // for numbers of values <= 256, all storage layouts are tested\n    assert numDocs > 256;\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Field.Store.NO));\n      \n      int valueCount = (int) counts.next();\n      long valueArray[] = new long[valueCount];\n      for (int j = 0; j < valueCount; j++) {\n        long value = values.next();\n        valueArray[j] = value;\n        doc.add(new SortedNumericDocValuesField(\"dv\", value));\n      }\n      Arrays.sort(valueArray);\n      for (int j = 0; j < valueCount; j++) {\n        doc.add(new StoredField(\"stored\", Long.toString(valueArray[j])));\n      }\n      writer.addDocument(doc);\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/10);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n\n    // merge some segments and ensure that at least one of them has more than\n    // 256 values\n    writer.forceMerge(numDocs / 256);\n\n    writer.shutdown();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : ir.leaves()) {\n      AtomicReader r = context.reader();\n      SortedNumericDocValues docValues = DocValues.getSortedNumeric(r, \"dv\");\n      for (int i = 0; i < r.maxDoc(); i++) {\n        String expected[] = r.document(i).getValues(\"stored\");\n        docValues.setDocument(i);\n        String actual[] = new String[docValues.count()];\n        for (int j = 0; j < actual.length; j++) {\n          actual[j] = Long.toString(docValues.valueAt(j));\n        }\n        assertArrayEquals(expected, actual);\n      }\n    }\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void doTestSortedNumericsVsStoredFields(LongProducer counts, LongProducer values) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    \n    // index some docs\n    int numDocs = atLeast(300);\n    // numDocs should be always > 256 so that in case of a codec that optimizes\n    // for numbers of values <= 256, all storage layouts are tested\n    assert numDocs > 256;\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Field.Store.NO));\n      \n      int valueCount = (int) counts.next();\n      long valueArray[] = new long[valueCount];\n      for (int j = 0; j < valueCount; j++) {\n        long value = values.next();\n        valueArray[j] = value;\n        doc.add(new SortedNumericDocValuesField(\"dv\", value));\n      }\n      Arrays.sort(valueArray);\n      for (int j = 0; j < valueCount; j++) {\n        doc.add(new StoredField(\"stored\", Long.toString(valueArray[j])));\n      }\n      writer.addDocument(doc);\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/10);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n\n    // merge some segments and ensure that at least one of them has more than\n    // 256 values\n    writer.forceMerge(numDocs / 256);\n\n    writer.shutdown();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : ir.leaves()) {\n      AtomicReader r = context.reader();\n      SortedNumericDocValues docValues = DocValues.getSortedNumeric(r, \"dv\");\n      for (int i = 0; i < r.maxDoc(); i++) {\n        String expected[] = r.document(i).getValues(\"stored\");\n        docValues.setDocument(i);\n        String actual[] = new String[docValues.count()];\n        for (int j = 0; j < actual.length; j++) {\n          actual[j] = Long.toString(docValues.valueAt(j));\n        }\n        assertArrayEquals(expected, actual);\n      }\n    }\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#doTestSortedNumericsVsStoredFields(LongProducer,LongProducer).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#doTestSortedNumericsVsStoredFields(LongProducer,LongProducer).mjava","sourceNew":"  private void doTestSortedNumericsVsStoredFields(LongProducer counts, LongProducer values) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    \n    // index some docs\n    int numDocs = atLeast(300);\n    // numDocs should be always > 256 so that in case of a codec that optimizes\n    // for numbers of values <= 256, all storage layouts are tested\n    assert numDocs > 256;\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Field.Store.NO));\n      \n      int valueCount = (int) counts.next();\n      long valueArray[] = new long[valueCount];\n      for (int j = 0; j < valueCount; j++) {\n        long value = values.next();\n        valueArray[j] = value;\n        doc.add(new SortedNumericDocValuesField(\"dv\", value));\n      }\n      Arrays.sort(valueArray);\n      for (int j = 0; j < valueCount; j++) {\n        doc.add(new StoredField(\"stored\", Long.toString(valueArray[j])));\n      }\n      writer.addDocument(doc);\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/10);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n\n    // merge some segments and ensure that at least one of them has more than\n    // 256 values\n    writer.forceMerge(numDocs / 256);\n\n    writer.close();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : ir.leaves()) {\n      AtomicReader r = context.reader();\n      SortedNumericDocValues docValues = DocValues.getSortedNumeric(r, \"dv\");\n      for (int i = 0; i < r.maxDoc(); i++) {\n        String expected[] = r.document(i).getValues(\"stored\");\n        docValues.setDocument(i);\n        String actual[] = new String[docValues.count()];\n        for (int j = 0; j < actual.length; j++) {\n          actual[j] = Long.toString(docValues.valueAt(j));\n        }\n        assertArrayEquals(expected, actual);\n      }\n    }\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void doTestSortedNumericsVsStoredFields(LongProducer counts, LongProducer values) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    \n    // index some docs\n    int numDocs = atLeast(300);\n    // numDocs should be always > 256 so that in case of a codec that optimizes\n    // for numbers of values <= 256, all storage layouts are tested\n    assert numDocs > 256;\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Field.Store.NO));\n      \n      int valueCount = (int) counts.next();\n      long valueArray[] = new long[valueCount];\n      for (int j = 0; j < valueCount; j++) {\n        long value = values.next();\n        valueArray[j] = value;\n        doc.add(new SortedNumericDocValuesField(\"dv\", value));\n      }\n      Arrays.sort(valueArray);\n      for (int j = 0; j < valueCount; j++) {\n        doc.add(new StoredField(\"stored\", Long.toString(valueArray[j])));\n      }\n      writer.addDocument(doc);\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/10);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n\n    // merge some segments and ensure that at least one of them has more than\n    // 256 values\n    writer.forceMerge(numDocs / 256);\n\n    writer.shutdown();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : ir.leaves()) {\n      AtomicReader r = context.reader();\n      SortedNumericDocValues docValues = DocValues.getSortedNumeric(r, \"dv\");\n      for (int i = 0; i < r.maxDoc(); i++) {\n        String expected[] = r.document(i).getValues(\"stored\");\n        docValues.setDocument(i);\n        String actual[] = new String[docValues.count()];\n        for (int j = 0; j < actual.length; j++) {\n          actual[j] = Long.toString(docValues.valueAt(j));\n        }\n        assertArrayEquals(expected, actual);\n      }\n    }\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#doTestSortedNumericsVsStoredFields(LongProducer,LongProducer).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#doTestSortedNumericsVsStoredFields(LongProducer,LongProducer).mjava","sourceNew":"  private void doTestSortedNumericsVsStoredFields(LongProducer counts, LongProducer values) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    \n    // index some docs\n    int numDocs = atLeast(300);\n    // numDocs should be always > 256 so that in case of a codec that optimizes\n    // for numbers of values <= 256, all storage layouts are tested\n    assert numDocs > 256;\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Field.Store.NO));\n      \n      int valueCount = (int) counts.next();\n      long valueArray[] = new long[valueCount];\n      for (int j = 0; j < valueCount; j++) {\n        long value = values.next();\n        valueArray[j] = value;\n        doc.add(new SortedNumericDocValuesField(\"dv\", value));\n      }\n      Arrays.sort(valueArray);\n      for (int j = 0; j < valueCount; j++) {\n        doc.add(new StoredField(\"stored\", Long.toString(valueArray[j])));\n      }\n      writer.addDocument(doc);\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/10);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n\n    // merge some segments and ensure that at least one of them has more than\n    // 256 values\n    writer.forceMerge(numDocs / 256);\n\n    writer.close();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    for (LeafReaderContext context : ir.leaves()) {\n      LeafReader r = context.reader();\n      SortedNumericDocValues docValues = DocValues.getSortedNumeric(r, \"dv\");\n      for (int i = 0; i < r.maxDoc(); i++) {\n        String expected[] = r.document(i).getValues(\"stored\");\n        docValues.setDocument(i);\n        String actual[] = new String[docValues.count()];\n        for (int j = 0; j < actual.length; j++) {\n          actual[j] = Long.toString(docValues.valueAt(j));\n        }\n        assertArrayEquals(expected, actual);\n      }\n    }\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void doTestSortedNumericsVsStoredFields(LongProducer counts, LongProducer values) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    \n    // index some docs\n    int numDocs = atLeast(300);\n    // numDocs should be always > 256 so that in case of a codec that optimizes\n    // for numbers of values <= 256, all storage layouts are tested\n    assert numDocs > 256;\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Field.Store.NO));\n      \n      int valueCount = (int) counts.next();\n      long valueArray[] = new long[valueCount];\n      for (int j = 0; j < valueCount; j++) {\n        long value = values.next();\n        valueArray[j] = value;\n        doc.add(new SortedNumericDocValuesField(\"dv\", value));\n      }\n      Arrays.sort(valueArray);\n      for (int j = 0; j < valueCount; j++) {\n        doc.add(new StoredField(\"stored\", Long.toString(valueArray[j])));\n      }\n      writer.addDocument(doc);\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/10);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n\n    // merge some segments and ensure that at least one of them has more than\n    // 256 values\n    writer.forceMerge(numDocs / 256);\n\n    writer.close();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : ir.leaves()) {\n      AtomicReader r = context.reader();\n      SortedNumericDocValues docValues = DocValues.getSortedNumeric(r, \"dv\");\n      for (int i = 0; i < r.maxDoc(); i++) {\n        String expected[] = r.document(i).getValues(\"stored\");\n        docValues.setDocument(i);\n        String actual[] = new String[docValues.count()];\n        for (int j = 0; j < actual.length; j++) {\n          actual[j] = Long.toString(docValues.valueAt(j));\n        }\n        assertArrayEquals(expected, actual);\n      }\n    }\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6652c74b2358a0b13223817a6a793bf1c9d0749d","date":1474465301,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#doTestSortedNumericsVsStoredFields(LongProducer,LongProducer).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#doTestSortedNumericsVsStoredFields(LongProducer,LongProducer).mjava","sourceNew":"  private void doTestSortedNumericsVsStoredFields(LongProducer counts, LongProducer values) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    \n    // index some docs\n    int numDocs = atLeast(300);\n    // numDocs should be always > 256 so that in case of a codec that optimizes\n    // for numbers of values <= 256, all storage layouts are tested\n    assert numDocs > 256;\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Field.Store.NO));\n      \n      int valueCount = (int) counts.next();\n      long valueArray[] = new long[valueCount];\n      for (int j = 0; j < valueCount; j++) {\n        long value = values.next();\n        valueArray[j] = value;\n        doc.add(new SortedNumericDocValuesField(\"dv\", value));\n      }\n      Arrays.sort(valueArray);\n      for (int j = 0; j < valueCount; j++) {\n        doc.add(new StoredField(\"stored\", Long.toString(valueArray[j])));\n      }\n      writer.addDocument(doc);\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/10);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n\n    // merge some segments and ensure that at least one of them has more than\n    // 256 values\n    writer.forceMerge(numDocs / 256);\n\n    writer.close();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    for (LeafReaderContext context : ir.leaves()) {\n      LeafReader r = context.reader();\n      SortedNumericDocValues docValues = DocValues.getSortedNumeric(r, \"dv\");\n      for (int i = 0; i < r.maxDoc(); i++) {\n        if (i > docValues.docID()) {\n          docValues.nextDoc();\n        }\n        String expected[] = r.document(i).getValues(\"stored\");\n        if (i < docValues.docID()) {\n          assertEquals(0, expected.length);\n        } else {\n          String actual[] = new String[docValues.docValueCount()];\n          for (int j = 0; j < actual.length; j++) {\n            actual[j] = Long.toString(docValues.nextValue());\n          }\n          assertArrayEquals(expected, actual);\n        }\n      }\n    }\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void doTestSortedNumericsVsStoredFields(LongProducer counts, LongProducer values) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    \n    // index some docs\n    int numDocs = atLeast(300);\n    // numDocs should be always > 256 so that in case of a codec that optimizes\n    // for numbers of values <= 256, all storage layouts are tested\n    assert numDocs > 256;\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Field.Store.NO));\n      \n      int valueCount = (int) counts.next();\n      long valueArray[] = new long[valueCount];\n      for (int j = 0; j < valueCount; j++) {\n        long value = values.next();\n        valueArray[j] = value;\n        doc.add(new SortedNumericDocValuesField(\"dv\", value));\n      }\n      Arrays.sort(valueArray);\n      for (int j = 0; j < valueCount; j++) {\n        doc.add(new StoredField(\"stored\", Long.toString(valueArray[j])));\n      }\n      writer.addDocument(doc);\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/10);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n\n    // merge some segments and ensure that at least one of them has more than\n    // 256 values\n    writer.forceMerge(numDocs / 256);\n\n    writer.close();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    for (LeafReaderContext context : ir.leaves()) {\n      LeafReader r = context.reader();\n      SortedNumericDocValues docValues = DocValues.getSortedNumeric(r, \"dv\");\n      for (int i = 0; i < r.maxDoc(); i++) {\n        String expected[] = r.document(i).getValues(\"stored\");\n        docValues.setDocument(i);\n        String actual[] = new String[docValues.count()];\n        for (int j = 0; j < actual.length; j++) {\n          actual[j] = Long.toString(docValues.valueAt(j));\n        }\n        assertArrayEquals(expected, actual);\n      }\n    }\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#doTestSortedNumericsVsStoredFields(LongProducer,LongProducer).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#doTestSortedNumericsVsStoredFields(LongProducer,LongProducer).mjava","sourceNew":"  private void doTestSortedNumericsVsStoredFields(LongProducer counts, LongProducer values) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    \n    // index some docs\n    int numDocs = atLeast(300);\n    // numDocs should be always > 256 so that in case of a codec that optimizes\n    // for numbers of values <= 256, all storage layouts are tested\n    assert numDocs > 256;\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Field.Store.NO));\n      \n      int valueCount = (int) counts.next();\n      long valueArray[] = new long[valueCount];\n      for (int j = 0; j < valueCount; j++) {\n        long value = values.next();\n        valueArray[j] = value;\n        doc.add(new SortedNumericDocValuesField(\"dv\", value));\n      }\n      Arrays.sort(valueArray);\n      for (int j = 0; j < valueCount; j++) {\n        doc.add(new StoredField(\"stored\", Long.toString(valueArray[j])));\n      }\n      writer.addDocument(doc);\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/10);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n\n    // merge some segments and ensure that at least one of them has more than\n    // 256 values\n    writer.forceMerge(numDocs / 256);\n\n    writer.close();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    for (LeafReaderContext context : ir.leaves()) {\n      LeafReader r = context.reader();\n      SortedNumericDocValues docValues = DocValues.getSortedNumeric(r, \"dv\");\n      for (int i = 0; i < r.maxDoc(); i++) {\n        if (i > docValues.docID()) {\n          docValues.nextDoc();\n        }\n        String expected[] = r.document(i).getValues(\"stored\");\n        if (i < docValues.docID()) {\n          assertEquals(0, expected.length);\n        } else {\n          String actual[] = new String[docValues.docValueCount()];\n          for (int j = 0; j < actual.length; j++) {\n            actual[j] = Long.toString(docValues.nextValue());\n          }\n          assertArrayEquals(expected, actual);\n        }\n      }\n    }\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void doTestSortedNumericsVsStoredFields(LongProducer counts, LongProducer values) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    \n    // index some docs\n    int numDocs = atLeast(300);\n    // numDocs should be always > 256 so that in case of a codec that optimizes\n    // for numbers of values <= 256, all storage layouts are tested\n    assert numDocs > 256;\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Field.Store.NO));\n      \n      int valueCount = (int) counts.next();\n      long valueArray[] = new long[valueCount];\n      for (int j = 0; j < valueCount; j++) {\n        long value = values.next();\n        valueArray[j] = value;\n        doc.add(new SortedNumericDocValuesField(\"dv\", value));\n      }\n      Arrays.sort(valueArray);\n      for (int j = 0; j < valueCount; j++) {\n        doc.add(new StoredField(\"stored\", Long.toString(valueArray[j])));\n      }\n      writer.addDocument(doc);\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/10);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n\n    // merge some segments and ensure that at least one of them has more than\n    // 256 values\n    writer.forceMerge(numDocs / 256);\n\n    writer.close();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    for (LeafReaderContext context : ir.leaves()) {\n      LeafReader r = context.reader();\n      SortedNumericDocValues docValues = DocValues.getSortedNumeric(r, \"dv\");\n      for (int i = 0; i < r.maxDoc(); i++) {\n        String expected[] = r.document(i).getValues(\"stored\");\n        docValues.setDocument(i);\n        String actual[] = new String[docValues.count()];\n        for (int j = 0; j < actual.length; j++) {\n          actual[j] = Long.toString(docValues.valueAt(j));\n        }\n        assertArrayEquals(expected, actual);\n      }\n    }\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#doTestSortedNumericsVsStoredFields(LongProducer,LongProducer).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#doTestSortedNumericsVsStoredFields(LongProducer,LongProducer).mjava","sourceNew":"  private void doTestSortedNumericsVsStoredFields(LongProducer counts, LongProducer values) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    \n    // index some docs\n    int numDocs = atLeast(300);\n    // numDocs should be always > 256 so that in case of a codec that optimizes\n    // for numbers of values <= 256, all storage layouts are tested\n    assert numDocs > 256;\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Field.Store.NO));\n      \n      int valueCount = (int) counts.next();\n      long valueArray[] = new long[valueCount];\n      for (int j = 0; j < valueCount; j++) {\n        long value = values.next();\n        valueArray[j] = value;\n        doc.add(new SortedNumericDocValuesField(\"dv\", value));\n      }\n      Arrays.sort(valueArray);\n      for (int j = 0; j < valueCount; j++) {\n        doc.add(new StoredField(\"stored\", Long.toString(valueArray[j])));\n      }\n      writer.addDocument(doc);\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/10);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n\n    // merge some segments and ensure that at least one of them has more than\n    // 256 values\n    writer.forceMerge(numDocs / 256);\n\n    writer.close();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    for (LeafReaderContext context : ir.leaves()) {\n      LeafReader r = context.reader();\n      SortedNumericDocValues docValues = DocValues.getSortedNumeric(r, \"dv\");\n      for (int i = 0; i < r.maxDoc(); i++) {\n        if (i > docValues.docID()) {\n          docValues.nextDoc();\n        }\n        String expected[] = r.document(i).getValues(\"stored\");\n        if (i < docValues.docID()) {\n          assertEquals(0, expected.length);\n        } else {\n          String actual[] = new String[docValues.docValueCount()];\n          for (int j = 0; j < actual.length; j++) {\n            actual[j] = Long.toString(docValues.nextValue());\n          }\n          assertArrayEquals(expected, actual);\n        }\n      }\n    }\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void doTestSortedNumericsVsStoredFields(LongProducer counts, LongProducer values) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    \n    // index some docs\n    int numDocs = atLeast(300);\n    // numDocs should be always > 256 so that in case of a codec that optimizes\n    // for numbers of values <= 256, all storage layouts are tested\n    assert numDocs > 256;\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Field.Store.NO));\n      \n      int valueCount = (int) counts.next();\n      long valueArray[] = new long[valueCount];\n      for (int j = 0; j < valueCount; j++) {\n        long value = values.next();\n        valueArray[j] = value;\n        doc.add(new SortedNumericDocValuesField(\"dv\", value));\n      }\n      Arrays.sort(valueArray);\n      for (int j = 0; j < valueCount; j++) {\n        doc.add(new StoredField(\"stored\", Long.toString(valueArray[j])));\n      }\n      writer.addDocument(doc);\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/10);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n\n    // merge some segments and ensure that at least one of them has more than\n    // 256 values\n    writer.forceMerge(numDocs / 256);\n\n    writer.close();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    for (LeafReaderContext context : ir.leaves()) {\n      LeafReader r = context.reader();\n      SortedNumericDocValues docValues = DocValues.getSortedNumeric(r, \"dv\");\n      for (int i = 0; i < r.maxDoc(); i++) {\n        String expected[] = r.document(i).getValues(\"stored\");\n        docValues.setDocument(i);\n        String actual[] = new String[docValues.count()];\n        for (int j = 0; j < actual.length; j++) {\n          actual[j] = Long.toString(docValues.valueAt(j));\n        }\n        assertArrayEquals(expected, actual);\n      }\n    }\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"11134e449dabe11d6d0ff6a564d84b82cbe93722","date":1477299083,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#doTestSortedNumericsVsStoredFields(LongSupplier,LongSupplier).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#doTestSortedNumericsVsStoredFields(LongProducer,LongProducer).mjava","sourceNew":"  private void doTestSortedNumericsVsStoredFields(LongSupplier counts, LongSupplier values) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    \n    // index some docs\n    int numDocs = atLeast(300);\n    // numDocs should be always > 256 so that in case of a codec that optimizes\n    // for numbers of values <= 256, all storage layouts are tested\n    assert numDocs > 256;\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Field.Store.NO));\n      \n      int valueCount = (int) counts.getAsLong();\n      long valueArray[] = new long[valueCount];\n      for (int j = 0; j < valueCount; j++) {\n        long value = values.getAsLong();\n        valueArray[j] = value;\n        doc.add(new SortedNumericDocValuesField(\"dv\", value));\n      }\n      Arrays.sort(valueArray);\n      for (int j = 0; j < valueCount; j++) {\n        doc.add(new StoredField(\"stored\", Long.toString(valueArray[j])));\n      }\n      writer.addDocument(doc);\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/10);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n\n    // merge some segments and ensure that at least one of them has more than\n    // 256 values\n    writer.forceMerge(numDocs / 256);\n\n    writer.close();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    TestUtil.checkReader(ir);\n    for (LeafReaderContext context : ir.leaves()) {\n      LeafReader r = context.reader();\n      SortedNumericDocValues docValues = DocValues.getSortedNumeric(r, \"dv\");\n      for (int i = 0; i < r.maxDoc(); i++) {\n        if (i > docValues.docID()) {\n          docValues.nextDoc();\n        }\n        String expected[] = r.document(i).getValues(\"stored\");\n        if (i < docValues.docID()) {\n          assertEquals(0, expected.length);\n        } else {\n          String actual[] = new String[docValues.docValueCount()];\n          for (int j = 0; j < actual.length; j++) {\n            actual[j] = Long.toString(docValues.nextValue());\n          }\n          assertArrayEquals(expected, actual);\n        }\n      }\n    }\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void doTestSortedNumericsVsStoredFields(LongProducer counts, LongProducer values) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    \n    // index some docs\n    int numDocs = atLeast(300);\n    // numDocs should be always > 256 so that in case of a codec that optimizes\n    // for numbers of values <= 256, all storage layouts are tested\n    assert numDocs > 256;\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Field.Store.NO));\n      \n      int valueCount = (int) counts.next();\n      long valueArray[] = new long[valueCount];\n      for (int j = 0; j < valueCount; j++) {\n        long value = values.next();\n        valueArray[j] = value;\n        doc.add(new SortedNumericDocValuesField(\"dv\", value));\n      }\n      Arrays.sort(valueArray);\n      for (int j = 0; j < valueCount; j++) {\n        doc.add(new StoredField(\"stored\", Long.toString(valueArray[j])));\n      }\n      writer.addDocument(doc);\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/10);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n\n    // merge some segments and ensure that at least one of them has more than\n    // 256 values\n    writer.forceMerge(numDocs / 256);\n\n    writer.close();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    for (LeafReaderContext context : ir.leaves()) {\n      LeafReader r = context.reader();\n      SortedNumericDocValues docValues = DocValues.getSortedNumeric(r, \"dv\");\n      for (int i = 0; i < r.maxDoc(); i++) {\n        if (i > docValues.docID()) {\n          docValues.nextDoc();\n        }\n        String expected[] = r.document(i).getValues(\"stored\");\n        if (i < docValues.docID()) {\n          assertEquals(0, expected.length);\n        } else {\n          String actual[] = new String[docValues.docValueCount()];\n          for (int j = 0; j < actual.length; j++) {\n            actual[j] = Long.toString(docValues.nextValue());\n          }\n          assertArrayEquals(expected, actual);\n        }\n      }\n    }\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d2714c85633b642b29871cf5ff8d17d3ba7bfd76","date":1477307753,"type":5,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#doTestSortedNumericsVsStoredFields(LongSupplier,LongSupplier).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#doTestSortedNumericsVsStoredFields(LongProducer,LongProducer).mjava","sourceNew":"  private void doTestSortedNumericsVsStoredFields(LongSupplier counts, LongSupplier values) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    \n    // index some docs\n    int numDocs = atLeast(300);\n    // numDocs should be always > 256 so that in case of a codec that optimizes\n    // for numbers of values <= 256, all storage layouts are tested\n    assert numDocs > 256;\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Field.Store.NO));\n      \n      int valueCount = (int) counts.getAsLong();\n      long valueArray[] = new long[valueCount];\n      for (int j = 0; j < valueCount; j++) {\n        long value = values.getAsLong();\n        valueArray[j] = value;\n        doc.add(new SortedNumericDocValuesField(\"dv\", value));\n      }\n      Arrays.sort(valueArray);\n      for (int j = 0; j < valueCount; j++) {\n        doc.add(new StoredField(\"stored\", Long.toString(valueArray[j])));\n      }\n      writer.addDocument(doc);\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/10);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n\n    // merge some segments and ensure that at least one of them has more than\n    // 256 values\n    writer.forceMerge(numDocs / 256);\n\n    writer.close();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    TestUtil.checkReader(ir);\n    for (LeafReaderContext context : ir.leaves()) {\n      LeafReader r = context.reader();\n      SortedNumericDocValues docValues = DocValues.getSortedNumeric(r, \"dv\");\n      for (int i = 0; i < r.maxDoc(); i++) {\n        if (i > docValues.docID()) {\n          docValues.nextDoc();\n        }\n        String expected[] = r.document(i).getValues(\"stored\");\n        if (i < docValues.docID()) {\n          assertEquals(0, expected.length);\n        } else {\n          String actual[] = new String[docValues.docValueCount()];\n          for (int j = 0; j < actual.length; j++) {\n            actual[j] = Long.toString(docValues.nextValue());\n          }\n          assertArrayEquals(expected, actual);\n        }\n      }\n    }\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void doTestSortedNumericsVsStoredFields(LongProducer counts, LongProducer values) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    \n    // index some docs\n    int numDocs = atLeast(300);\n    // numDocs should be always > 256 so that in case of a codec that optimizes\n    // for numbers of values <= 256, all storage layouts are tested\n    assert numDocs > 256;\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Field.Store.NO));\n      \n      int valueCount = (int) counts.next();\n      long valueArray[] = new long[valueCount];\n      for (int j = 0; j < valueCount; j++) {\n        long value = values.next();\n        valueArray[j] = value;\n        doc.add(new SortedNumericDocValuesField(\"dv\", value));\n      }\n      Arrays.sort(valueArray);\n      for (int j = 0; j < valueCount; j++) {\n        doc.add(new StoredField(\"stored\", Long.toString(valueArray[j])));\n      }\n      writer.addDocument(doc);\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/10);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n\n    // merge some segments and ensure that at least one of them has more than\n    // 256 values\n    writer.forceMerge(numDocs / 256);\n\n    writer.close();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    for (LeafReaderContext context : ir.leaves()) {\n      LeafReader r = context.reader();\n      SortedNumericDocValues docValues = DocValues.getSortedNumeric(r, \"dv\");\n      for (int i = 0; i < r.maxDoc(); i++) {\n        if (i > docValues.docID()) {\n          docValues.nextDoc();\n        }\n        String expected[] = r.document(i).getValues(\"stored\");\n        if (i < docValues.docID()) {\n          assertEquals(0, expected.length);\n        } else {\n          String actual[] = new String[docValues.docValueCount()];\n          for (int j = 0; j < actual.length; j++) {\n            actual[j] = Long.toString(docValues.nextValue());\n          }\n          assertArrayEquals(expected, actual);\n        }\n      }\n    }\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"80d0e6d59ae23f4a6f30eaf40bfb40742300287f","date":1477598926,"type":5,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#doTestSortedNumericsVsStoredFields(LongSupplier,LongSupplier).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#doTestSortedNumericsVsStoredFields(LongProducer,LongProducer).mjava","sourceNew":"  private void doTestSortedNumericsVsStoredFields(LongSupplier counts, LongSupplier values) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    \n    // index some docs\n    int numDocs = atLeast(300);\n    // numDocs should be always > 256 so that in case of a codec that optimizes\n    // for numbers of values <= 256, all storage layouts are tested\n    assert numDocs > 256;\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Field.Store.NO));\n      \n      int valueCount = (int) counts.getAsLong();\n      long valueArray[] = new long[valueCount];\n      for (int j = 0; j < valueCount; j++) {\n        long value = values.getAsLong();\n        valueArray[j] = value;\n        doc.add(new SortedNumericDocValuesField(\"dv\", value));\n      }\n      Arrays.sort(valueArray);\n      for (int j = 0; j < valueCount; j++) {\n        doc.add(new StoredField(\"stored\", Long.toString(valueArray[j])));\n      }\n      writer.addDocument(doc);\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/10);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n\n    // merge some segments and ensure that at least one of them has more than\n    // 256 values\n    writer.forceMerge(numDocs / 256);\n\n    writer.close();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    TestUtil.checkReader(ir);\n    for (LeafReaderContext context : ir.leaves()) {\n      LeafReader r = context.reader();\n      SortedNumericDocValues docValues = DocValues.getSortedNumeric(r, \"dv\");\n      for (int i = 0; i < r.maxDoc(); i++) {\n        if (i > docValues.docID()) {\n          docValues.nextDoc();\n        }\n        String expected[] = r.document(i).getValues(\"stored\");\n        if (i < docValues.docID()) {\n          assertEquals(0, expected.length);\n        } else {\n          String actual[] = new String[docValues.docValueCount()];\n          for (int j = 0; j < actual.length; j++) {\n            actual[j] = Long.toString(docValues.nextValue());\n          }\n          assertArrayEquals(expected, actual);\n        }\n      }\n    }\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void doTestSortedNumericsVsStoredFields(LongProducer counts, LongProducer values) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    \n    // index some docs\n    int numDocs = atLeast(300);\n    // numDocs should be always > 256 so that in case of a codec that optimizes\n    // for numbers of values <= 256, all storage layouts are tested\n    assert numDocs > 256;\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Field.Store.NO));\n      \n      int valueCount = (int) counts.next();\n      long valueArray[] = new long[valueCount];\n      for (int j = 0; j < valueCount; j++) {\n        long value = values.next();\n        valueArray[j] = value;\n        doc.add(new SortedNumericDocValuesField(\"dv\", value));\n      }\n      Arrays.sort(valueArray);\n      for (int j = 0; j < valueCount; j++) {\n        doc.add(new StoredField(\"stored\", Long.toString(valueArray[j])));\n      }\n      writer.addDocument(doc);\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/10);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n\n    // merge some segments and ensure that at least one of them has more than\n    // 256 values\n    writer.forceMerge(numDocs / 256);\n\n    writer.close();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    for (LeafReaderContext context : ir.leaves()) {\n      LeafReader r = context.reader();\n      SortedNumericDocValues docValues = DocValues.getSortedNumeric(r, \"dv\");\n      for (int i = 0; i < r.maxDoc(); i++) {\n        if (i > docValues.docID()) {\n          docValues.nextDoc();\n        }\n        String expected[] = r.document(i).getValues(\"stored\");\n        if (i < docValues.docID()) {\n          assertEquals(0, expected.length);\n        } else {\n          String actual[] = new String[docValues.docValueCount()];\n          for (int j = 0; j < actual.length; j++) {\n            actual[j] = Long.toString(docValues.nextValue());\n          }\n          assertArrayEquals(expected, actual);\n        }\n      }\n    }\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d14ffaac9c4a4a2c750bf0cd956506802561e062"],"c6f080a2ab37c464dd98db173f6cbf10dc74f211":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","d14ffaac9c4a4a2c750bf0cd956506802561e062"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["c9fb5f46e264daf5ba3860defe623a89d202dd87","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"11134e449dabe11d6d0ff6a564d84b82cbe93722":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["c9fb5f46e264daf5ba3860defe623a89d202dd87","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d2714c85633b642b29871cf5ff8d17d3ba7bfd76"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"d2714c85633b642b29871cf5ff8d17d3ba7bfd76":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","11134e449dabe11d6d0ff6a564d84b82cbe93722"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d2714c85633b642b29871cf5ff8d17d3ba7bfd76"],"d14ffaac9c4a4a2c750bf0cd956506802561e062":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"c6f080a2ab37c464dd98db173f6cbf10dc74f211":[],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","17e5da53e4e5bd659e22add9bba1cfa222e7e30d","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f"],"11134e449dabe11d6d0ff6a564d84b82cbe93722":["d2714c85633b642b29871cf5ff8d17d3ba7bfd76"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c6f080a2ab37c464dd98db173f6cbf10dc74f211","d14ffaac9c4a4a2c750bf0cd956506802561e062"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","11134e449dabe11d6d0ff6a564d84b82cbe93722","d2714c85633b642b29871cf5ff8d17d3ba7bfd76"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"d2714c85633b642b29871cf5ff8d17d3ba7bfd76":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d14ffaac9c4a4a2c750bf0cd956506802561e062":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","c6f080a2ab37c464dd98db173f6cbf10dc74f211"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c6f080a2ab37c464dd98db173f6cbf10dc74f211","80d0e6d59ae23f4a6f30eaf40bfb40742300287f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}