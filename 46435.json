{"path":"lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","commits":[{"id":"a45bec74b98f6fc05f52770cfb425739e6563960","date":1375119292,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String,int).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount, indexStartFP, indexIn));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix, int indexDivisor)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      if (indexDivisor != -1) {\n        indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n        int indexVersion = readIndexHeader(indexIn);\n        if (indexVersion != version) {\n          throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n        }\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      if (indexDivisor != -1) {\n        seekDir(indexIn, indexDirOffset);\n      }\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexDivisor != -1 ? indexIn.readVLong() : 0;\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount, indexStartFP, indexIn));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      if (indexDivisor != -1) {\n        indexIn.close();\n      }\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","date":1376366778,"type":1,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String,int).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount, indexStartFP, indexIn));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix, int indexDivisor)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      if (indexDivisor != -1) {\n        indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n        int indexVersion = readIndexHeader(indexIn);\n        if (indexVersion != version) {\n          throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n        }\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      if (indexDivisor != -1) {\n        seekDir(indexIn, indexDirOffset);\n      }\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexDivisor != -1 ? indexIn.readVLong() : 0;\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount, indexStartFP, indexIn));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      if (indexDivisor != -1) {\n        indexIn.close();\n      }\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1a88d7b0899b7d22dcbd4cf8ca35d9ec9850ab98","date":1377268487,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount, indexStartFP, longsSize, indexIn));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount, indexStartFP, indexIn));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e3263230f04a1aa8d431d722fdfce583a9542c18","date":1377603209,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTreeTermsWriter.TERMS_VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount, indexStartFP, longsSize, indexIn));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount, indexStartFP, longsSize, indexIn));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2f948dd442d23baa6cbb28daf77c8db78b351329","date":1378742876,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTreeTermsWriter.TERMS_VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount, indexStartFP, longsSize, indexIn));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount, indexStartFP, indexIn));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1f3b037cd083286b2af89f96e768f85dcd8072d6","date":1396337805,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n      }\n      \n      // verify\n      if (version >= BlockTreeTermsWriter.VERSION_CHECKSUM) {\n        CodecUtil.checksumEntireFile(indexIn);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTreeTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount, indexStartFP, longsSize, indexIn));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTreeTermsWriter.TERMS_VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount, indexStartFP, longsSize, indexIn));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5eb2511ababf862ea11e10761c70ee560cd84510","date":1396607225,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n      }\n      \n      // verify\n      if (version >= BlockTreeTermsWriter.VERSION_CHECKSUM) {\n        CodecUtil.checksumEntireFile(indexIn);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTreeTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount, indexStartFP, longsSize, indexIn));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTreeTermsWriter.TERMS_VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount, indexStartFP, longsSize, indexIn));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0974f33be0e2189e71f36b67f1017f4072b1a126","date":1398347867,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n      }\n      \n      // verify\n      if (version >= BlockTreeTermsWriter.VERSION_CHECKSUM) {\n        CodecUtil.checksumEntireFile(indexIn);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTreeTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n\n        BytesRef minTerm, maxTerm;\n        if (version >= BlockTreeTermsWriter.VERSION_MIN_MAX_TERMS) {\n          minTerm = readBytesRef(in);\n          maxTerm = readBytesRef(in);\n        } else {\n          minTerm = maxTerm = null;\n        }\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n      }\n      \n      // verify\n      if (version >= BlockTreeTermsWriter.VERSION_CHECKSUM) {\n        CodecUtil.checksumEntireFile(indexIn);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTreeTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount, indexStartFP, longsSize, indexIn));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["2ac16fe5f6279d582010071b6129ea3073953c15","2ac16fe5f6279d582010071b6129ea3073953c15"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","date":1398844771,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n      }\n      \n      // verify\n      if (version >= BlockTreeTermsWriter.VERSION_CHECKSUM) {\n        CodecUtil.checksumEntireFile(indexIn);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTreeTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n\n        BytesRef minTerm, maxTerm;\n        if (version >= BlockTreeTermsWriter.VERSION_MIN_MAX_TERMS) {\n          minTerm = readBytesRef(in);\n          maxTerm = readBytesRef(in);\n        } else {\n          minTerm = maxTerm = null;\n        }\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n      }\n      \n      // verify\n      if (version >= BlockTreeTermsWriter.VERSION_CHECKSUM) {\n        CodecUtil.checksumEntireFile(indexIn);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTreeTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount, indexStartFP, longsSize, indexIn));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5ad80176d91a6f70fe93880e43dfd697dc4e63ed","date":1400176913,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n      }\n      \n      // verify\n      if (version >= BlockTreeTermsWriter.VERSION_CHECKSUM) {\n        CodecUtil.checksumEntireFile(indexIn);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTreeTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n\n        BytesRef minTerm, maxTerm;\n        if (version >= BlockTreeTermsWriter.VERSION_MIN_MAX_TERMS) {\n          minTerm = readBytesRef(in);\n          maxTerm = readBytesRef(in);\n        } else {\n          minTerm = maxTerm = null;\n        }\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n      }\n      \n      // verify\n      if (version >= BlockTreeTermsWriter.VERSION_CHECKSUM) {\n        CodecUtil.checksumEntireFile(indexIn);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTreeTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n\n        BytesRef minTerm, maxTerm;\n        if (version >= BlockTreeTermsWriter.VERSION_MIN_MAX_TERMS) {\n          minTerm = readBytesRef(in);\n          maxTerm = readBytesRef(in);\n        } else {\n          minTerm = maxTerm = null;\n        }\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4d637064d608752565d4f9f41b2497dfdfdde50e","date":1400798123,"type":5,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n      }\n      \n      // verify\n      if (version >= BlockTreeTermsWriter.VERSION_CHECKSUM) {\n        CodecUtil.checksumEntireFile(indexIn);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTreeTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n\n        BytesRef minTerm, maxTerm;\n        if (version >= BlockTreeTermsWriter.VERSION_MIN_MAX_TERMS) {\n          minTerm = readBytesRef(in);\n          maxTerm = readBytesRef(in);\n        } else {\n          minTerm = maxTerm = null;\n        }\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n      }\n      \n      // verify\n      if (version >= BlockTreeTermsWriter.VERSION_CHECKSUM) {\n        CodecUtil.checksumEntireFile(indexIn);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTreeTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n\n        BytesRef minTerm, maxTerm;\n        if (version >= BlockTreeTermsWriter.VERSION_MIN_MAX_TERMS) {\n          minTerm = readBytesRef(in);\n          maxTerm = readBytesRef(in);\n        } else {\n          minTerm = maxTerm = null;\n        }\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4d637064d608752565d4f9f41b2497dfdfdde50e","date":1400798123,"type":6,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsReader#VersionBlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  /** Sole constructor. */\n  public VersionBlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                                     PostingsReaderBase postingsReader, IOContext ioContext,\n                                     String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, VersionBlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, VersionBlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final long version = in.readVLong();\n        final Pair<BytesRef,Long> rootCode = VersionBlockTreeTermsWriter.FST_OUTPUTS.newPair(code, version);\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = numTerms;\n        final long sumDocFreq = numTerms;\n        assert numTerms <= Integer.MAX_VALUE;\n        final int docCount = (int) numTerms;\n        final int longsSize = in.readVInt();\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexIn.readVLong();\n        VersionFieldReader previous = fields.put(fieldInfo.name,       \n                                                 new VersionFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                        indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion);\n      }\n      \n      // verify\n      if (version >= BlockTreeTermsWriter.VERSION_CHECKSUM) {\n        CodecUtil.checksumEntireFile(indexIn);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final int numBytes = in.readVInt();\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTreeTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n\n        BytesRef minTerm, maxTerm;\n        if (version >= BlockTreeTermsWriter.VERSION_MIN_MAX_TERMS) {\n          minTerm = readBytesRef(in);\n          maxTerm = readBytesRef(in);\n        } else {\n          minTerm = maxTerm = null;\n        }\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"5eb2511ababf862ea11e10761c70ee560cd84510":["2f948dd442d23baa6cbb28daf77c8db78b351329","1f3b037cd083286b2af89f96e768f85dcd8072d6"],"1f3b037cd083286b2af89f96e768f85dcd8072d6":["2f948dd442d23baa6cbb28daf77c8db78b351329"],"5ad80176d91a6f70fe93880e43dfd697dc4e63ed":["0974f33be0e2189e71f36b67f1017f4072b1a126"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":["1f3b037cd083286b2af89f96e768f85dcd8072d6","0974f33be0e2189e71f36b67f1017f4072b1a126"],"a45bec74b98f6fc05f52770cfb425739e6563960":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"e3263230f04a1aa8d431d722fdfce583a9542c18":["1a88d7b0899b7d22dcbd4cf8ca35d9ec9850ab98"],"0974f33be0e2189e71f36b67f1017f4072b1a126":["1f3b037cd083286b2af89f96e768f85dcd8072d6"],"1a88d7b0899b7d22dcbd4cf8ca35d9ec9850ab98":["8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee"],"4d637064d608752565d4f9f41b2497dfdfdde50e":["0974f33be0e2189e71f36b67f1017f4072b1a126","5ad80176d91a6f70fe93880e43dfd697dc4e63ed"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"2f948dd442d23baa6cbb28daf77c8db78b351329":["a45bec74b98f6fc05f52770cfb425739e6563960","e3263230f04a1aa8d431d722fdfce583a9542c18"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["4d637064d608752565d4f9f41b2497dfdfdde50e"]},"commit2Childs":{"5eb2511ababf862ea11e10761c70ee560cd84510":[],"1f3b037cd083286b2af89f96e768f85dcd8072d6":["5eb2511ababf862ea11e10761c70ee560cd84510","c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","0974f33be0e2189e71f36b67f1017f4072b1a126"],"5ad80176d91a6f70fe93880e43dfd697dc4e63ed":["4d637064d608752565d4f9f41b2497dfdfdde50e"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":[],"a45bec74b98f6fc05f52770cfb425739e6563960":["2f948dd442d23baa6cbb28daf77c8db78b351329"],"e3263230f04a1aa8d431d722fdfce583a9542c18":["2f948dd442d23baa6cbb28daf77c8db78b351329"],"0974f33be0e2189e71f36b67f1017f4072b1a126":["5ad80176d91a6f70fe93880e43dfd697dc4e63ed","c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","4d637064d608752565d4f9f41b2497dfdfdde50e"],"1a88d7b0899b7d22dcbd4cf8ca35d9ec9850ab98":["e3263230f04a1aa8d431d722fdfce583a9542c18"],"4d637064d608752565d4f9f41b2497dfdfdde50e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a45bec74b98f6fc05f52770cfb425739e6563960","8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["1a88d7b0899b7d22dcbd4cf8ca35d9ec9850ab98"],"2f948dd442d23baa6cbb28daf77c8db78b351329":["5eb2511ababf862ea11e10761c70ee560cd84510","1f3b037cd083286b2af89f96e768f85dcd8072d6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5eb2511ababf862ea11e10761c70ee560cd84510","c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}