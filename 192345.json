{"path":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.fieldType().indexed() && doInvert) {\n\n        if (i > 0) {\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            fieldState.position += posIncr;\n            if (fieldState.position > 0) {\n              fieldState.position--;\n            }\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n        } finally {\n          stream.close();\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.fieldType().indexed() && doInvert) {\n\n        if (i > 0) {\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            fieldState.position += posIncr;\n            if (fieldState.position > 0) {\n              fieldState.position--;\n            }\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n        } finally {\n          stream.close();\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d1336abe0899b2984e5652903556c1925fbdca9f","date":1329580100,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (fieldType.indexed() && doInvert) {\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n\n        if (i > 0) {\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            fieldState.position += posIncr;\n            if (fieldState.position > 0) {\n              fieldState.position--;\n            }\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n        } finally {\n          stream.close();\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.fieldType().indexed() && doInvert) {\n\n        if (i > 0) {\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            fieldState.position += posIncr;\n            if (fieldState.position > 0) {\n              fieldState.position--;\n            }\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n        } finally {\n          stream.close();\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":["6eb141f80638abdb6ffaa5149877f36ea39b6ad5"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0e748132a1ca480bd503ee795aee58f5765809a6","date":1331838761,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (fieldType.indexed() && doInvert) {\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n\n        if (i > 0) {\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            int position = fieldState.position + posIncr;\n            if (position > 0) {\n              position--;\n            } else if (position < 0) {\n              throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n            }\n            \n            // position is legal, we can safely place it in fieldState now.\n            // not sure if anything will use fieldState after non-aborting exc...\n            fieldState.position = position;\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n        } finally {\n          stream.close();\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (fieldType.indexed() && doInvert) {\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n\n        if (i > 0) {\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            fieldState.position += posIncr;\n            if (fieldState.position > 0) {\n              fieldState.position--;\n            }\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n        } finally {\n          stream.close();\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":["8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5"],"bugIntro":["a7c7a5405c388fd86e5962126be8ad09283eb5cc"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c5f000280bc18391509bbb40c4a2a2c7515d54d3","date":1339339354,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (fieldType.indexed() && doInvert) {\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n\n        if (i > 0) {\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \")\");\n            }\n            if (fieldState.position == 0 && posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0)\");\n            }\n            int position = fieldState.position + posIncr;\n            if (position > 0) {\n              // NOTE: confusing: this \"mirrors\" the\n              // position++ we do below\n              position--;\n            } else if (position < 0) {\n              throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n            }\n            \n            // position is legal, we can safely place it in fieldState now.\n            // not sure if anything will use fieldState after non-aborting exc...\n            fieldState.position = position;\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (fieldType.indexed() && doInvert) {\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n\n        if (i > 0) {\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            int position = fieldState.position + posIncr;\n            if (position > 0) {\n              position--;\n            } else if (position < 0) {\n              throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n            }\n            \n            // position is legal, we can safely place it in fieldState now.\n            // not sure if anything will use fieldState after non-aborting exc...\n            fieldState.position = position;\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n        } finally {\n          stream.close();\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":["a7c7a5405c388fd86e5962126be8ad09283eb5cc","782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cd65a3c65e7917a381c935b0b663d8e783bd9a1e","date":1339372221,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (fieldType.indexed() && doInvert) {\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \")\");\n            }\n            if (fieldState.position == 0 && posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0)\");\n            }\n            int position = fieldState.position + posIncr;\n            if (position > 0) {\n              // NOTE: confusing: this \"mirrors\" the\n              // position++ we do below\n              position--;\n            } else if (position < 0) {\n              throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n            }\n            \n            // position is legal, we can safely place it in fieldState now.\n            // not sure if anything will use fieldState after non-aborting exc...\n            fieldState.position = position;\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n            \n            if (checkOffsets) {\n              int startOffset = fieldState.offset + offsetAttribute.startOffset();\n              int endOffset = fieldState.offset + offsetAttribute.endOffset();\n              if (startOffset < 0 || endOffset < startOffset) {\n                throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                    + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n              }\n              if (startOffset < lastStartOffset) {\n                throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                     + startOffset + \" is < lastStartOffset=\" + lastStartOffset);\n              }\n              lastStartOffset = startOffset;\n            }\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (fieldType.indexed() && doInvert) {\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n\n        if (i > 0) {\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \")\");\n            }\n            if (fieldState.position == 0 && posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0)\");\n            }\n            int position = fieldState.position + posIncr;\n            if (position > 0) {\n              // NOTE: confusing: this \"mirrors\" the\n              // position++ we do below\n              position--;\n            } else if (position < 0) {\n              throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n            }\n            \n            // position is legal, we can safely place it in fieldState now.\n            // not sure if anything will use fieldState after non-aborting exc...\n            fieldState.position = position;\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":["a7c7a5405c388fd86e5962126be8ad09283eb5cc"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a","date":1341524239,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \")\");\n            }\n            if (fieldState.position == 0 && posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0)\");\n            }\n            int position = fieldState.position + posIncr;\n            if (position > 0) {\n              // NOTE: confusing: this \"mirrors\" the\n              // position++ we do below\n              position--;\n            } else if (position < 0) {\n              throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n            }\n            \n            // position is legal, we can safely place it in fieldState now.\n            // not sure if anything will use fieldState after non-aborting exc...\n            fieldState.position = position;\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n            \n            if (checkOffsets) {\n              int startOffset = fieldState.offset + offsetAttribute.startOffset();\n              int endOffset = fieldState.offset + offsetAttribute.endOffset();\n              if (startOffset < 0 || endOffset < startOffset) {\n                throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                    + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n              }\n              if (startOffset < lastStartOffset) {\n                throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                     + startOffset + \" is < lastStartOffset=\" + lastStartOffset);\n              }\n              lastStartOffset = startOffset;\n            }\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (fieldType.indexed() && doInvert) {\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \")\");\n            }\n            if (fieldState.position == 0 && posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0)\");\n            }\n            int position = fieldState.position + posIncr;\n            if (position > 0) {\n              // NOTE: confusing: this \"mirrors\" the\n              // position++ we do below\n              position--;\n            } else if (position < 0) {\n              throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n            }\n            \n            // position is legal, we can safely place it in fieldState now.\n            // not sure if anything will use fieldState after non-aborting exc...\n            fieldState.position = position;\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n            \n            if (checkOffsets) {\n              int startOffset = fieldState.offset + offsetAttribute.startOffset();\n              int endOffset = fieldState.offset + offsetAttribute.endOffset();\n              if (startOffset < 0 || endOffset < startOffset) {\n                throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                    + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n              }\n              if (startOffset < lastStartOffset) {\n                throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                     + startOffset + \" is < lastStartOffset=\" + lastStartOffset);\n              }\n              lastStartOffset = startOffset;\n            }\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0c59b968cd5180c64260dcc70c3f53320d2f848a","date":1342796758,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (fieldType.indexed() && doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \")\");\n            }\n            if (fieldState.position == 0 && posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0)\");\n            }\n            int position = fieldState.position + posIncr;\n            if (position > 0) {\n              // NOTE: confusing: this \"mirrors\" the\n              // position++ we do below\n              position--;\n            } else if (position < 0) {\n              throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n            }\n            \n            // position is legal, we can safely place it in fieldState now.\n            // not sure if anything will use fieldState after non-aborting exc...\n            fieldState.position = position;\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n            \n            if (checkOffsets) {\n              int startOffset = fieldState.offset + offsetAttribute.startOffset();\n              int endOffset = fieldState.offset + offsetAttribute.endOffset();\n              if (startOffset < 0 || endOffset < startOffset) {\n                throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                    + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n              }\n              if (startOffset < lastStartOffset) {\n                throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                     + startOffset + \" is < lastStartOffset=\" + lastStartOffset);\n              }\n              lastStartOffset = startOffset;\n            }\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (fieldType.indexed() && doInvert) {\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \")\");\n            }\n            if (fieldState.position == 0 && posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0)\");\n            }\n            int position = fieldState.position + posIncr;\n            if (position > 0) {\n              // NOTE: confusing: this \"mirrors\" the\n              // position++ we do below\n              position--;\n            } else if (position < 0) {\n              throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n            }\n            \n            // position is legal, we can safely place it in fieldState now.\n            // not sure if anything will use fieldState after non-aborting exc...\n            fieldState.position = position;\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n            \n            if (checkOffsets) {\n              int startOffset = fieldState.offset + offsetAttribute.startOffset();\n              int endOffset = fieldState.offset + offsetAttribute.endOffset();\n              if (startOffset < 0 || endOffset < startOffset) {\n                throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                    + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n              }\n              if (startOffset < lastStartOffset) {\n                throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                     + startOffset + \" is < lastStartOffset=\" + lastStartOffset);\n              }\n              lastStartOffset = startOffset;\n            }\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","date":1343059585,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (fieldType.indexed() && doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \")\");\n            }\n            if (fieldState.position == 0 && posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0)\");\n            }\n            int position = fieldState.position + posIncr;\n            if (position > 0) {\n              // NOTE: confusing: this \"mirrors\" the\n              // position++ we do below\n              position--;\n            } else if (position < 0) {\n              throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n            }\n            \n            // position is legal, we can safely place it in fieldState now.\n            // not sure if anything will use fieldState after non-aborting exc...\n            fieldState.position = position;\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n            \n            if (checkOffsets) {\n              int startOffset = fieldState.offset + offsetAttribute.startOffset();\n              int endOffset = fieldState.offset + offsetAttribute.endOffset();\n              if (startOffset < 0 || endOffset < startOffset) {\n                throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                    + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n              }\n              if (startOffset < lastStartOffset) {\n                throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                     + startOffset + \" is < lastStartOffset=\" + lastStartOffset);\n              }\n              lastStartOffset = startOffset;\n            }\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (fieldType.indexed() && doInvert) {\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \")\");\n            }\n            if (fieldState.position == 0 && posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0)\");\n            }\n            int position = fieldState.position + posIncr;\n            if (position > 0) {\n              // NOTE: confusing: this \"mirrors\" the\n              // position++ we do below\n              position--;\n            } else if (position < 0) {\n              throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n            }\n            \n            // position is legal, we can safely place it in fieldState now.\n            // not sure if anything will use fieldState after non-aborting exc...\n            fieldState.position = position;\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n            \n            if (checkOffsets) {\n              int startOffset = fieldState.offset + offsetAttribute.startOffset();\n              int endOffset = fieldState.offset + offsetAttribute.endOffset();\n              if (startOffset < 0 || endOffset < startOffset) {\n                throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                    + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n              }\n              if (startOffset < lastStartOffset) {\n                throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                     + startOffset + \" is < lastStartOffset=\" + lastStartOffset);\n              }\n              lastStartOffset = startOffset;\n            }\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aba371508186796cc6151d8223a5b4e16d02e26e","date":1343474871,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \")\");\n            }\n            if (fieldState.position == 0 && posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0)\");\n            }\n            int position = fieldState.position + posIncr;\n            if (position > 0) {\n              // NOTE: confusing: this \"mirrors\" the\n              // position++ we do below\n              position--;\n            } else if (position < 0) {\n              throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n            }\n            \n            // position is legal, we can safely place it in fieldState now.\n            // not sure if anything will use fieldState after non-aborting exc...\n            fieldState.position = position;\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n            \n            if (checkOffsets) {\n              int startOffset = fieldState.offset + offsetAttribute.startOffset();\n              int endOffset = fieldState.offset + offsetAttribute.endOffset();\n              if (startOffset < 0 || endOffset < startOffset) {\n                throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                    + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n              }\n              if (startOffset < lastStartOffset) {\n                throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                     + startOffset + \" is < lastStartOffset=\" + lastStartOffset);\n              }\n              lastStartOffset = startOffset;\n            }\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \")\");\n            }\n            if (fieldState.position == 0 && posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0)\");\n            }\n            int position = fieldState.position + posIncr;\n            if (position > 0) {\n              // NOTE: confusing: this \"mirrors\" the\n              // position++ we do below\n              position--;\n            } else if (position < 0) {\n              throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n            }\n            \n            // position is legal, we can safely place it in fieldState now.\n            // not sure if anything will use fieldState after non-aborting exc...\n            fieldState.position = position;\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n            \n            if (checkOffsets) {\n              int startOffset = fieldState.offset + offsetAttribute.startOffset();\n              int endOffset = fieldState.offset + offsetAttribute.endOffset();\n              if (startOffset < 0 || endOffset < startOffset) {\n                throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                    + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n              }\n              if (startOffset < lastStartOffset) {\n                throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                     + startOffset + \" is < lastStartOffset=\" + lastStartOffset);\n              }\n              lastStartOffset = startOffset;\n            }\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1d028314cced5858683a1bb4741423d0f934257b","date":1346596535,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \")\");\n            }\n            if (fieldState.position == 0 && posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0)\");\n            }\n            int position = fieldState.position + posIncr;\n            if (position > 0) {\n              // NOTE: confusing: this \"mirrors\" the\n              // position++ we do below\n              position--;\n            } else if (position < 0) {\n              throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n            }\n            \n            // position is legal, we can safely place it in fieldState now.\n            // not sure if anything will use fieldState after non-aborting exc...\n            fieldState.position = position;\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n            \n            if (checkOffsets) {\n              int startOffset = fieldState.offset + offsetAttribute.startOffset();\n              int endOffset = fieldState.offset + offsetAttribute.endOffset();\n              if (startOffset < 0 || endOffset < startOffset) {\n                throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                    + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n              }\n              if (startOffset < lastStartOffset) {\n                throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                     + startOffset + \" is < lastStartOffset=\" + lastStartOffset);\n              }\n              lastStartOffset = startOffset;\n            }\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (fieldType.indexed() && doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \")\");\n            }\n            if (fieldState.position == 0 && posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0)\");\n            }\n            int position = fieldState.position + posIncr;\n            if (position > 0) {\n              // NOTE: confusing: this \"mirrors\" the\n              // position++ we do below\n              position--;\n            } else if (position < 0) {\n              throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n            }\n            \n            // position is legal, we can safely place it in fieldState now.\n            // not sure if anything will use fieldState after non-aborting exc...\n            fieldState.position = position;\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n            \n            if (checkOffsets) {\n              int startOffset = fieldState.offset + offsetAttribute.startOffset();\n              int endOffset = fieldState.offset + offsetAttribute.endOffset();\n              if (startOffset < 0 || endOffset < startOffset) {\n                throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                    + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n              }\n              if (startOffset < lastStartOffset) {\n                throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                     + startOffset + \" is < lastStartOffset=\" + lastStartOffset);\n              }\n              lastStartOffset = startOffset;\n            }\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a7c7a5405c388fd86e5962126be8ad09283eb5cc","date":1357256120,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          if (hasMoreTokens) {\n            consumer.start(field);\n\n            do {\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n\n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              if (posIncr < 0) {\n                throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \")\");\n              }\n              if (fieldState.position == 0 && posIncr == 0) {\n                throw new IllegalArgumentException(\"first position increment must be > 0 (got 0)\");\n              }\n              int position = fieldState.position + posIncr;\n              if (position > 0) {\n                // NOTE: confusing: this \"mirrors\" the\n                // position++ we do below\n                position--;\n              } else if (position < 0) {\n                throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n              }\n              \n              // position is legal, we can safely place it in fieldState now.\n              // not sure if anything will use fieldState after non-aborting exc...\n              fieldState.position = position;\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n              \n              if (checkOffsets) {\n                int startOffset = fieldState.offset + offsetAttribute.startOffset();\n                int endOffset = fieldState.offset + offsetAttribute.endOffset();\n                if (startOffset < 0 || endOffset < startOffset) {\n                  throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                      + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n                }\n                if (startOffset < lastStartOffset) {\n                  throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                       + startOffset + \" is < lastStartOffset=\" + lastStartOffset);\n                }\n                lastStartOffset = startOffset;\n              }\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n            } while (stream.incrementToken());\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \")\");\n            }\n            if (fieldState.position == 0 && posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0)\");\n            }\n            int position = fieldState.position + posIncr;\n            if (position > 0) {\n              // NOTE: confusing: this \"mirrors\" the\n              // position++ we do below\n              position--;\n            } else if (position < 0) {\n              throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n            }\n            \n            // position is legal, we can safely place it in fieldState now.\n            // not sure if anything will use fieldState after non-aborting exc...\n            fieldState.position = position;\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n            \n            if (checkOffsets) {\n              int startOffset = fieldState.offset + offsetAttribute.startOffset();\n              int endOffset = fieldState.offset + offsetAttribute.endOffset();\n              if (startOffset < 0 || endOffset < startOffset) {\n                throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                    + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n              }\n              if (startOffset < lastStartOffset) {\n                throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                     + startOffset + \" is < lastStartOffset=\" + lastStartOffset);\n              }\n              lastStartOffset = startOffset;\n            }\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":["c5f000280bc18391509bbb40c4a2a2c7515d54d3","0e748132a1ca480bd503ee795aee58f5765809a6","cd65a3c65e7917a381c935b0b663d8e783bd9a1e","8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5","1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":["f9c800d581a0aa5fb80893a3d02e1ec5e2a86ec5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          if (hasMoreTokens) {\n            consumer.start(field);\n\n            do {\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n\n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              if (posIncr < 0) {\n                throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \")\");\n              }\n              if (fieldState.position == 0 && posIncr == 0) {\n                throw new IllegalArgumentException(\"first position increment must be > 0 (got 0)\");\n              }\n              int position = fieldState.position + posIncr;\n              if (position > 0) {\n                // NOTE: confusing: this \"mirrors\" the\n                // position++ we do below\n                position--;\n              } else if (position < 0) {\n                throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n              }\n              \n              // position is legal, we can safely place it in fieldState now.\n              // not sure if anything will use fieldState after non-aborting exc...\n              fieldState.position = position;\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n              \n              if (checkOffsets) {\n                int startOffset = fieldState.offset + offsetAttribute.startOffset();\n                int endOffset = fieldState.offset + offsetAttribute.endOffset();\n                if (startOffset < 0 || endOffset < startOffset) {\n                  throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                      + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n                }\n                if (startOffset < lastStartOffset) {\n                  throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                       + startOffset + \" is < lastStartOffset=\" + lastStartOffset);\n                }\n                lastStartOffset = startOffset;\n              }\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n            } while (stream.incrementToken());\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \")\");\n            }\n            if (fieldState.position == 0 && posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0)\");\n            }\n            int position = fieldState.position + posIncr;\n            if (position > 0) {\n              // NOTE: confusing: this \"mirrors\" the\n              // position++ we do below\n              position--;\n            } else if (position < 0) {\n              throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n            }\n            \n            // position is legal, we can safely place it in fieldState now.\n            // not sure if anything will use fieldState after non-aborting exc...\n            fieldState.position = position;\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n            \n            if (checkOffsets) {\n              int startOffset = fieldState.offset + offsetAttribute.startOffset();\n              int endOffset = fieldState.offset + offsetAttribute.endOffset();\n              if (startOffset < 0 || endOffset < startOffset) {\n                throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                    + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n              }\n              if (startOffset < lastStartOffset) {\n                throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                     + startOffset + \" is < lastStartOffset=\" + lastStartOffset);\n              }\n              lastStartOffset = startOffset;\n            }\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f9c800d581a0aa5fb80893a3d02e1ec5e2a86ec5","date":1368203359,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          if (hasMoreTokens) {\n            consumer.start(field);\n\n            do {\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n\n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              if (posIncr < 0) {\n                throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n              }\n              if (fieldState.position == 0 && posIncr == 0) {\n                throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n              }\n              int position = fieldState.position + posIncr;\n              if (position > 0) {\n                // NOTE: confusing: this \"mirrors\" the\n                // position++ we do below\n                position--;\n              } else if (position < 0) {\n                throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n              }\n              \n              // position is legal, we can safely place it in fieldState now.\n              // not sure if anything will use fieldState after non-aborting exc...\n              fieldState.position = position;\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n              \n              if (checkOffsets) {\n                int startOffset = fieldState.offset + offsetAttribute.startOffset();\n                int endOffset = fieldState.offset + offsetAttribute.endOffset();\n                if (startOffset < 0 || endOffset < startOffset) {\n                  throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                      + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n                }\n                if (startOffset < lastStartOffset) {\n                  throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                       + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n                }\n                lastStartOffset = startOffset;\n              }\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n            } while (stream.incrementToken());\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          if (hasMoreTokens) {\n            consumer.start(field);\n\n            do {\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n\n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              if (posIncr < 0) {\n                throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \")\");\n              }\n              if (fieldState.position == 0 && posIncr == 0) {\n                throw new IllegalArgumentException(\"first position increment must be > 0 (got 0)\");\n              }\n              int position = fieldState.position + posIncr;\n              if (position > 0) {\n                // NOTE: confusing: this \"mirrors\" the\n                // position++ we do below\n                position--;\n              } else if (position < 0) {\n                throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n              }\n              \n              // position is legal, we can safely place it in fieldState now.\n              // not sure if anything will use fieldState after non-aborting exc...\n              fieldState.position = position;\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n              \n              if (checkOffsets) {\n                int startOffset = fieldState.offset + offsetAttribute.startOffset();\n                int endOffset = fieldState.offset + offsetAttribute.endOffset();\n                if (startOffset < 0 || endOffset < startOffset) {\n                  throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                      + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n                }\n                if (startOffset < lastStartOffset) {\n                  throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                       + startOffset + \" is < lastStartOffset=\" + lastStartOffset);\n                }\n                lastStartOffset = startOffset;\n              }\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n            } while (stream.incrementToken());\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":["a7c7a5405c388fd86e5962126be8ad09283eb5cc"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f3f8a0f8bebc057ea4bdf65150b3fdc539db3d03","date":1377018786,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          if (hasMoreTokens) {\n            consumer.start(field);\n\n            do {\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n\n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              if (posIncr < 0) {\n                throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n              }\n              if (fieldState.position == 0 && posIncr == 0) {\n                throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n              }\n              int position = fieldState.position + posIncr;\n              if (position > 0) {\n                // NOTE: confusing: this \"mirrors\" the\n                // position++ we do below\n                position--;\n              } else if (position < 0) {\n                throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n              }\n              \n              // position is legal, we can safely place it in fieldState now.\n              // not sure if anything will use fieldState after non-aborting exc...\n              fieldState.position = position;\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n              \n              if (checkOffsets) {\n                int startOffset = fieldState.offset + offsetAttribute.startOffset();\n                int endOffset = fieldState.offset + offsetAttribute.endOffset();\n                if (startOffset < 0 || endOffset < startOffset) {\n                  throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                      + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n                }\n                if (startOffset < lastStartOffset) {\n                  throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                       + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n                }\n                lastStartOffset = startOffset;\n              }\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n            } while (stream.incrementToken());\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n          // TODO: maybe add some safety? then again, its already checked \n          // when we come back around to the field...\n          fieldState.position += posIncrAttribute.getPositionIncrement();\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          if (hasMoreTokens) {\n            consumer.start(field);\n\n            do {\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n\n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              if (posIncr < 0) {\n                throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n              }\n              if (fieldState.position == 0 && posIncr == 0) {\n                throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n              }\n              int position = fieldState.position + posIncr;\n              if (position > 0) {\n                // NOTE: confusing: this \"mirrors\" the\n                // position++ we do below\n                position--;\n              } else if (position < 0) {\n                throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n              }\n              \n              // position is legal, we can safely place it in fieldState now.\n              // not sure if anything will use fieldState after non-aborting exc...\n              fieldState.position = position;\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n              \n              if (checkOffsets) {\n                int startOffset = fieldState.offset + offsetAttribute.startOffset();\n                int endOffset = fieldState.offset + offsetAttribute.endOffset();\n                if (startOffset < 0 || endOffset < startOffset) {\n                  throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                      + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n                }\n                if (startOffset < lastStartOffset) {\n                  throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                       + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n                }\n                lastStartOffset = startOffset;\n              }\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n            } while (stream.incrementToken());\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          if (hasMoreTokens) {\n            consumer.start(field);\n\n            do {\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n\n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              if (posIncr < 0) {\n                throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n              }\n              if (fieldState.position == 0 && posIncr == 0) {\n                throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n              }\n              int position = fieldState.position + posIncr;\n              if (position > 0) {\n                // NOTE: confusing: this \"mirrors\" the\n                // position++ we do below\n                position--;\n              } else if (position < 0) {\n                throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n              }\n              \n              // position is legal, we can safely place it in fieldState now.\n              // not sure if anything will use fieldState after non-aborting exc...\n              fieldState.position = position;\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n              \n              if (checkOffsets) {\n                int startOffset = fieldState.offset + offsetAttribute.startOffset();\n                int endOffset = fieldState.offset + offsetAttribute.endOffset();\n                if (startOffset < 0 || endOffset < startOffset) {\n                  throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                      + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n                }\n                if (startOffset < lastStartOffset) {\n                  throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                       + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n                }\n                lastStartOffset = startOffset;\n              }\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n            } while (stream.incrementToken());\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n          // TODO: maybe add some safety? then again, its already checked \n          // when we come back around to the field...\n          fieldState.position += posIncrAttribute.getPositionIncrement();\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          if (hasMoreTokens) {\n            consumer.start(field);\n\n            do {\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n\n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              if (posIncr < 0) {\n                throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n              }\n              if (fieldState.position == 0 && posIncr == 0) {\n                throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n              }\n              int position = fieldState.position + posIncr;\n              if (position > 0) {\n                // NOTE: confusing: this \"mirrors\" the\n                // position++ we do below\n                position--;\n              } else if (position < 0) {\n                throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n              }\n              \n              // position is legal, we can safely place it in fieldState now.\n              // not sure if anything will use fieldState after non-aborting exc...\n              fieldState.position = position;\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n              \n              if (checkOffsets) {\n                int startOffset = fieldState.offset + offsetAttribute.startOffset();\n                int endOffset = fieldState.offset + offsetAttribute.endOffset();\n                if (startOffset < 0 || endOffset < startOffset) {\n                  throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                      + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n                }\n                if (startOffset < lastStartOffset) {\n                  throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                       + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n                }\n                lastStartOffset = startOffset;\n              }\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n            } while (stream.incrementToken());\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"782ed6a4b4ba50ec19734fc8db4e570ee193d627","date":1381127065,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        try (TokenStream stream = field.tokenStream(docState.analyzer)) {\n          // reset the TokenStream to the first token\n          stream.reset();\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          if (hasMoreTokens) {\n            consumer.start(field);\n\n            do {\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n\n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              if (posIncr < 0) {\n                throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n              }\n              if (fieldState.position == 0 && posIncr == 0) {\n                throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n              }\n              int position = fieldState.position + posIncr;\n              if (position > 0) {\n                // NOTE: confusing: this \"mirrors\" the\n                // position++ we do below\n                position--;\n              } else if (position < 0) {\n                throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n              }\n              \n              // position is legal, we can safely place it in fieldState now.\n              // not sure if anything will use fieldState after non-aborting exc...\n              fieldState.position = position;\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n              \n              if (checkOffsets) {\n                int startOffset = fieldState.offset + offsetAttribute.startOffset();\n                int endOffset = fieldState.offset + offsetAttribute.endOffset();\n                if (startOffset < 0 || endOffset < startOffset) {\n                  throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                      + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n                }\n                if (startOffset < lastStartOffset) {\n                  throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                       + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n                }\n                lastStartOffset = startOffset;\n              }\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n            } while (stream.incrementToken());\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n          // TODO: maybe add some safety? then again, its already checked \n          // when we come back around to the field...\n          fieldState.position += posIncrAttribute.getPositionIncrement();\n          fieldState.offset += offsetAttribute.endOffset();\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        boolean success2 = false;\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          if (hasMoreTokens) {\n            consumer.start(field);\n\n            do {\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n\n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              if (posIncr < 0) {\n                throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n              }\n              if (fieldState.position == 0 && posIncr == 0) {\n                throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n              }\n              int position = fieldState.position + posIncr;\n              if (position > 0) {\n                // NOTE: confusing: this \"mirrors\" the\n                // position++ we do below\n                position--;\n              } else if (position < 0) {\n                throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n              }\n              \n              // position is legal, we can safely place it in fieldState now.\n              // not sure if anything will use fieldState after non-aborting exc...\n              fieldState.position = position;\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n              \n              if (checkOffsets) {\n                int startOffset = fieldState.offset + offsetAttribute.startOffset();\n                int endOffset = fieldState.offset + offsetAttribute.endOffset();\n                if (startOffset < 0 || endOffset < startOffset) {\n                  throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                      + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n                }\n                if (startOffset < lastStartOffset) {\n                  throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                       + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n                }\n                lastStartOffset = startOffset;\n              }\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n            } while (stream.incrementToken());\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n          // TODO: maybe add some safety? then again, its already checked \n          // when we come back around to the field...\n          fieldState.position += posIncrAttribute.getPositionIncrement();\n          fieldState.offset += offsetAttribute.endOffset();\n          success2 = true;\n        } finally {\n          if (!success2) {\n            IOUtils.closeWhileHandlingException(stream);\n          } else {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":["c5f000280bc18391509bbb40c4a2a2c7515d54d3","8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5","1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4b9ad5f40b2ccdbcb584fa26d35ff8cebc4fa64e","date":1391042632,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        /*\n        * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n        * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n        * but rather a finally that takes note of the problem.\n        */\n\n        boolean succeededInProcessingField = false;\n        try (TokenStream stream = field.tokenStream(docState.analyzer)) {\n          // reset the TokenStream to the first token\n          stream.reset();\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          if (hasMoreTokens) {\n            consumer.start(field);\n\n            do {\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n\n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              if (posIncr < 0) {\n                throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n              }\n              if (fieldState.position == 0 && posIncr == 0) {\n                throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n              }\n              int position = fieldState.position + posIncr;\n              if (position > 0) {\n                // NOTE: confusing: this \"mirrors\" the\n                // position++ we do below\n                position--;\n              } else if (position < 0) {\n                throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n              }\n              \n              // position is legal, we can safely place it in fieldState now.\n              // not sure if anything will use fieldState after non-aborting exc...\n              fieldState.position = position;\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n              \n              if (checkOffsets) {\n                int startOffset = fieldState.offset + offsetAttribute.startOffset();\n                int endOffset = fieldState.offset + offsetAttribute.endOffset();\n                if (startOffset < 0 || endOffset < startOffset) {\n                  throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                      + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n                }\n                if (startOffset < lastStartOffset) {\n                  throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                       + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n                }\n                lastStartOffset = startOffset;\n              }\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n            } while (stream.incrementToken());\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n          // TODO: maybe add some safety? then again, its already checked \n          // when we come back around to the field...\n          fieldState.position += posIncrAttribute.getPositionIncrement();\n          fieldState.offset += offsetAttribute.endOffset();\n          /* if success was false above there is an exception coming through and we won't get here.*/\n          succeededInProcessingField = true;\n        } finally {\n          if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n            docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        try (TokenStream stream = field.tokenStream(docState.analyzer)) {\n          // reset the TokenStream to the first token\n          stream.reset();\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          if (hasMoreTokens) {\n            consumer.start(field);\n\n            do {\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n\n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              if (posIncr < 0) {\n                throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n              }\n              if (fieldState.position == 0 && posIncr == 0) {\n                throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n              }\n              int position = fieldState.position + posIncr;\n              if (position > 0) {\n                // NOTE: confusing: this \"mirrors\" the\n                // position++ we do below\n                position--;\n              } else if (position < 0) {\n                throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n              }\n              \n              // position is legal, we can safely place it in fieldState now.\n              // not sure if anything will use fieldState after non-aborting exc...\n              fieldState.position = position;\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n              \n              if (checkOffsets) {\n                int startOffset = fieldState.offset + offsetAttribute.startOffset();\n                int endOffset = fieldState.offset + offsetAttribute.endOffset();\n                if (startOffset < 0 || endOffset < startOffset) {\n                  throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                      + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n                }\n                if (startOffset < lastStartOffset) {\n                  throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                       + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n                }\n                lastStartOffset = startOffset;\n              }\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n            } while (stream.incrementToken());\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n          // TODO: maybe add some safety? then again, its already checked \n          // when we come back around to the field...\n          fieldState.position += posIncrAttribute.getPositionIncrement();\n          fieldState.offset += offsetAttribute.endOffset();\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"426a4760316fc52cf79e191cadfcb328dfc2d1ca","date":1394042725,"type":3,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        /*\n        * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n        * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n        * but rather a finally that takes note of the problem.\n        */\n\n        boolean succeededInProcessingField = false;\n        try (TokenStream stream = field.tokenStream(docState.analyzer)) {\n          // reset the TokenStream to the first token\n          stream.reset();\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          if (hasMoreTokens) {\n            consumer.start(field);\n\n            do {\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n\n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              if (posIncr < 0) {\n                throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n              }\n              if (fieldState.position == 0 && posIncr == 0) {\n                throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n              }\n              int position = fieldState.position + posIncr;\n              if (position > 0) {\n                // NOTE: confusing: this \"mirrors\" the\n                // position++ we do below\n                position--;\n              } else if (position < 0) {\n                throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n              }\n              \n              // position is legal, we can safely place it in fieldState now.\n              // not sure if anything will use fieldState after non-aborting exc...\n              fieldState.position = position;\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n              \n              if (checkOffsets) {\n                int startOffset = fieldState.offset + offsetAttribute.startOffset();\n                int endOffset = fieldState.offset + offsetAttribute.endOffset();\n                if (startOffset < 0 || endOffset < startOffset) {\n                  throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                      + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n                }\n                if (startOffset < lastStartOffset) {\n                  throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                       + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n                }\n                lastStartOffset = startOffset;\n              }\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n            } while (stream.incrementToken());\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n          // TODO: maybe add some safety? then again, its already checked \n          // when we come back around to the field...\n          fieldState.position += posIncrAttribute.getPositionIncrement();\n          fieldState.offset += offsetAttribute.endOffset();\n\n\n          if (docState.maxTermPrefix != null) {\n            final String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\";\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            docState.maxTermPrefix = null;\n            throw new IllegalArgumentException(msg);\n          }\n\n          /* if success was false above there is an exception coming through and we won't get here.*/\n          succeededInProcessingField = true;\n        } finally {\n          if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n            docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        /*\n        * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n        * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n        * but rather a finally that takes note of the problem.\n        */\n\n        boolean succeededInProcessingField = false;\n        try (TokenStream stream = field.tokenStream(docState.analyzer)) {\n          // reset the TokenStream to the first token\n          stream.reset();\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          if (hasMoreTokens) {\n            consumer.start(field);\n\n            do {\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n\n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              if (posIncr < 0) {\n                throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n              }\n              if (fieldState.position == 0 && posIncr == 0) {\n                throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n              }\n              int position = fieldState.position + posIncr;\n              if (position > 0) {\n                // NOTE: confusing: this \"mirrors\" the\n                // position++ we do below\n                position--;\n              } else if (position < 0) {\n                throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n              }\n              \n              // position is legal, we can safely place it in fieldState now.\n              // not sure if anything will use fieldState after non-aborting exc...\n              fieldState.position = position;\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n              \n              if (checkOffsets) {\n                int startOffset = fieldState.offset + offsetAttribute.startOffset();\n                int endOffset = fieldState.offset + offsetAttribute.endOffset();\n                if (startOffset < 0 || endOffset < startOffset) {\n                  throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                      + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n                }\n                if (startOffset < lastStartOffset) {\n                  throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                       + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n                }\n                lastStartOffset = startOffset;\n              }\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n            } while (stream.incrementToken());\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n          // TODO: maybe add some safety? then again, its already checked \n          // when we come back around to the field...\n          fieldState.position += posIncrAttribute.getPositionIncrement();\n          fieldState.offset += offsetAttribute.endOffset();\n          /* if success was false above there is an exception coming through and we won't get here.*/\n          succeededInProcessingField = true;\n        } finally {\n          if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n            docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"96ea64d994d340044e0d57aeb6a5871539d10ca5","date":1394225445,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        /*\n        * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n        * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n        * but rather a finally that takes note of the problem.\n        */\n\n        boolean succeededInProcessingField = false;\n        try (TokenStream stream = field.tokenStream(docState.analyzer)) {\n          // reset the TokenStream to the first token\n          stream.reset();\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          if (hasMoreTokens) {\n            consumer.start(field);\n\n            do {\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n\n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              if (posIncr < 0) {\n                throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n              }\n              if (fieldState.position == 0 && posIncr == 0) {\n                throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n              }\n              int position = fieldState.position + posIncr;\n              if (position > 0) {\n                // NOTE: confusing: this \"mirrors\" the\n                // position++ we do below\n                position--;\n              } else if (position < 0) {\n                throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n              }\n              \n              // position is legal, we can safely place it in fieldState now.\n              // not sure if anything will use fieldState after non-aborting exc...\n              fieldState.position = position;\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n              \n              if (checkOffsets) {\n                int startOffset = fieldState.offset + offsetAttribute.startOffset();\n                int endOffset = fieldState.offset + offsetAttribute.endOffset();\n                if (startOffset < 0 || endOffset < startOffset) {\n                  throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                      + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n                }\n                if (startOffset < lastStartOffset) {\n                  throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                       + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n                }\n                lastStartOffset = startOffset;\n              }\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n            } while (stream.incrementToken());\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n          // TODO: maybe add some safety? then again, its already checked \n          // when we come back around to the field...\n          fieldState.position += posIncrAttribute.getPositionIncrement();\n          fieldState.offset += offsetAttribute.endOffset();\n\n\n          if (docState.maxTermPrefix != null) {\n            final String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\";\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            docState.maxTermPrefix = null;\n            throw new IllegalArgumentException(msg);\n          }\n\n          /* if success was false above there is an exception coming through and we won't get here.*/\n          succeededInProcessingField = true;\n        } finally {\n          if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n            docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        /*\n        * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n        * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n        * but rather a finally that takes note of the problem.\n        */\n\n        boolean succeededInProcessingField = false;\n        try (TokenStream stream = field.tokenStream(docState.analyzer)) {\n          // reset the TokenStream to the first token\n          stream.reset();\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          if (hasMoreTokens) {\n            consumer.start(field);\n\n            do {\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n\n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              if (posIncr < 0) {\n                throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n              }\n              if (fieldState.position == 0 && posIncr == 0) {\n                throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n              }\n              int position = fieldState.position + posIncr;\n              if (position > 0) {\n                // NOTE: confusing: this \"mirrors\" the\n                // position++ we do below\n                position--;\n              } else if (position < 0) {\n                throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n              }\n              \n              // position is legal, we can safely place it in fieldState now.\n              // not sure if anything will use fieldState after non-aborting exc...\n              fieldState.position = position;\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n              \n              if (checkOffsets) {\n                int startOffset = fieldState.offset + offsetAttribute.startOffset();\n                int endOffset = fieldState.offset + offsetAttribute.endOffset();\n                if (startOffset < 0 || endOffset < startOffset) {\n                  throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                      + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n                }\n                if (startOffset < lastStartOffset) {\n                  throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                       + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n                }\n                lastStartOffset = startOffset;\n              }\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n            } while (stream.incrementToken());\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n          // TODO: maybe add some safety? then again, its already checked \n          // when we come back around to the field...\n          fieldState.position += posIncrAttribute.getPositionIncrement();\n          fieldState.offset += offsetAttribute.endOffset();\n          /* if success was false above there is an exception coming through and we won't get here.*/\n          succeededInProcessingField = true;\n        } finally {\n          if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n            docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"52c7e49be259508735752fba88085255014a6ecf","date":1398706273,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":null,"sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        /*\n        * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n        * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n        * but rather a finally that takes note of the problem.\n        */\n\n        boolean succeededInProcessingField = false;\n        try (TokenStream stream = field.tokenStream(docState.analyzer)) {\n          // reset the TokenStream to the first token\n          stream.reset();\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          if (hasMoreTokens) {\n            consumer.start(field);\n\n            do {\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n\n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              if (posIncr < 0) {\n                throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n              }\n              if (fieldState.position == 0 && posIncr == 0) {\n                throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n              }\n              int position = fieldState.position + posIncr;\n              if (position > 0) {\n                // NOTE: confusing: this \"mirrors\" the\n                // position++ we do below\n                position--;\n              } else if (position < 0) {\n                throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n              }\n              \n              // position is legal, we can safely place it in fieldState now.\n              // not sure if anything will use fieldState after non-aborting exc...\n              fieldState.position = position;\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n              \n              if (checkOffsets) {\n                int startOffset = fieldState.offset + offsetAttribute.startOffset();\n                int endOffset = fieldState.offset + offsetAttribute.endOffset();\n                if (startOffset < 0 || endOffset < startOffset) {\n                  throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                      + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n                }\n                if (startOffset < lastStartOffset) {\n                  throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                       + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n                }\n                lastStartOffset = startOffset;\n              }\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n            } while (stream.incrementToken());\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n          // TODO: maybe add some safety? then again, its already checked \n          // when we come back around to the field...\n          fieldState.position += posIncrAttribute.getPositionIncrement();\n          fieldState.offset += offsetAttribute.endOffset();\n\n\n          if (docState.maxTermPrefix != null) {\n            final String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\";\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            docState.maxTermPrefix = null;\n            throw new IllegalArgumentException(msg);\n          }\n\n          /* if success was false above there is an exception coming through and we won't get here.*/\n          succeededInProcessingField = true;\n        } finally {\n          if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n            docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3394716f52b34ab259ad5247e7595d9f9db6e935","date":1398791921,"type":4,"author":"Michael McCandless","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":null,"sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        /*\n        * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n        * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n        * but rather a finally that takes note of the problem.\n        */\n\n        boolean succeededInProcessingField = false;\n        try (TokenStream stream = field.tokenStream(docState.analyzer)) {\n          // reset the TokenStream to the first token\n          stream.reset();\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          if (hasMoreTokens) {\n            consumer.start(field);\n\n            do {\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n\n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              if (posIncr < 0) {\n                throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n              }\n              if (fieldState.position == 0 && posIncr == 0) {\n                throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n              }\n              int position = fieldState.position + posIncr;\n              if (position > 0) {\n                // NOTE: confusing: this \"mirrors\" the\n                // position++ we do below\n                position--;\n              } else if (position < 0) {\n                throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n              }\n              \n              // position is legal, we can safely place it in fieldState now.\n              // not sure if anything will use fieldState after non-aborting exc...\n              fieldState.position = position;\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n              \n              if (checkOffsets) {\n                int startOffset = fieldState.offset + offsetAttribute.startOffset();\n                int endOffset = fieldState.offset + offsetAttribute.endOffset();\n                if (startOffset < 0 || endOffset < startOffset) {\n                  throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                      + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n                }\n                if (startOffset < lastStartOffset) {\n                  throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                       + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n                }\n                lastStartOffset = startOffset;\n              }\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n            } while (stream.incrementToken());\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n          // TODO: maybe add some safety? then again, its already checked \n          // when we come back around to the field...\n          fieldState.position += posIncrAttribute.getPositionIncrement();\n          fieldState.offset += offsetAttribute.endOffset();\n\n\n          if (docState.maxTermPrefix != null) {\n            final String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\";\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            docState.maxTermPrefix = null;\n            throw new IllegalArgumentException(msg);\n          }\n\n          /* if success was false above there is an exception coming through and we won't get here.*/\n          succeededInProcessingField = true;\n        } finally {\n          if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n            docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","date":1398844771,"type":4,"author":"Dawid Weiss","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":null,"sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n      final IndexableFieldType fieldType = field.fieldType();\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (doInvert) {\n        final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        // only bother checking offsets if something will consume them.\n        // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n        final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n        int lastStartOffset = 0;\n\n        if (i > 0) {\n          fieldState.position += analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;\n        }\n\n        /*\n        * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n        * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n        * but rather a finally that takes note of the problem.\n        */\n\n        boolean succeededInProcessingField = false;\n        try (TokenStream stream = field.tokenStream(docState.analyzer)) {\n          // reset the TokenStream to the first token\n          stream.reset();\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          if (hasMoreTokens) {\n            consumer.start(field);\n\n            do {\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n\n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              if (posIncr < 0) {\n                throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n              }\n              if (fieldState.position == 0 && posIncr == 0) {\n                throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n              }\n              int position = fieldState.position + posIncr;\n              if (position > 0) {\n                // NOTE: confusing: this \"mirrors\" the\n                // position++ we do below\n                position--;\n              } else if (position < 0) {\n                throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n              }\n              \n              // position is legal, we can safely place it in fieldState now.\n              // not sure if anything will use fieldState after non-aborting exc...\n              fieldState.position = position;\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n              \n              if (checkOffsets) {\n                int startOffset = fieldState.offset + offsetAttribute.startOffset();\n                int endOffset = fieldState.offset + offsetAttribute.endOffset();\n                if (startOffset < 0 || endOffset < startOffset) {\n                  throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                      + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n                }\n                if (startOffset < lastStartOffset) {\n                  throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                       + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n                }\n                lastStartOffset = startOffset;\n              }\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n            } while (stream.incrementToken());\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n          // TODO: maybe add some safety? then again, its already checked \n          // when we come back around to the field...\n          fieldState.position += posIncrAttribute.getPositionIncrement();\n          fieldState.offset += offsetAttribute.endOffset();\n\n\n          if (docState.maxTermPrefix != null) {\n            final String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\";\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            docState.maxTermPrefix = null;\n            throw new IllegalArgumentException(msg);\n          }\n\n          /* if success was false above there is an exception coming through and we won't get here.*/\n          succeededInProcessingField = true;\n        } finally {\n          if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n            docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n          }\n        }\n\n        fieldState.offset += analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["1d028314cced5858683a1bb4741423d0f934257b","a7c7a5405c388fd86e5962126be8ad09283eb5cc"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":["4b9ad5f40b2ccdbcb584fa26d35ff8cebc4fa64e","426a4760316fc52cf79e191cadfcb328dfc2d1ca"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":["426a4760316fc52cf79e191cadfcb328dfc2d1ca","3394716f52b34ab259ad5247e7595d9f9db6e935"],"426a4760316fc52cf79e191cadfcb328dfc2d1ca":["4b9ad5f40b2ccdbcb584fa26d35ff8cebc4fa64e"],"c5f000280bc18391509bbb40c4a2a2c7515d54d3":["0e748132a1ca480bd503ee795aee58f5765809a6"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["f9c800d581a0aa5fb80893a3d02e1ec5e2a86ec5","f3f8a0f8bebc057ea4bdf65150b3fdc539db3d03"],"0e748132a1ca480bd503ee795aee58f5765809a6":["d1336abe0899b2984e5652903556c1925fbdca9f"],"782ed6a4b4ba50ec19734fc8db4e570ee193d627":["f3f8a0f8bebc057ea4bdf65150b3fdc539db3d03"],"4b9ad5f40b2ccdbcb584fa26d35ff8cebc4fa64e":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"f3f8a0f8bebc057ea4bdf65150b3fdc539db3d03":["f9c800d581a0aa5fb80893a3d02e1ec5e2a86ec5"],"3394716f52b34ab259ad5247e7595d9f9db6e935":["426a4760316fc52cf79e191cadfcb328dfc2d1ca","52c7e49be259508735752fba88085255014a6ecf"],"0c59b968cd5180c64260dcc70c3f53320d2f848a":["cd65a3c65e7917a381c935b0b663d8e783bd9a1e"],"1d028314cced5858683a1bb4741423d0f934257b":["0c59b968cd5180c64260dcc70c3f53320d2f848a","aba371508186796cc6151d8223a5b4e16d02e26e"],"cd65a3c65e7917a381c935b0b663d8e783bd9a1e":["c5f000280bc18391509bbb40c4a2a2c7515d54d3"],"aba371508186796cc6151d8223a5b4e16d02e26e":["33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a","0c59b968cd5180c64260dcc70c3f53320d2f848a"],"33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a":["cd65a3c65e7917a381c935b0b663d8e783bd9a1e"],"a7c7a5405c388fd86e5962126be8ad09283eb5cc":["1d028314cced5858683a1bb4741423d0f934257b"],"f9c800d581a0aa5fb80893a3d02e1ec5e2a86ec5":["a7c7a5405c388fd86e5962126be8ad09283eb5cc"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d1336abe0899b2984e5652903556c1925fbdca9f":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":["cd65a3c65e7917a381c935b0b663d8e783bd9a1e","0c59b968cd5180c64260dcc70c3f53320d2f848a"],"52c7e49be259508735752fba88085255014a6ecf":["426a4760316fc52cf79e191cadfcb328dfc2d1ca"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3394716f52b34ab259ad5247e7595d9f9db6e935"]},"commit2Childs":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"96ea64d994d340044e0d57aeb6a5871539d10ca5":[],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":[],"426a4760316fc52cf79e191cadfcb328dfc2d1ca":["96ea64d994d340044e0d57aeb6a5871539d10ca5","c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","3394716f52b34ab259ad5247e7595d9f9db6e935","52c7e49be259508735752fba88085255014a6ecf"],"c5f000280bc18391509bbb40c4a2a2c7515d54d3":["cd65a3c65e7917a381c935b0b663d8e783bd9a1e"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["d1336abe0899b2984e5652903556c1925fbdca9f"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"0e748132a1ca480bd503ee795aee58f5765809a6":["c5f000280bc18391509bbb40c4a2a2c7515d54d3"],"782ed6a4b4ba50ec19734fc8db4e570ee193d627":["4b9ad5f40b2ccdbcb584fa26d35ff8cebc4fa64e"],"4b9ad5f40b2ccdbcb584fa26d35ff8cebc4fa64e":["96ea64d994d340044e0d57aeb6a5871539d10ca5","426a4760316fc52cf79e191cadfcb328dfc2d1ca"],"f3f8a0f8bebc057ea4bdf65150b3fdc539db3d03":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"3394716f52b34ab259ad5247e7595d9f9db6e935":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"1d028314cced5858683a1bb4741423d0f934257b":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","a7c7a5405c388fd86e5962126be8ad09283eb5cc"],"0c59b968cd5180c64260dcc70c3f53320d2f848a":["1d028314cced5858683a1bb4741423d0f934257b","aba371508186796cc6151d8223a5b4e16d02e26e","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7"],"cd65a3c65e7917a381c935b0b663d8e783bd9a1e":["0c59b968cd5180c64260dcc70c3f53320d2f848a","33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7"],"aba371508186796cc6151d8223a5b4e16d02e26e":["1d028314cced5858683a1bb4741423d0f934257b"],"33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a":["aba371508186796cc6151d8223a5b4e16d02e26e"],"a7c7a5405c388fd86e5962126be8ad09283eb5cc":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","f9c800d581a0aa5fb80893a3d02e1ec5e2a86ec5"],"f9c800d581a0aa5fb80893a3d02e1ec5e2a86ec5":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","f3f8a0f8bebc057ea4bdf65150b3fdc539db3d03"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d1336abe0899b2984e5652903556c1925fbdca9f":["0e748132a1ca480bd503ee795aee58f5765809a6"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":[],"52c7e49be259508735752fba88085255014a6ecf":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","96ea64d994d340044e0d57aeb6a5871539d10ca5","c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}