{"path":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testTermVectorWithoutOffsetsThrowsException().mjava","commits":[{"id":"815287248ca7a77db68038baad5698c5767f36a7","date":1350761762,"type":0,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testTermVectorWithoutOffsetsThrowsException().mjava","pathOld":"/dev/null","sourceNew":"  public void testTermVectorWithoutOffsetsThrowsException()\n      throws IOException, InvalidTokenOffsetsException {\n    final String TEXT = \"the fox did not jump\";\n    final Directory directory = newDirectory();\n    final IndexWriter indexWriter = new IndexWriter(directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, null));\n    try {\n      final Document document = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorOffsets(false);\n      customType.setStoreTermVectorPositions(true);\n      document.add(new Field(FIELD, new OverlappingTokenStream(), customType));\n      indexWriter.addDocument(document);\n    } finally {\n      indexWriter.close();\n    }\n    final IndexReader indexReader = DirectoryReader.open(directory);\n    try {\n      assertEquals(1, indexReader.numDocs());\n      final TokenStream tokenStream = TokenSources\n          .getTokenStream(\n              indexReader.getTermVector(0, FIELD),\n              false);\n      fail(\"TokenSources.getTokenStream should throw IllegalArgumentException if term vector has no offsets\");\n    }\n    catch (IllegalArgumentException e) {\n      // expected\n    }\n    finally {\n      indexReader.close();\n      directory.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["000498895a9d8c442dd10d03121bd753ec00bc0e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"62e52115b56781006682fd92c6938efaf174304d","date":1351014780,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testTermVectorWithoutOffsetsThrowsException().mjava","pathOld":"/dev/null","sourceNew":"  public void testTermVectorWithoutOffsetsThrowsException()\n      throws IOException, InvalidTokenOffsetsException {\n    final String TEXT = \"the fox did not jump\";\n    final Directory directory = newDirectory();\n    final IndexWriter indexWriter = new IndexWriter(directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, null));\n    try {\n      final Document document = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorOffsets(false);\n      customType.setStoreTermVectorPositions(true);\n      document.add(new Field(FIELD, new OverlappingTokenStream(), customType));\n      indexWriter.addDocument(document);\n    } finally {\n      indexWriter.close();\n    }\n    final IndexReader indexReader = DirectoryReader.open(directory);\n    try {\n      assertEquals(1, indexReader.numDocs());\n      final TokenStream tokenStream = TokenSources\n          .getTokenStream(\n              indexReader.getTermVector(0, FIELD),\n              false);\n      fail(\"TokenSources.getTokenStream should throw IllegalArgumentException if term vector has no offsets\");\n    }\n    catch (IllegalArgumentException e) {\n      // expected\n    }\n    finally {\n      indexReader.close();\n      directory.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"000498895a9d8c442dd10d03121bd753ec00bc0e","date":1389468193,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testTermVectorWithoutOffsetsThrowsException().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testTermVectorWithoutOffsetsThrowsException().mjava","sourceNew":"  public void testTermVectorWithoutOffsetsThrowsException()\n      throws IOException, InvalidTokenOffsetsException {\n    final Directory directory = newDirectory();\n    final IndexWriter indexWriter = new IndexWriter(directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, null));\n    try {\n      final Document document = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorOffsets(false);\n      customType.setStoreTermVectorPositions(true);\n      document.add(new Field(FIELD, new OverlappingTokenStream(), customType));\n      indexWriter.addDocument(document);\n    } finally {\n      indexWriter.close();\n    }\n    final IndexReader indexReader = DirectoryReader.open(directory);\n    try {\n      assertEquals(1, indexReader.numDocs());\n      TokenSources.getTokenStream(\n              indexReader.getTermVector(0, FIELD),\n              false);\n      fail(\"TokenSources.getTokenStream should throw IllegalArgumentException if term vector has no offsets\");\n    }\n    catch (IllegalArgumentException e) {\n      // expected\n    }\n    finally {\n      indexReader.close();\n      directory.close();\n    }\n  }\n\n","sourceOld":"  public void testTermVectorWithoutOffsetsThrowsException()\n      throws IOException, InvalidTokenOffsetsException {\n    final String TEXT = \"the fox did not jump\";\n    final Directory directory = newDirectory();\n    final IndexWriter indexWriter = new IndexWriter(directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, null));\n    try {\n      final Document document = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorOffsets(false);\n      customType.setStoreTermVectorPositions(true);\n      document.add(new Field(FIELD, new OverlappingTokenStream(), customType));\n      indexWriter.addDocument(document);\n    } finally {\n      indexWriter.close();\n    }\n    final IndexReader indexReader = DirectoryReader.open(directory);\n    try {\n      assertEquals(1, indexReader.numDocs());\n      final TokenStream tokenStream = TokenSources\n          .getTokenStream(\n              indexReader.getTermVector(0, FIELD),\n              false);\n      fail(\"TokenSources.getTokenStream should throw IllegalArgumentException if term vector has no offsets\");\n    }\n    catch (IllegalArgumentException e) {\n      // expected\n    }\n    finally {\n      indexReader.close();\n      directory.close();\n    }\n  }\n\n","bugFix":["815287248ca7a77db68038baad5698c5767f36a7"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testTermVectorWithoutOffsetsThrowsException().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testTermVectorWithoutOffsetsThrowsException().mjava","sourceNew":"  public void testTermVectorWithoutOffsetsThrowsException()\n      throws IOException, InvalidTokenOffsetsException {\n    final Directory directory = newDirectory();\n    final IndexWriter indexWriter = new IndexWriter(directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, null));\n    try {\n      final Document document = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorOffsets(false);\n      customType.setStoreTermVectorPositions(true);\n      document.add(new Field(FIELD, new OverlappingTokenStream(), customType));\n      indexWriter.addDocument(document);\n    } finally {\n      indexWriter.shutdown();\n    }\n    final IndexReader indexReader = DirectoryReader.open(directory);\n    try {\n      assertEquals(1, indexReader.numDocs());\n      TokenSources.getTokenStream(\n              indexReader.getTermVector(0, FIELD),\n              false);\n      fail(\"TokenSources.getTokenStream should throw IllegalArgumentException if term vector has no offsets\");\n    }\n    catch (IllegalArgumentException e) {\n      // expected\n    }\n    finally {\n      indexReader.close();\n      directory.close();\n    }\n  }\n\n","sourceOld":"  public void testTermVectorWithoutOffsetsThrowsException()\n      throws IOException, InvalidTokenOffsetsException {\n    final Directory directory = newDirectory();\n    final IndexWriter indexWriter = new IndexWriter(directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, null));\n    try {\n      final Document document = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorOffsets(false);\n      customType.setStoreTermVectorPositions(true);\n      document.add(new Field(FIELD, new OverlappingTokenStream(), customType));\n      indexWriter.addDocument(document);\n    } finally {\n      indexWriter.close();\n    }\n    final IndexReader indexReader = DirectoryReader.open(directory);\n    try {\n      assertEquals(1, indexReader.numDocs());\n      TokenSources.getTokenStream(\n              indexReader.getTermVector(0, FIELD),\n              false);\n      fail(\"TokenSources.getTokenStream should throw IllegalArgumentException if term vector has no offsets\");\n    }\n    catch (IllegalArgumentException e) {\n      // expected\n    }\n    finally {\n      indexReader.close();\n      directory.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testTermVectorWithoutOffsetsThrowsException().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testTermVectorWithoutOffsetsThrowsException().mjava","sourceNew":"  public void testTermVectorWithoutOffsetsThrowsException()\n      throws IOException, InvalidTokenOffsetsException {\n    final Directory directory = newDirectory();\n    final IndexWriter indexWriter = new IndexWriter(directory,\n        newIndexWriterConfig(null));\n    try {\n      final Document document = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorOffsets(false);\n      customType.setStoreTermVectorPositions(true);\n      document.add(new Field(FIELD, new OverlappingTokenStream(), customType));\n      indexWriter.addDocument(document);\n    } finally {\n      indexWriter.shutdown();\n    }\n    final IndexReader indexReader = DirectoryReader.open(directory);\n    try {\n      assertEquals(1, indexReader.numDocs());\n      TokenSources.getTokenStream(\n              indexReader.getTermVector(0, FIELD),\n              false);\n      fail(\"TokenSources.getTokenStream should throw IllegalArgumentException if term vector has no offsets\");\n    }\n    catch (IllegalArgumentException e) {\n      // expected\n    }\n    finally {\n      indexReader.close();\n      directory.close();\n    }\n  }\n\n","sourceOld":"  public void testTermVectorWithoutOffsetsThrowsException()\n      throws IOException, InvalidTokenOffsetsException {\n    final Directory directory = newDirectory();\n    final IndexWriter indexWriter = new IndexWriter(directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, null));\n    try {\n      final Document document = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorOffsets(false);\n      customType.setStoreTermVectorPositions(true);\n      document.add(new Field(FIELD, new OverlappingTokenStream(), customType));\n      indexWriter.addDocument(document);\n    } finally {\n      indexWriter.shutdown();\n    }\n    final IndexReader indexReader = DirectoryReader.open(directory);\n    try {\n      assertEquals(1, indexReader.numDocs());\n      TokenSources.getTokenStream(\n              indexReader.getTermVector(0, FIELD),\n              false);\n      fail(\"TokenSources.getTokenStream should throw IllegalArgumentException if term vector has no offsets\");\n    }\n    catch (IllegalArgumentException e) {\n      // expected\n    }\n    finally {\n      indexReader.close();\n      directory.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testTermVectorWithoutOffsetsThrowsException().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testTermVectorWithoutOffsetsThrowsException().mjava","sourceNew":"  public void testTermVectorWithoutOffsetsThrowsException()\n      throws IOException, InvalidTokenOffsetsException {\n    final Directory directory = newDirectory();\n    final IndexWriter indexWriter = new IndexWriter(directory,\n        newIndexWriterConfig(null));\n    try {\n      final Document document = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorOffsets(false);\n      customType.setStoreTermVectorPositions(true);\n      document.add(new Field(FIELD, new OverlappingTokenStream(), customType));\n      indexWriter.addDocument(document);\n    } finally {\n      indexWriter.close();\n    }\n    final IndexReader indexReader = DirectoryReader.open(directory);\n    try {\n      assertEquals(1, indexReader.numDocs());\n      TokenSources.getTokenStream(\n              indexReader.getTermVector(0, FIELD),\n              false);\n      fail(\"TokenSources.getTokenStream should throw IllegalArgumentException if term vector has no offsets\");\n    }\n    catch (IllegalArgumentException e) {\n      // expected\n    }\n    finally {\n      indexReader.close();\n      directory.close();\n    }\n  }\n\n","sourceOld":"  public void testTermVectorWithoutOffsetsThrowsException()\n      throws IOException, InvalidTokenOffsetsException {\n    final Directory directory = newDirectory();\n    final IndexWriter indexWriter = new IndexWriter(directory,\n        newIndexWriterConfig(null));\n    try {\n      final Document document = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorOffsets(false);\n      customType.setStoreTermVectorPositions(true);\n      document.add(new Field(FIELD, new OverlappingTokenStream(), customType));\n      indexWriter.addDocument(document);\n    } finally {\n      indexWriter.shutdown();\n    }\n    final IndexReader indexReader = DirectoryReader.open(directory);\n    try {\n      assertEquals(1, indexReader.numDocs());\n      TokenSources.getTokenStream(\n              indexReader.getTermVector(0, FIELD),\n              false);\n      fail(\"TokenSources.getTokenStream should throw IllegalArgumentException if term vector has no offsets\");\n    }\n    catch (IllegalArgumentException e) {\n      // expected\n    }\n    finally {\n      indexReader.close();\n      directory.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae230518a1a68acc124bef8df61ef94bd7c1295e","date":1417181719,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testTermVectorWithoutOffsetsThrowsException().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testTermVectorWithoutOffsetsThrowsException().mjava","sourceNew":"  public void testTermVectorWithoutOffsetsThrowsException()\n      throws IOException, InvalidTokenOffsetsException {\n    final Directory directory = newDirectory();\n    final IndexWriter indexWriter = new IndexWriter(directory,\n        newIndexWriterConfig(null));\n    try {\n      final Document document = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorOffsets(false);\n      customType.setStoreTermVectorPositions(true);\n      document.add(new Field(FIELD, new OverlappingTokenStream(), customType));\n      indexWriter.addDocument(document);\n    } finally {\n      indexWriter.close();\n    }\n    final IndexReader indexReader = DirectoryReader.open(directory);\n    try {\n      assertEquals(1, indexReader.numDocs());\n      TokenSources.getTokenStream(\n              indexReader.getTermVector(0, FIELD));\n      fail(\"TokenSources.getTokenStream should throw IllegalArgumentException if term vector has no offsets\");\n    }\n    catch (IllegalArgumentException e) {\n      // expected\n    }\n    finally {\n      indexReader.close();\n      directory.close();\n    }\n  }\n\n","sourceOld":"  public void testTermVectorWithoutOffsetsThrowsException()\n      throws IOException, InvalidTokenOffsetsException {\n    final Directory directory = newDirectory();\n    final IndexWriter indexWriter = new IndexWriter(directory,\n        newIndexWriterConfig(null));\n    try {\n      final Document document = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorOffsets(false);\n      customType.setStoreTermVectorPositions(true);\n      document.add(new Field(FIELD, new OverlappingTokenStream(), customType));\n      indexWriter.addDocument(document);\n    } finally {\n      indexWriter.close();\n    }\n    final IndexReader indexReader = DirectoryReader.open(directory);\n    try {\n      assertEquals(1, indexReader.numDocs());\n      TokenSources.getTokenStream(\n              indexReader.getTermVector(0, FIELD),\n              false);\n      fail(\"TokenSources.getTokenStream should throw IllegalArgumentException if term vector has no offsets\");\n    }\n    catch (IllegalArgumentException e) {\n      // expected\n    }\n    finally {\n      indexReader.close();\n      directory.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5d62e4938659e263e96ae8188e11aea8a940aea5","date":1430230314,"type":5,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testTermVectorWithoutOffsetsDoesntWork().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testTermVectorWithoutOffsetsThrowsException().mjava","sourceNew":"  public void testTermVectorWithoutOffsetsDoesntWork()\n      throws IOException, InvalidTokenOffsetsException {\n    final Directory directory = newDirectory();\n    final IndexWriter indexWriter = new IndexWriter(directory,\n        newIndexWriterConfig(null));\n    try {\n      final Document document = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorOffsets(false);\n      customType.setStoreTermVectorPositions(true);\n      document.add(new Field(FIELD, new OverlappingTokenStream(), customType));\n      indexWriter.addDocument(document);\n    } finally {\n      indexWriter.close();\n    }\n    final IndexReader indexReader = DirectoryReader.open(directory);\n    try {\n      assertEquals(1, indexReader.numDocs());\n      final TokenStream tokenStream =\n          TokenSources.getTermVectorTokenStreamOrNull(FIELD, indexReader.getTermVectors(0), -1);\n      assertNull(tokenStream);\n    }\n    finally {\n      indexReader.close();\n      directory.close();\n    }\n  }\n\n","sourceOld":"  public void testTermVectorWithoutOffsetsThrowsException()\n      throws IOException, InvalidTokenOffsetsException {\n    final Directory directory = newDirectory();\n    final IndexWriter indexWriter = new IndexWriter(directory,\n        newIndexWriterConfig(null));\n    try {\n      final Document document = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorOffsets(false);\n      customType.setStoreTermVectorPositions(true);\n      document.add(new Field(FIELD, new OverlappingTokenStream(), customType));\n      indexWriter.addDocument(document);\n    } finally {\n      indexWriter.close();\n    }\n    final IndexReader indexReader = DirectoryReader.open(directory);\n    try {\n      assertEquals(1, indexReader.numDocs());\n      TokenSources.getTokenStream(\n              indexReader.getTermVector(0, FIELD));\n      fail(\"TokenSources.getTokenStream should throw IllegalArgumentException if term vector has no offsets\");\n    }\n    catch (IllegalArgumentException e) {\n      // expected\n    }\n    finally {\n      indexReader.close();\n      directory.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ae230518a1a68acc124bef8df61ef94bd7c1295e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"62e52115b56781006682fd92c6938efaf174304d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","815287248ca7a77db68038baad5698c5767f36a7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"5d62e4938659e263e96ae8188e11aea8a940aea5":["ae230518a1a68acc124bef8df61ef94bd7c1295e"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["000498895a9d8c442dd10d03121bd753ec00bc0e"],"000498895a9d8c442dd10d03121bd753ec00bc0e":["815287248ca7a77db68038baad5698c5767f36a7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5d62e4938659e263e96ae8188e11aea8a940aea5"],"815287248ca7a77db68038baad5698c5767f36a7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"ae230518a1a68acc124bef8df61ef94bd7c1295e":["5d62e4938659e263e96ae8188e11aea8a940aea5"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"62e52115b56781006682fd92c6938efaf174304d":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["62e52115b56781006682fd92c6938efaf174304d","815287248ca7a77db68038baad5698c5767f36a7"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["ae230518a1a68acc124bef8df61ef94bd7c1295e"],"5d62e4938659e263e96ae8188e11aea8a940aea5":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"000498895a9d8c442dd10d03121bd753ec00bc0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"815287248ca7a77db68038baad5698c5767f36a7":["62e52115b56781006682fd92c6938efaf174304d","000498895a9d8c442dd10d03121bd753ec00bc0e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["62e52115b56781006682fd92c6938efaf174304d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}